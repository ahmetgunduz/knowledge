// The markup language of this document is AsciiDoc
:encoding: UTF-8
:toc:
:toclevels: 4

= Algorithms and data structures


== Misc. terms

In-place:: An algorithm using +O(1)+ auxiliary memory space.  Often even +O(log n)+ is considered as in place.
Sentinel:: A sentinel is a dummy object that allows us to simplify boundary conditions.
Memoization:: The solution to a given (sub)problem is memoized in a `memo pad' (aka table).  E.g. upfront or when first encountering it.  When later seeing the same (sub)problem again, its solution can be looked up in the memo.  See also <<dynamic_programming>>.
[[whp]]
With high probability (w.h.p.):: An event E occurs with high probability if Pr[E] ≥ 1−1/n^c^ for any constant c.


== Asymptotic notations / big O notation
In computer science, big O notation is used to classify algorithms by how they respond to changes in input size, typically regarding running time space (memory/disk/...).

In the following +n+ is the _input size_, +f(n)+ is the _number of steps_ needed by an algorithm.

Types of asymptotic notations:

[cols="3,2,2,6,6"]
|====
| notation | | relation of growth rate | definition | notes
| +f(n) ∊ ο(g(n))+ | little-oh | f < g | ++For all c>0 there exists an n~0~>0 such that \|f(n)\| < c⋅\|g(n)\| for all n≥n~0~++ | f is dominated by g asymptotically.  Intuitively: grows strictly slower than. Rarely used in computer science.
| +f(n) ∊ O(g(n))+ | big-oh    | f ≤ g | ++There exist an c>0 and n~0~>0 such that \|f(n)\| ≤ c⋅\|g(n)\| for all n≥n~0~++ | Asymptotic upper bound (Mnemonic: O has a squiggle at the top (at least in some fonts)). Intuitively: grows no faster than. No claim on how tight the upper bound is; technically it woudn't be wrong to say that a linear algorigthm is +O(2^n)+.
| +f(n) ∊ Θ(g(n))+ | big-theta | f = g | ++There exist an c~1~>0, c~2~>0 and n~0~>0 such that c~1~⋅\|g(n)\| ≤ \|f(n)\| ≤ c~2~⋅\|g(n)\| for all n≥n~0~++ | Asymptotic tight bound (Mnemonic: the bar is in the middle). +Θ(g(n)) = O(g(n)) ∩ Ω(g(n))+
| +f(n) ∊ Ω(g(n))+ | big-omega | f ≥ g | Like O, but ≥ instead ≤ | Asymptotic lower bound (Mnemonic: the bar is at the bottom).
| +f(n) ∊ ω(g(n))+ | little-omega | f > g | Like ο, but ≥ instead ≤ | f dominates g asymptotically. Rarely used in computer science.
|====

Θ is also called _rate/order of growth_.

Note: Because ++O(g(n))++ is really a set, we should actually write ++f(n) ∊ O(g(n))++.  However we often write ++f(n)=O(g(n))++, the equal sign meaning ∊.
Informally, especially in computer science, the big-oh notation often is permitted to be somewhat abused to describe an asymptotic tight bound (it really only describes an asymptotic upper bound) where using big-theta notation might be more factually appropriate in a given context.

_worst case_ / _average case_ / _best case_ refers to the worst / average / best input -- a ``good'' input results in a short running time of the algorithm, a ``bad'' input results in a long running time.  For many algorithms we only care about the worst case, not the average case, because a) the worst case occurs fairly often in practice b) the average case is often as bad as the worst case c) it's difficult to know what an ``average'' input is (often it is assumed that all possible inputs are equally likely).

_Tight bounds_: An upper bound is said to be a tight upper bound (aka _supremum_) if no smaller value is an upper bound.  Likewise for tight lower bounds (aka _infimum_).

_Asymptotic efficiency_: Only look at rate of growth.  An algorithm is said to be _asymptotically optimal_ if, roughly speaking, its big-oh is equal to the big-oh of the best possible algorithm.

_amortized time_: `amortized +O(f)+' for operation o: In a sequence of length L of such o operations, the overall time is +O(L*f)+.  I.e. one of those o operations might use a particular large amount of time compared to the average case, but that time is amortized in the large.  A typical example is appending to an array; if the capacity is full, a new array of larger capacity needs to be allocated, and the data has to be copied.

An _output-sensitive algorithm_ is an algorithm whose running time depends on the size of the output, in addition to, or instread of, the size of the input.

Common functions ordered after order of growth: c, log~c~(n), n, n·log~c~(n), n^c^, c^n^, n!, n^n^


See also:

- http://bigocheatsheet.com/
- http://stackoverflow.com/questions/1364444/difference-between-big-o-and-little-o-notation
- http://stackoverflow.com/questions/2986074/algorithm-analysis-orders-of-growth-question


== Computational complexity classes

The field of computational _complexity classes_ categorizes decidable decision problems by how difficult they are to solve. "Difficult", in this sense, is described in terms of the needed computational resources.  A _decision problem_ is a problem with a binary answer, e.g. yes or no.  A _function problem_ can have answers that are more complex than a simple `yes' or `no'.  Function problems can be transformed into decision problems and vice versa.  Thus computational complexity can focus on decision problems. An _intractable problem_ is one that can be solved in theory (i.e. which is in R), but which in practice takes too long to be usefull. There's no exact definition, but in general problems not in P (but in R) are considered intractable.

Common complexity classes:

P:: (Decision) problems solvable in at most polynomial tyme (n^c^).  If you can establish a problem as not in P, you provide good evidence for its intractability.  You'd better spend your time developing an approximation algorithm or solve a tractable special case.

NP (non-determiniatic polynomial):: (Decision) problems solveable in polynomial time via a ``lucky'' algorithm: Like in dynamic programm the algorithm makes a guess at each branch points where it could follow multiple paths.  However, if the overall answer of the decision problem is yes, it magically (being an awsome cool fairy tale computer) always guesses the path that ultimatively leads to the yes.
+
Equivalently: (Decision) problems where the a given yes-answer (e.g. yes, this sudoku has a solution), has a proof (can take more than polynomial time) (e.g. this solved sudoku) which can be checked in at most polynomial time (e.g. take the alleged solution / proof and verify it holds up to the sudoku rules).
+
Is a nondeterministic computation model.  It's not a realistic model, but it's still a usefull model.

EXP:: (Decision) problems solvable in at most exponential tyme (2^n^).

R (recursive):: (Decision) problems solvable in finite time. Etymology: R stands for recursive, which in the old days stood for `will terminate'.

NP-hard (or X-hard in general):: At least as hard as every element in NP (X in general) (i.e. same hardness or harder, but not less hard than any element in NP (X in general))

NP-complete (or X-complete in general):: Intersection of NP and NP-hard.

[[pseudo_polynomial]]
Pseudo-polynomial:: A numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input, but is exponential in the length of the input – the number of bits required to represent it.  E.g. <<knapsack>>, <<ford_fulkerson_algorithm>>.

Visualization of complexity classes, ordered on a line after hardness:

--------------------------------------------------
              P-complete  NP-complete  EXP-complete    R-complete
easier <----------|----------|-------------|---------|------> harder
      
P(incl P-complete)   P-hard (incl P-complete)
<-----------------+----------------------------------------->

      NP (incl NP-complete)     NP-hard (incl NP-complete)
<----------------------------+------------------------------>
--------------------------------------------------

Most people think P≠NP is true, but no one could prove it so far. It's one of the Millenium Prize Problems.  P≠NP translates to ``you can't engineer luck'', or to  ``solving problems is harder than checking solutions''.  NP is an awfully powerfull model of computation.  It can use this fairy tale computer which always magically guesses the right path.  So NP `obviously' is more powerfull than P -- except we don't know how to proof it.

Examples of NP-complete problems:

- Determining whether a graph contains a simple path with at least a given number of edges
- <<TSP,Travelling salesman problem>>
- <<knapsack>>
- <<hamiltoninan_path_problem>>
- _Boolean satisfiability_ (_SAT_) problem: *to-do*:
- _Subset sum problem_: Given a set (or multiset) of integers, is there a non-empty subset whose sum is zero?
- _clique problems_
 * Finding the maximum clique (a clique with the largest number of vertices)
 * Finding the maximum weight clique in a weighted graph
 * Listing all maximal cliques (cliques that cannot be enlarged)
- _minimum vertex cover_
- _maximum independent set problem_
- _Graph coloring_ regarding vertices (edges): Coloring the vertices (edges) of a graph such that no two adjacent vertices (edges) share the same color.


== Algorithm classifications by implementations

=== Recursion vs iteration

- What is computable by recursive functions is computable by an iterative model and vice versa.

- KISS: Use whichever is more easy to reason about for the given problem.  Since recursion maps easily to proof by induction, for many problems recursion is a straight forward choice.

* Recursion has to pay expense of function calls and function returns, which is typically larger than the (conditional) jump used in the iterative solution.  However in case of tail calls and an compiler featuring tail call optimization becomes pretty much equivalent to iteration since the machine code is iterative.

* Recursion needs memory on the stack for all the locals, the stack frame (the return address, the old stack pointer, ...).  However there are iterative solutions which need an stack or queue, which internally probably uses the heap with all its overhead in space and time.  It depends on the queue/stack implementation which is more efficient in terms of memory usage, locality, ....

- Modern compilers are good at converting some recursions to loops without even asking.


Terms: _base case_ is input for which the solution is directly known.  When the recursion arrives at the base case it is said to _bottom out_.


=== Recipes for convertion recursion to iteration

==== Tail call
Recipe for translating recursion into iteration for a function ++foo++ for the case where recursive calls are convertible to tail calls:

. Convert all recursive calls into tail calls.  If you're programming language supports tail call optimization, you're already done.

. Enclose the body of the function with a ++while(true) { ... }++ loop.

. Replace each call to ++foo++ according to this scheme: ``++foo(f1(...), f2(...), ...)++'' => ``++x1=f1(...); x2=f2(...); ...; continue;++''

. For languages where identifiers need to be defined: For each +x+ object introduced in the previous step, define the object before the while loop introduced earlier.

. Tidy up.


==== Non tail call
`Recipe' for translating recursion into iteration in case there are n multiple recursive calls which are not tail calls and not convertible to tail calls.  It's more tips than a proper recipe.

- Remember that all local variables (which includes parameters) and the return address are on the stack.  So if one needs to know the return address, i.e. one of multiple possible places, it gets nasty difficult.

- Enclose the whole body in a ++stack<...> s; s.push(args); while (!s.empty()) { current_args = s.pop(); ... }++

- Instead of n times recursively calling foo like ++foo(args1); foo(args2);...++ push the args on the stack in reverse order ++s.push(args2); s.push(args1)++.




Recipe for turning a non-tail call recursive function ++foo++ into one having a tail call:

. Identify what work is being done between the recursive call and the return statement.  That delivers a function +g(x,y)+, so the respective expression could be written as ++return g(foo(...), bar)++.
. Extend the function to do that +g+ work for us.  Extend it with an new accumulator argument, ++foo(..., acc=default_doing_nothing)++, and replace all return statements ++return lorem;++ with ++return g(lorem, acc);++.
. Now you can replace very occurrence of ++return g(foo(...), bar)++ with ++return foo(..., bar)++, since we don't have to do +g+ ourselves any more, we can let +foo+ do +g+ for us.

--------------------------------------------------
// example step 1
def factorial(n):
    if n < 2: return 1
    return factorial(n - 1) * n // thus we have an g: g(x,y)=x*y

// example step 2
def factorial(n, acc=1):
     if n < 2: return 1 * acc
     return (n * factorial(n - 1)) * acc //==factorial(n-1)*(acc*n)

// example step 3
def factorial(n, acc=1):
     if n < 2: return acc * 1
     return factorial(n - 1, acc*n)
--------------------------------------------------
See also: http://blog.moertel.com/posts/2013-05-11-recursive-to-iterative.html


==== Non tail call

--------------------------------------------------
stack localsAndParamsStack;
stack addrStack;
addr = FunEntr;
auto done = false;
do {
  switch (addr) {
  case FunEntry:
    ...
  case X:
    ...
  }
} while (not done);
--------------------------------------------------


*to-do*: mind implicit return at end of original function

*to-do*: how to return values from called function?

How to translate calls and returns:

--------------------------------------------------
             function call                      | return
machine instr.     pseudo code in loop          | pseudo code in loop
 -----------------------------------------------|-------------------------
                                                | continue
                                                |
(save locals)      localsAndParamsStack.push(   | localsAndParams = 
                       locals and params)       |    localsAndParamsStack.pop()
                                                |
push params        params = new params          |
                                                |
push returnAddr    addrStack.push(addr)         |
                                                |
jmp funAddr        addr = FunEntry              | addr = addrStack.pop()
                   continue                     |
                                                |
                                                | if (addrStack.empty())
--------------------------------------------------


=== Deterministic vs non-deterministic
*to-do*

=== Serial vs parallel vs distributed
*to-do*

=== Exact vs approximate
*to-do*


== Online Algorithms

An online algorithm is _k-competitive_ if there exists a constant k such that its cost is at most k times the cost of an optimal offline algorithm, ignoring a constant offset.  The factor k is called the _competitive ratio_.  If the constant offset is zero, the algorithm is said to be _strictly competitive_.  Using the paging problem as example, an _conservative algorithm_ has a cost of at most n  (n is the number of available pages) for any sequence of consecutive requests that involve (up to) n distinct pages. In other words, a conservative algorithm only updates its current hypothesis when making a mistake.


=== Randomized online algorithms

There are diffrent models of adversaries: The _oblivious adversary_ knows what algorithm I am running, but he sees none of the coin flips.  It's the adversary model commonly used.  The _fully adaptive adversary_ additionally also sees the coin flips. But that is uninteresting from a randomized perspective, because then the algorithm could as well be deterministic. The _adaptive adversary_ is somewhere inbetween. It knows the previous coin flips, but not the future flips. So it's an online algorithm itself.


== Algorithm design techniques / paradigms

[[relation_between_techniques]]
=== Relation between techniques

Decrease and conquer is similar to divide and conquer.  However the latter splits the problem into two or more sub problems.  The former doesn't need to combine the results of the sub problems.

In dynamic programming, subproblems overlapp and we need to solve them only once. In divide/decrease and conquer, sub problems do not overlap.

Dynamic programming vs greedy algorithm: in dynamic programming and divide/decrease and conquer the choices are made depending on the result of the sub problems. I.e. the sub problems are solved first.  The greedy algorithm makes first a (greedy) choice, thus reduces the problem to a subproblem, and then solves that remaining subproblem.


=== Brute force (aka exhaustive search)
This is the naive method of trying every possible solution to see which is best.


[[divide_and_conquer]]
=== Divide and Conquer

_Divide_ the problem into two or more subproblems that are smaller instances of the same problem.  _Conquer_ the subproblems by solving them recursively.  If the size of a subproblem is small enough, stop recursion (we say the recursion _bottoms out_) and solve it (we call that small subproblem a _base case_) in a straightforward manner.  _Combine_ the solutions the subproblems into the solution of the original problem.  See also <<relation_between_techniques>>.

Examples: Quick sort


[[decrease_and_conquer]]
=== Decrease and conquer (aka prune and search)

In each step the problem is turned into one single sub problem of smaller size, where as the rest ist pruned.  The algorithm stops when the base case is reached.  My thoughts: The size of a subproblem is typically by a constant factor (on average) smaller than one of the parent problem -- if the size would only decrease by a constant amount, in the worst case 1, it would just be the naive brute force solution.  See also See also <<relation_between_techniques>>.

Examples: binary search, quickselect.


[[dynamic_programming]]
=== Dynamic programming (DP)
Basic idea: `carefull brute force'.  Use brute force, i.e. try all possible ways (and in case of optimization problems, take the best one).  However do that `carefully', by dividing the problem recursively into subproblems and use <<memoization>> to solve a particular subproblem only once.  Thus DP is often good for optimizations problems.  The memo is typically an associative array with +O(1)+ insert and lookup time.

The following demonstrates dynamic programing by solving the <<rod_cutting_problem>>: Consider a steel company cutting steel rods and selling the pieces.  For simplicity lengths are integers.  Given a table of prices which states the price for a rod of length i.  How to cut a rod of length n into multiple smaller rods to maximize revenue.

Dynamic programming needs two hallmarks:

1. _Optimal substructure_: An optimal solution to the problem contains within it optimal solutions to subproblems.  I.e. if you have an optimal solutions to each sub problem, you can combine them to form the optimal solution to the original problem.  Example: in the rod cutting problem, if we cut a rod of length +n+ in two pieces,  that gives us two new subproblems, namely optimally cutting these two pieces.

2. _Overlapping subproblems_: A given sub-problem has to be solved/computed many times.  If that's not the case, there's no point in doing memoization.  Example: in the rod cutting problem, the problem of cutting a rod of length 2 has to be solved again and again within the problem of cutting a rod of length greater than 2.  Effectively the sub-problems form a directed graph, where x->y means subproblem x depends on subproblem y (i.e. y must be solved first).

Dynamic programming recipe:

1. _Define all subproblems_: I.e. define all vertices in the subproblem DAG. Details: Typically the input is a sequence of n items. For a given problem, it's subproblems are often either suffixes [i:] (Θ(n)) or prefixes [:i] (Θ(n)) or substrings [i:j] (Θ(n*n)).
2. _Guessing_ (I would say try all): For each step (i.e. node / subproblem), think about all the possible paths (i.e. outgoing edges) that have to be tried.
3. _Recurrence_: Same as step 2, but more formal: Formulate the recursive DP(...) function which returns the min/max/..., which includes defining the base cases.  Check that graph of subproblems is acyclic, i.e. is a DAG.
4. _Implement algorithm_: Implement DP(...), e.g. using one of the approaches presented below: top-down, bottom-up approach or shortest-path in DAG.
5. _Solve original problem_: Just call your algorithm with the right arguments. E.g. in the rod cutting problem, with the original rod lenght as in the problem statement.
6. _Reconstructing a solution_: Step 5 only gave a the value of the optimal solution (e.g. in case of the <<knapsack>> problem: the maximal value is 42), but you might also want to know which choices led there (e.g. which items to pack into the knapsack).
+
Variant 1) Each vertex also stores which choice it made.  Analogous to
DP(a,b,c,...), make it accessible e.g. via DPChoice(a,b,c,...).  Starting at
the root vertex, follow the path of those choices.
+
Variant 2) Starting at root of the DAG (e.g. DP(0,X) in the knapsack problem),
for current DP(a,b,c,...), try again, analogous to step 3, all possible paths
and take the one which results in the current DP(a,b,c,...), then recurse to
the choosen subproblem.

Approaches to implement the actual algorithm, see step 4 above:

_top-down approach_: DFS traverse the subproblem DAG from the root via recursion.  At each node, solve a particular problem only once (when it is first encountered) and in this case save its solution in the memo, and when it later is encountered again, look up the solution in the memo.

_bottom-up approach_: Iteratively solve the subproblems, in reverse topological order of the subproblem DAG.  Each iteration blindly uses the memo (knowing the solution must be there due to the topological order) and then memoizes the solution in memo. In general does the same computation as the top-down approach, provided you only solve those subproblems needed to ultimatively solve the orginal problem (e.g. a naive bottom-up approach of solving the _knapsack_ problem solves the whole DAG / matrix which includes nodes not reachable from the root / original problem).  Sometimes the bottom-up approach can save space, because you might know that you only need the last i solutions, e.g. in the fibonacci example you only need the last two. The topological sorted DAG helps to see if that is the case and how big i is.

_shortest path in DAG_: Often (*to-do*: when exactly / when not?) possible: Solve the <<shortest_path_problem>> (which is has a specialiced, more efficient version for DAGs) in the DAG.

Overall running time: +O(#subprobs * time/subprob)+.  Step 1 gives you #subprobs.  Step 3, i.e. the implemenation of DP, gives you time/subprob.  Recall that each subproblem is solved at most once.

Tiny example: An algorithm returning the n-th fibonacci number. For realistic examples, see <<edit_distance>>, <<knapsack>>.

--------------------------------------------------
# bottup-up                          # top down
                                     memo = {}
fun fib(n):                          fun fib(n):
  memo = {}                            if n in memo: return memo[n]
  for k=1 to(incl) n
    if k<=2: f = 1                     <--same
    else: f = memo[k-1]+memo[k-2]      <--" (recursive calls instead lookup)
    memo[k] = f                        <--"
                                       return f
--------------------------------------------------

Trivia: `Dynamic programming' is a wierd term, just take it for what it is. Still: in british english, `programming' means optimize.  The inventor, bellman, choose it for reasons among `sounds cool to a congress man', `to hide the fact he was doing math research'.

Example algorithms or example problems solvable with dynamic programing: Bellman-Ford, Floyd-Warshall, edit distance, <<knapsack>> (<<rod_cutting_problem>>, change-making problem), <<Dijkstra>>. *to-do* more examples of problems which can be solved using dynamic programming, e.g. from the problems sections. https://en.wikipedia.org/wiki/Dynamic_programming


[[greedy_technique]]
=== Greedy technique / algorithm

A _greedy algorithm_ repeatedly makes locally best choice/decision, ignoring effect on future, with the hope, but not guarantee, of finding an optimal solution to the overall problem.

Problems for which a greedy algorithm works well generally have these two properties:

- _Optimal substructure_: See also <<dynamic_programing>>.  Rational: The choice we just made (an optimal solution to a (mini) sub problem), plus the optimal solution to the subproblem that remains (which we will solve recursively), yields an optimal solution to the original problem.

- _Greedy choice property_: Locally optimal choices lead to globally optimal solutions.

In many problems, a greedy strategy does not in general produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a global optimal solution in a reasonable time.  A greedy algorithm never reconsiders its choices; it makes locally best choices. This is the main difference from dynamic programming, which is exhaustive and is guaranteed to find the solution.

Example algorithms: (Greedy) best-first search, A*, <<Dijkstra>>, fractional knapsack problem, change-making problem for canonical coin system. *to-do*: more examples.


=== Linear programming (LP)

_Integer linear programming_ (ILP) adds the additional constraint that numbers must be integers, making the problem NP-complete.

_standard form_ (aka _general form_, or _primal form_ (see LP duality)): Maximize, by solving for x⃑, a linear objective function x⃑·c⃑, subject to the linear inequalities A·x⃑≤b⃑ and to x⃑≥0. A, c⃑ and b⃑ are constant.

Any linear problem can be converted to the standard form. Original problem wants to minimize: switch signs of c⃑'s coefficients. Original problem has not a non-negative constraint on x~j~: Replace x~j~ by xʹ~j~-xʺ~j~.  Original problem has an equality constraint, say x~1~+x~2~=42: Replace that constraint by two constraints, x~1~+x~2~≤42 and -x~1~-x~2~≤-42.  Original problem has an ≥ constraint, say x~1~≥42 : Replace that constraint by -1 times the original constraint, e.g. -x~1~≤-42.

_certificate of optimality_: *to-do*

_LP duality_: Know the concept, but you probably won't use it often in practice. Every primal form has a _dual form_, where ``everything is inversed'': Minimize, by solving for y⃑, a linear objective function b⃑·y⃑, subject to the linear inequalities Aᵀ·y⃑≥c⃑ and to y⃑≥0.  The primal form and the dual form are equivalent.


References:

- MIT course 6.046J, Design and Analysis of Algorithms (Spring 2015), Lecture 15: "Linear Programming: LP, reductions, Simplex":  https://www.youtube.com/watch?v=WwMz2fJwUCg&t=603s[video], https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-notes/MIT6_046JS15_lec15.pdf[lecture notes]


=== Heuristic method
Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms.

*to-do*

=== Branch and bound

*to-do*


[[ADT]]
== Abstract data types (ADT)
An abstract data type is defined only by the operations that may be performed on it and by mathematical pre-conditions and constraints on the effects (and possibly cost) of those operations.  In OO lingo, it is an interface.  See also <<data structures>>,  which in OO are (non-abstract) classes.

=== Summary

*to-do*: finish tables

*to-do*: combine header cells , e.g. queue and stack are specialized deques

linear collections, excluding priority queues
|=====
|               | list | array | deque | queue  | stack
|insert-at(iter)| x    |       |       |        |      
|insert-front   | x    |       | x     | x      | x
|insert-back    | (x)  |       | x     |        |
|find(pos)      |      | x     |       |        |
|find-front     | x    | x     | x     |        | x
|find-back      | (x)  | x     | x     | x      |
|delete-front   | x    |       | x     |        | x
|delete-back    | (x)  |       | x     | x      |
|delete(iter)   | x    |       |       |        |
|successor/pred.| x    | x     |       |        |
|=====

associative collections and ordered by a key, plus priority queues
|=====
|               | priority queue | BST
|insert         | x              | x
|find(key)      |                | x
|find-min       | x              | x
|find-max       |                | x
|delete-min     | x              | x
|delete-max     |                | x
|delete(key)    |                | x
|successor/pred.|                | x
|=====

// associative unordered collections
// |=====         | set | 
// |insert
// |find(value)
// |delete
// |=====

*to-do*: draw is-specialization/generalization DAG plus data structures implementing them


[[collection]]
[[container]]
=== Collection (aka container)
Grouping of data items.  Generally, the data tiems will be of the same type.

Common operations: Create empty container, report number of objects it stores (size), delete all its objects (clear), insert new objects, remove objects, provide access to stored objects.

[[linear_collection]]
.Linear collections
The elements form a sequence. Example ADTs: <<list_adt>>, <<stack>>, <<queue>> (<<priority_queue>> [not associative since only the min element can directly be accessed], <<deque>>, <<depq>>)

[[associative_collection]]
.Associative collections (sorted or unsorted)
Given a key, the collection yiels a value. Example ADTs: <<associative_array>> (<<set>> [value being the key] (<<multiset>>))

.Graphs
Data items have associations with one or data items in the collection. Example ADTs: <<tree_adt>>.


Notably usually not considered a collection: fixed-sized arrays


[[array_data_type]]
=== Array data type

Random access, fixed size.

Implementation: array data structure


[[list_adt]]
=== List (aka sequence)

Sequencial access (no random access)

Implementations: linked list, doubly linked list, array data structure


[[map]]
[[associative_array]]
[[dictionary]]
=== Associative array (aka map, symbol table, dictionary)
<<collection>> of (key, value) _pairs_ (aka _items_), such that each key appears at most once in the collection.  Specialization of <<multimap>>.

Operations: _insert_ (aka add) a pair, _delete_ (aka remove) a pair, _look-up_ (aka search, find) value associated to a given key.  Optionally also _iterate_ over all pairs, _modify_ (aka reassign), the value of an already existing pair.

Implementations: association list, hash table, binary search tree, radix trees, tries, Judy arrays, ....


[[multimap]]
==== Multimap (aka multihash)
Is a generalization of a <<map>> (aka associative array) in which more than one value may be associated with a given key.  My words: As with <<multiset>>s, this is used in two distinct senses: either equal values are considered identical, and are simply counted, or equal values are considered equivalent, and are stored as distinct items.


[[bag]]
[[multiset]]
==== Multiset (aka bag)
A specialization of an <<associative_array>> in that the value part of the associative array's (key, value) pairs is absent or a sentinel value (like 1).

A generalization of a <<set>> in that it allows duplicates.  This is used in two distinct senses: either equal values are considered identical, and are simply counted, or equal values are considered equivalent, and are stored as distinct items.


[[Set]]
==== Set
A specialication of a <<multiset>> (which in turn is a specialization of an <<associative_array>>), in that no duplicates are allowed.


[[deck]]
[[dequeue]]
[[deque]]
=== Double-ended queue (aka dequeue, deque, deck)
<<linear_collection>> where elements can only be inserted to and removed from either side of the sequence.  Is a generalization of a <<queue>> and a <<stack>> in that elements can be inserted and removed to/from both sides.

Implementations: <<circular_buffer>> which resizes when it's full. <<dynamic_array>>, placing the current elements in its middle, and resize when either side becomes full.

Implemented more specialized ADTs: <<collection>>.

Terminology: Deque is the abbrevation of double-ended queue.  Deque (pronounced deck) is the abbbreviation thereof.  Deck is as in an deck of cars, which also provides a good mental image.

See also: - http://www.codeproject.com/Articles/5425/An-In-Depth-Study-of-the-STL-Deque-Container
- C&plus;&plus;'s deque allows random access/insertion, is thus pretty similar to vector. vector vs deque discussions: http://stackoverflow.com/questions/5345152/why-would-i-prefer-using-vector-to-deque, http://www.gotw.ca/gotw/054.htm


[[depq]]
=== Double-ended priority queue (aka depq or double-ended heap)
*to-do*


[[queue]]
=== Queue
<<linear_collection>> where the element removed is prespecified by a first-in-first-out (FIFO) policy.  Is a specialization of a <<deque>> in that insertion is only allowed on one side and removal only on the other side.

Common operations: Elememts can only be added to its _tail_ side (_enqueue_), and only be removed from the other side called _head_ (_dequeue_).  The only element that can be accessed is the one on the head side (_front_ or _peek_).

Common implementations offer +O(1)+ time and +O(1)+ auxiliary space for these operation and +O(n)+ space for the collection aspect.

Common implementations: circular buffer, doubly linked list, singly linked list with an additional pointer to the last node

Implemented more general ADTs: <<collection>>, <<deque>>


[[priority_queue]]
=== Priority Queue
A min (max) priority queue is similar to a queue, however dequeue extracts the element with the max (min) key.  I.e. each element has a key.  Principal operations for a max-(min-)priority queue: _insert_ (aka _enqueue_), _dequeue_ (aka _extract-max_(__-min__)), _peek_ (aka _max_(_min_)), _increase-key_(_decrease-key_).

Sorting and priority queues: If it is possible to perform integer sorting in O(n) time per key, then the same time bound applies to the time per insertion or deletion operation in a priority queue data structure (Thorup 2007.  It's however a complicated reduction).  *to-do*: elaborate more on relation sorting to priority queues

Common implementations: <<heap>>, self-balancing binary tree


[[stack]]
=== Stack
<<linear_collection>> where the element removed is prespecified by a last-in-first-out (LIFO) policy.  Is a specialization of a <<deque>> in that insertion and removal are only allowed on one single side.

Main operations:  Insertion is often called _push_ and can be only to one side called _top_.
Removal is often called _pop_ and can only be the element at the top end.  The only element that can be accessed is the one on the top end of the stack (_top_ or _peek_).

Implementations: <<array>>, <<linked_list>>.


[[graph_adt]]
=== Graph (ADT)
Chapter <<graph_theory>> explains the mathematical theory behind the graph ADT.

Common implementations:

- _Adjacency list_: Typically for sparse graphs.  Collection of unordered vertex lists, one for each vertex.  Sub-forms of how to implement an adjacency list:
 * An associative array associates each vertex (being the key) to an unordered list (being the value) of its adjacent vertices.  For the associative array, often a hash table is used.  If the key can be an integer, e.g. when the vertices are enumerated, then a simple array can be used instead the associative array.
 * Object graph of vertices: Each vertex has a collection of pointers to its adjacent vertices. Optionally, each element in that collection, actually representing an outgoing edge, also stores other properties of the edge, e.g. its weight.  However note that for undirected graphs, each edge is stored redundantely twice. *to-do*: why is apparently the associative array before much more common than this graph variant?
 * Object graph of vertices and edges: Each vertex object has a collection of pointers to its outgoing edges.  Each edge object has a pointer to its start and end vertex.
- _Edge list_: A collection of edge objects, each edge object storing something to identify the start and end vertex, possibly additionally also the edge's weight.
- _Adjacency matrix_ |V|×|V|:  Rows represent source vertices and columns represent destination vertices and cells the associated edge.  Data on vertices typically stored externally.  Typically for dense graphs, or when a quick way is needed to tell if two vertices are adjacent.  Does not work for multigraphs. *to-do* symetric for undirected graphs, inf for not adjacent vertexes, edge weights...
- _Incidence matrix_ |V|×|E|: The rows represent the columns, the columns the edges, a cell is 1 if the associated vertex is an start point of the assiciated edge, -1 if it's the end point, and 0 otherwise.  In a weighted graph, the 1s are replaced by the edge's weight.


[[tree_ADT]]
=== Tree
Note that there is a distinction between a tree as an abstract data type (what this chapter is about), a data structure (see <<data_structures>>) and a topic in graph theory (see <<tree_graph_theory>>).

Terms (see also those of <<graph_theory>>, in particular <<tree_graph_theory>> and <<DAG>>):

- _siblings_: nodes with the same parent.
- _cousins_: nodes with the same grand parent.
- _internal node_: A node with at least one child.
- _external node_ (aka _leaf_): A node with no children.
- _degree_: Number of sub trees of a node
- _level_: Depth + 1
- _size_: Number of nodes
- _height of tree / node_: Informally: Largest distance (see <<graph_theory>>) between root / that node and any leaf. Formally: ++height(node) = max(height(node.left), height(node.right)) + 1++, whereas height of NULL is -1 (equalently: height of leaf node is 0). Height of tree, height of root, depth of deepest leaf are all synonymous.
- _depth_ of node: Distance from root to that node.

Implementations: See those of <<graph_adt>>,  and the methods for storing a <<binary_tree>>


[[binary_search_tree]]
=== Binary Search Tree (BST)

Is a specialized <<binary_tree>> where 1) each node has a comparable key 2) for each node: the key of the left child, if child present, is smaller-or-equal than the node's key, and the key of the right child, if present, is larger-or-equal than the node's key.  Be +n+ the number of elements.  +h≥lg n+ the height of the tree.  The expected height is +h=lg n+ for a randomly built binary tree.  Some implementations store data only in the leaves.

_binary search tree property:_ If node +y+ is in the left subtree of node +x+, then +y.key<=x.key+, if +y+ is in +x+'s right subtree, then +y.key>=x.key+.

_Search_ key +k+: +O(h)+. Recursively or iteratively, for current node +x+, if +k<x.key+ continue with left subtree, else right subtree.

[[rotation]]_Left/right rotation_: +O(1)+. Preserves the order of elements of an in-order traversal. Note that thus, in case of an BST, also preserves the binary-search-tree property.
+
The following visualizes left/rifght rotation. x/y/r are nodes, A/B/C are subtrees.  The middle subtree B changes the parent from x/y to y/x, and x/y swap parent/child relation which includes that the new parent's parent must be changed to r.
+
----------------------------------------------------------------------
   r                 r
   x      left       y  
A     y     →     x    C
     B C    ←    A B
          right    
A  x ByC         AxB y C
----------------------------------------------------------------------

_Min_/_Max_: +O(h)+. Follow left/right subtree until the leaf is reached.

_Successor_/_Predecessor_: +O(h)+. **To-do**

Implementations: <<avl_tree>>, <<red_black_tree>>

Implements these more general ADTs: <<associative_array>>


[[disjoint_set]]
=== Disjoint-set (aka union-find, merge-find set)
A collection of n elements, partitioned into a number of disjoint sets. Or from another point of view: Given an undireced graph of n vertices, keeps track of connected components, and thus can answer which vertices are connected.

Usually each set chooses one of its elements as the representative; that representative element identifies the set. It is undefined which element is chosen, but it stays the same as long as the data structure is not modified.

Main operations:

- make-set(v): Adds element / vertex v to the collection, as a new set containing only that element.
- find-set(v): Returns the id of the set / connected-component element / vertex v is in. To see if elements / vertices u and v are in same set / connected: find-set(u)==find-set(v).
- merge-sets(u,v): Merges the sets of elements u and v / adds edge between vertex u and vertex v. It is undefined what the id of the new set is.

Implementations: <<disjoint_set_linked_list>>, <<disjoint_set_forest>>

|============
|                  | linked-list | forest / union by rank | forest / union by rank + path compression
| insert(v)        |             | O(1)                   | O(1)
| find-set(v)      | *to-do*     | O(log n)               | O(log* n)
| merge-sets(u,v)  |             | O(log n)               | O(log* n)
|============

Applications: <<Kruskal>>'s algorithm, *to-do*

Trivia: Was invented specifically to make Kruskal's algorithm more efficient


[[data_structures]]
== Data structures
A concrete particular way of organizing data in memory.  In OO lingo, its is a (non-abstract) class.  See also <<ADT>>, which is in OO lingo an interface.


[[table]]
[[array]]
=== Array data structure (aka table)
Fixed size, +Θ(1)+ time for indexing, with a very low constant factor.  ++O(0)++ wasted space.  Due to the fixed size, elements cannot be added / removed.


[[dynamic_table]]
[[dynamic_array]]
=== Dynamic array (aka array list, dynamic table, resizeable array)
In contrast to <<array>> the size is variable, thus allows elements to be added / removed.  _Capacity_ is the number of elements the container could currently hold, and the _size_ is the number of elements it actually currently contains.

[[table_doubling]]
==== Table doubling
When size equals capacity upon an insertion, create a new table with double the capacity and copy all elements over.  Thus insertions are +Θ(1)+ amortized.  Upon deletions, when you don't mind slack, never resize the table (as the STL does), or half the capacity when size drops below capacity/4. In that case both insertions and deletions are +Θ(1)+ amortized. (You can't half the capacity when the size reaches half the capacity because in a sequence like inserting/deleting/inserting/deleting, each operation could encompass a table resize which would mean +O(n)+ per operation.)  Of course, other constants than 2 can be used, as long as the factor which is to do shrink is greater than the factor to enlarge.

One can get +Θ(1)+ by roughly this idea: When you remark that you start to get full, start a new table with a larger capacity, initially empty.  On each insertions operation, copy a constant amount of items from the old table to the new one.  Once the old table is really full, just switch over to the new table.  All in all it's quite complicated, so it's not that often used.


[[linked_list]]
=== Linked list

Implementation of the ADT <<list>>.

Orthogonal properties:

- Singly, Doubly or Multiply linked
- Circular linked yes/no
- Sentinel nodes yes/no


[[circular_buffer]]
=== Circular buffer (aka cyclic buffer, ring buffer, circular queue)
Uses a single, fixed-size buffer as if it were connected end-to-end.

Internally uses 1) an array which's size equals circular's buffer capacity, 2) an pointer (or index) to the first element and 3) one to the last element.  Pointers in a circular buffer wrap around at the underlying array border (array.first and array.last (according array.size=circular_buffer.capacity)).

Implements the ADT <<queue>>

Difficulties:

- Depending on the exact implementation, distinguish the case that the buffer is empty and that it is full is not possible, because in both cases start and end point to the same element.


=== Direct-address table
Implements the <<associative_array>> ADT.  An array of size |U|, where U is the universe, i.e. the set of possible keys.  A key's value is the index into the array where the data corresponding to the key is stored.

Time: +O(1)+ worst average best case.  Space: +O(|U|)+.


[[hash_table]]
=== Hash tables
Implements the <<associative_array>> ADT.  Is an array of size m. A <<hash>> function +h(k,m)+ is used to map a key +k+ to [0,m), i.e. to an index into the table. When two keys hash to the same slot that is called a _collision_. The following subchapters describe ways how to deal with collisions. +α=n/m+ is the _load factor_ of the table.

In general, the various variants have the following properties: Search/insert/delete time in +O(k)+ for the best and average case, and ++O(k+n)++ worst case. Space is usually +O(n)+.

==== Collision resolution with (separate) chaining
Each table slot has associated a sequence of items, typically a singly linked list. The expected chain length is the table's load factor.

Insert/delete/find: +Θ(1)+ (Actually +Θ(1+loadfactor)+, but when loadfactor=O(1) (i.e. m=Ω(n)), it becomes +Θ(1)+). Rational: Paying O(n) to find table slot, then O(loadfactor) to walk the list.

Loadfactor should be +Θ(1)+ (i.e. m should be +Θ(n)+). If +m+ is too small, the loadfactor is too high, in the worst case not +Θ(1)+ anymore.  That would lead to hash table operations not being +Θ(1)+ anymore.  If +m+ is too large, we waste space.

==== Collision resolution with open addressing
Each slot can really only take one key, and has an attribute whether it's free. If a hash maps a given key to an non-free slot, a probe sequence is used iteratively to ultimatively find a free slot. Typically delition and table resize are possible but complicated, since *to-do*.  Unlike with chaining, if all slots are used, the table must be enlarged, see also <<table_doubling>>.

Probe sequences. +h(k,m,i)+ is the same as +h(k,m)+, with the aditional parameter i, denothin the i-th probe. If h(k,m,0) returns a used slot, you try h(k,m,1) and so on.

Linear probing:: ++h(k,m,i) = (h(k,m)+i) mod m++. Good locality, but most sensitive to primary clustering.

Quadratic probing:: Try m1=m0+1, m2=m1+2=m0+3, m3=m2+3=m0+6. Properties between linear probing and double hashing

Double hashing:: ++h(k,m,i) = (h(k,m)+i*h2(k,m)) mod m++. Interval is computed by another hashfuncion. Bad locality, but exhibits virtualy no clustering. m is typically a power of two. If m is even, h2 should deliver an odd number, else every 2nd slot will never be probed.


==== Perfect hashing (FKS)
*to-do*


==== Cuckoo hashing
*to-do*


=== Association list
Is an implementation of the ADT <<associative_array>>.

*to-do*


[[binary_tree]]
=== Binary tree

A <<tree>> data structure in which each node has at most two children.  Note that a <<binary_search_tree>> is something else with more restrictions.

Properties:

- _full_(aka _proper_): Every node other than the leaves has two children.
- _perfect_: (aka ambiguously (see next) complete): A full binary tree in which all leaves have the same depth
- _complete_: Every level, except possibly the last, is completely filled, and all nodes are as far left as possible.
- [[balanced]]_balanced_: height is +Θ(lg |V|)+
- [[weight_balanced]]_weight balanced_: The size difference between the left and the right subtree is kept within some constant factor.
- _degenerated_ (aka _pathological_): Each node has at most one child.  The tree is thus effectively a linked list.

Methods of storing:

- See <<graph_adt>>
- As an implicit data structure in an array.  Be i the current node's index, 0 the first index, then its parent is at index floor((i-1)/2), its right child at 2i+1 and its left child at 2i+2.  In the case of a complete binary tree, no space is wasted.  See also <<binary_heap>> which commonly uses this scheme.


[[naive_BST]]
=== (Naive) binary search tree (data structure)

A data structure implementing the binary search tree ADT. When inserting, the elements are always inserted as leaves, whithout changing previous nodes.

Optionally, store data in leaves only.  Each non-leaf node stores the min and max of the leaves in its subtrees; alternatively, we can store the max value in the left subtree if we want to store just one value per node.  See for example <<1d_orthogonal_range_searching>>.


[[random_binary_tree]]
=== Randomized binary search tree

Randomly permute the input before building the <<naive_BST>>.

*to-do*: 

Expected height E[height]=O(lg(n))


[[avl_tree]]
=== AVL Tree

A data structure implementing the binary search tree ADT.  Is a <<balanced>> binary search tree; balance is ensured by the following invariant: For each node n: |height(n.left) - heigh(n.right)| ≤ 1.  From that (indirectly) follows: tree height ≈ 1.44 lg(|V|).

Time complexity: O(log n) average and worst case for all basic operations (search, insert, delete).

Space complexity: O(n)

Each node stores its _balance factor_, which is the difference in height of the left and right subree. Must be in range [-1,1].

Rough description of how insertion/deletion work:

1. First do a normal BST insertion or deletetion (which honor the BST property)

2. For each node on the path from the newly inserted node up to the root: if balance factor is not in range [-1,1], fix it by only _rotation_ operations.

See also <<search_tree_comparison>>


[[red_black_tree]]
=== Red-Black tree

A data structure implementing the binary search tree ADT.  Is a <<balanced>> binary search tree; Balance is preserved by attributing each node with one of two colors (typically called `red' and `black') in a way that satisfies red-black properties (see below).  Tree height ≈ 2*lg(|V|).

red-black properties:

- Roots and NILs are black (typically NILs are called the leaves and all other `poper' nodes are called internal nodes).

- Every red node has a black parent (i.e. never two consequtive red nodes on a simple path)

- For each descendant of a node n, the number of black nodes on the simple path from n to descendant is the same

Time and space complexity: save as <<AVL>> tree.

See also <<search_tree_comparison>>


=== 2-3 tree
*to-do*


=== 2-3-4 tree
*to-do*


=== B-tree
*to-do*


=== B+ tree
See data_base_systems.txt


[[vEB_tree]]
=== Van Emde Boas tree (aka vEB tree)
Is a tree data structure implementing the ordered <<associative_array>> ADT with m-bit integer keys. It performs all operations in O(lg m) time. The vEB tree has good space efficiency when it contains a large number of elements


[[cartesian_tree]]
=== Cartesian tree

A binary tree having the heap property and having the additional porperty that its in-order traversal delivers a given sequence S. Can be built in O(n) time from S and vice versa.

Example:
--------------------------------------------------
S = [8,7,2,8,6,9,4,5]

T =      2
        / \______
       7         4
      /       __/ \
     8       6     5
            / \
           8   9
--------------------------------------------------

Building cartesian tree T from a sequence S, which assumes parent pointers: To process each new value x, start at the node representing the value prior to x in the sequence and follow the path from this node to the root of the tree until finding a value y smaller than x. This node y is the parent of x, and the previous right child of y becomes the new left child of x. 

Building sequence S from cartesian tree T: If T's nodes are not labeled with values as above, which is the case in many applications, label each node with it's depth. Do an inorder traversal, resulting in an array of node labels.  LCA on T is still the same as RMQ on S.

Applications:

- <<RMQ>> in S corresponds to <<LCA_problem,LCA>> in T.

- <<rectangular_range_query>>

- A <<treap>> is a specialization of a cartesian tree

References:

- https://www.geeksforgeeks.org/cartesian-tree/



=== Treap (aka priority search tree)

A _treap_ is a <<cartesian_tree>> in which each key is given a (randomly chosen) numeric priority.  In other words, it's a balanced (with high probability) binary search tree. The idea is to use randomization and the heap property to maintain balance with heigh probability, i.e. balancedness is not guaranteed. Search, insert, delete in O(log n) time with high probability, but O(n) worst case.

Each node in the this BST additionally has a priority, which is assigned a random value upon insertion. Upon insertion/deletion, both the BST invariant regarding the key and the heap property regarding the priority have to be fullfulled. This is done by a normal BST insert using the key, and then do rotations until the heap property is fullfulled regarding the priorities.

Trivia: The name is a portmenteau of tree and heap.


=== Skip list

Is a data structure implementing the ordered <<associative_array>> ADT. Search, insert, delete in +O(lg n)+ time with high probability (opposed to `only' on average)


[[kd_tree]]
=== kd tree

Assumptions for simplicity: No two points have same x coordinate, and no two points have same y coordinate.

A _k-dimensional kd tree_ is a non-balanced binary search tree storing k-dimensional points.  They are often used for k-dimensional <<rectangular_range_query>>.  Informal description for the 2D case: Each subtree represents an rectangular area,  possibly unbounded, i.e. rectangle edges can be at infinity.  The root represents the whole plane.  As normal in BSTs, each node stores a value, here a point.  Each node splits its associated area in two halfes by an imaginary line through its point.  Nodes at even tree levels split vertically, nodes at odd tree levels horizontally.  Each of its subtrees is then associated with one of the halfes.  Thus the plane is recursively diveded into smaller and smaller rectangles.

Algorithm to build tree:  Partion the set points in x-direction in two equal halfes by finding the median point in x direction, see <<order_statistics>>.  That point becomes the root.  For each of the two partitions recurse.  Only that now, on depth 1 of the recursion, we partition in y-direction.  In general, in a k-dimensianal tree, partatition after (depth%k+1)-th dimension.  Efficient order statistics algorithm are O(n), but these beasts are rather complicated,  so one is often left with an O(n log n) algorithm.  To build a kd-tree, one approach is to pass the set of points as two arrays of pointers, the 2st represents the points ordered in x direction, the 2nd represents the points ordered in y direction.  Before calling the generic build algorithm for the root, we presort those two arrays.  For the generic build algorithm its now easy to find the median and to do the partitioning.

--------------------------------------------------
build-2dimensional-kd-tree(points, depth): node
  if points.size=1: return new leaf node having value points[0]
  if depth is even: splitdirection = in x
  else            : splitdirection = in y
  find median of points in splitdirection -> median, smallerpoints, greaterpoints
  node = new node
  node.value = median
  node.right = build-2dimensional-kd-tree(smallerpoints, depth+1)
  node.left = build-2dimensional-kd-tree(greaterpoints, depth+1)
  return node
--------------------------------------------------

*to-do*

Applications:

- <<rectangular_range_query>>

References:

- http://www.cs.utah.edu/~lifeifei/cs6931/kdtree.pdf

- https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/kdtrees.pdf

- Book "Computational Geometry - Algorithms and Applications", subchapter "kd-trees"



[[range_tree]]
=== range tree

*to-do*.

Applications:

- <<rectangular_range_query>>

References:

- https://courses.csail.mit.edu/6.851/spring12/scribe/lec3.pdf

- Book "Computational Geometry - Algorithms and Applications", subchapter "Range Trees"


=== splay tree

*to-do*


[[search_tree_comparison]]
=== Search tree comparison

<<avl_tree>> vs <<red_black_tree>>: theoretically equivalent since time and space complexity are identical.  AVL trees are more rigidly balanced (≈ 1.44 lg(|V|)) than red-black trees (≈ 2 lg(|V|)), whereas the number of rotations when inserting or deleting is O(lg n) for AVL and O(1) for red-black.  Followingly prefer AVL when number of lookup operations dominate sum of insert/delete operations, and red-black oth erwise.

*to-do*: B-trees for 2ndary memory


[[heap]]
=== Heap
A _heap_ is a specialized tree-based data structure that satisfies the _heap property_: If node A is a parent node of B, then the key of node A is ordered with respect to the key of node B with the same ordering applying across the heap.  In a _max heap_ the parent node key is greater than or equal to those of the children, in a _min heap_ it's smaller than or equal.  Thus the element with the largest (max heap) / lowest (min heap) key is always stored at the root.  Note that there is no implied ordering between siblings or cousins.

Time complexities for binary, binomial, Fibonacci, pairing, Brodal, rank pairing, strict Fibonacci:

Creation::
- create-heap: create an empty heap
- make-heap (aka build-heap aka heapify): create a heap out of given elements. +O(n)+ binary, others *to-do*.
- union (aka merge): +Θ(m lg(n+m))+ binary, +O(lg(n))+ binomial, +Θ(1)+ others

Inspection::
- min (max) (aka peek or find-min/max): +Θ(1)+
- size()

Modification::
- extract-min(-max) (aka pop): +O(lg(n))+
- insert: +Θ(lg(n))+ binary, +Θ(1)+ others
- decrease-(increase-)key: +Θ(lg(n))+ binary & binomial & pairing,  +O(1)+ others

Applications of heaps:

- The heap data structure is one maximally efficient implementation of the <<priority_queue>> ADT.
- Merge sort
- <<Dijkstra>>'s shortest-path algorithm
- Order statistics

A heap data structure should not be confused with `the heap' which is a common name for the pool of memory from which dynamically allocated memory is allocated.


[[binary_heap]]
==== Binary heap
In a _binary heap_ the tree is a complete <<binary_tree>>. *to-do*: study implementation of the basic operations.

--------------------------------------------------
        0
    1       2
  3   4    5  6      0123456789 array index
 7 8 9               01-2---3-- tree level
--------------------------------------------------

parent(i) = floor((i-1)/2)
right-child(i) = (i+1)*2 - 1
left-child(i) = (i+1)*2


- +heapify(i)+.  Assumes that children of node +i+ are max heaps, but +i+ might violate the heap property.  Time: +O(lg(nst))+, where nst are the number of nodes in the sub tree rooted at i.
- +build_heap()+:  Converts an array into a heap.  Common implementation: in a bottom-up manner, for each node, starting at one-before-leaf-height height, call +heapify+.  Time: +O(n)+.


==== Fibonacci heap
*to-do*:


[[tries_comparision]]
=== Tries / suffix tree/arrays comparison

Data structure used to store a node (having up to Σ childs), and the resulting querry time and space needed, both in the trie (as opposed to the node).

|=======
|                               | query     | space      | ordering
| c. trie / array               | O(P)      | O(k·Σ)     | ✓
| c. trie / BST                 | O(P·lg Σ) | O(k)       | ✓
| c. trie / weight balanced BST | O(P+lg k) | O(k)       | ✓
| c. trie / w.b. BST + trimming | O(P+lg Σ) | O(k)       | ✓
| c. trie / van Emde Boas       | O(P)      | O(k·lglgΣ) | ✓
| c. trie / hash table          | O(P)      | O(k)       | ✗
| suffix tray                   | O(P+lg Σ) | O(k)       | ?
| suffix array                  | ?         | ?          | ?
|=======


[[trie]]
=== Trie (aka digital tree, radix tree, prefix tree)
See this as an intro to what a trie is. In practice you will use a <<compressed_trie>>, see there for more details.

Is an implementation of the ordered <<associative_array>> ADT. It stores +k+ items (key/value pairs), the stored keys being strings _T~i~_, i=1,...,k, the strings' letters are from alphabet _Σ_.  Internally, there is also the special letter _$_ which represents the end of a string.  A trie is a rooted tree where each edge is labeled with a letter.  Thus child nodes have an order.  A root-to-leaf path represents a string/key, the so reached leaf node the value associated with that key.  The strings derived from paths root to leaf are called _words_, the strings from other paths are called _prefixes_. |T|=|T~1~|+...+|T~k~| is the max number of nodes stored in the tree. In query/lookop, _P_ is a pattern/query searched in the trie.

Since the child nodes have an ordering, an in-order traversal prints the stored keys in order.

A trie can be seen as a DFA (Deterministic finite automaton) without loops (mind: a loop is not a cycle).  A trie can be compressed into an DAFSA (deterministic acyclic finite state automaton).  A trie eliminates prefix redundancy.  A DAFSA additionally also removes suffix redundancy.

Applications: See <<suffix_tree>>

Trivia: The name trie is a pun on re__trie__val.  Originally pronounced it as `tree'.  However, other authors pronounce it as `try', in an attempt to distuinguish it from `tree'.


[[compressed_trie]]
=== Compressed trie (aka radix tree, radix trie, compact prefix tree)
Based on <<trie>>, see there for terms and symbols. Is an implementation of the ordered <<associative_array>> ADT.  A radix tree is a space-optimized trie, where a node with only one child is removed, merging its two adjacent edges into one, which is then labeled with the concatenation of the labels of the two previous edges.  That is each edge is no longer labeled with a single character but potentially with a string. The [[letter_depth]]_letter depth_ of a node is the lenght of the key/path that leads to it.

Each node has at least two children, so there are less internal nodes than leaves. Recall there are as many leaves as stored strings, i.e. k. Thus there are O(k) nodes in the compressed trie.

Data structure used to store a node (having up to Σ childs), and the resulting querry time and space needed, both in the trie (as opposed to the node).

Compared to a hash table:

- A trie can have (depending on how nodes are represented) predictable look-up time +O(k)+.  A hash table has +O(k+n)+ time complexity worst case:  O(k) is used to generate the key, looking up the key is O(1) average but O(n) worst case.
- A trie does not need a hash function
- A trie can provide an ordering of the entries by key.  I.e. a trie supports ordered traversal.
- Locality is worse for a key, since it randomly accesses the nodes.
- A trie typically uses more space than a hash table, since the graph uses quite a lot pointers.

Compared to a binary tree:

- Binary tree has +O(k * (lg n))+ time complexity for look-up, insertion, deletion.  Mind that comparing a key requires +O(k)+; in many times the worst-case occurs, due to long prefixes towards the leaves.

References:

- https://courses.csail.mit.edu/6.851/spring12/scribe/lec16.pdf


[[suffix_tree]]
=== Suffix tree (aka PAT tree, suffix trie)
Based on compressed trie, see there for terms and symbols. Given a text T, append $ (see <<trie>>), then store all suffixes of T in a (compressed) <<trie>>. The value associated with a leaf is the starting position of the suffix in T.

When given multiple documents T^i^, i=1, 2, ..., n, append $~1~, $~2~, ... and $~n~ to each T^i^ respectively, and then throw all suffixes of each document into the suffix tree.

The suffix tree contains O(T) nodes, thuse O(T) space needed (using reasonable representations of the trie, see there).

Details: Seeing a letter as a string of size one, all edges in a compressed trie have strings on them. Such a string can be stored in O(1) by storing only the indicies (within T) of its first and last letter.

Applications:

- <<string_searching>>:
  * Find (all) occcurences of pattern P in text T. Create suffix tree for T. Using P as key delivers subtree whose leaves corresponds to all occurences of P in T.
  * Find first i occurences of pattern P in text T: Augment the leave nodes so they build a linked list, and augment internal nodes by a pointer to its leftmost descendant leaf.
  * Find number of occurences of pattern P in T: Augment internal nodes in the suffix tree with the number of leaves. Use P as key, the found node thus delivers number of leaves, which equals number of occurences.
  * Find longest substring that occures twice in P: Augment internal nodes with their <<letter_depth>>. Then search the internal node with the largest letter depth in O(T) time.
  * Find longest substring that occures in multiple documents T^i^: Similar to above, but look for the internal nodes with maximum letter depth with greater than one distinct $~x~ below.
  * Longest common prefix of two substrings in T in O(1) time: Take the two leaves corresponding to the indexes where the substring start, <<LCA_problem,find LCA>> in O(1) time, and the letter depth of the found node is the answer.
  * Find all occurences of T[i:j]: Instead of, as in a normal search, finding the node from the root in O(j-i) time, find the (j-i)the ancestor of leaf i in O(1) time (via an _LA_query_)


=== Suffix array
*to-do*

References:
- https://courses.csail.mit.edu/6.851/spring12/scribe/lec16.pdf


=== Suffix tray

Is a combination of a suffix tree and a suffix array,

Note that there's this similar idea ....


*to-do*

References:
- https://courses.csail.mit.edu/6.851/spring07/scribe/lec09.pdf


=== DAFSA as data structure
Represents a finite (since it has no cycles) set of strings aka keys.  Single source vertex.  Each edge is labeled by a letter / symbol.  Each vertex has at most one vertex which is labeled with a given letter.  The accepted strings are formed by the letters on paths from the source to any sink / NIL vertex.

Can be seen as an compact form of a trie.  Uses less space than a trie.  A trie eliminates prefix redundancy.  A DAFSA additionally also removes suffix redundancy.  A trie can store attributes for each string aka key, whereas a DAFSA cannot.

Is an implementation of the ADT <<associative_array>>.


[[disjoint_set_linked_list]]
=== Disjoint-set linked list

A data structure implementing the <<disjoint_set>> ADT, using a linked list for each set. The element at the head of each list is chosen as its representative.

*to-do*


[[disjoint_set_forest]]
=== Disjoint-set forest

A data structure implementing the <<disjoint_set>> ADT using a forest.  Each set is represented by a tree, each element is represented by a tree vertex.  The set representative is the root vertex.  Vertices have parent pointers.

+find_set(v)+ follows the parent pointers until the root, and then returns the root, which is the set representative.  +merge_sets(u,v)+ adds the root of one tree as a new child to the root of the other tree.

A common optimization is _path-compression_: When following the path from a vertex to the root, make all visited nodes direct children of the root. This helps to make the tree rather shallow.

Another common optimization is _union by rank_: Each vertex is assigned a rank, which is an upper bound on the height of the subtree rooted at the given vertex.  +make-set(v)+ sets it to zero, i.e. to the true height.  +merge_sets+ attaches the smaller tree to the larger tree, according to the ranks of the two roots.  The rank of the receiving root is increased by one.  Path compression does not change the rank of any vertex.  Path compression only reduces the height of a tree, so the rank, being an upper bound on the height, is still valid.  Note that it would not be trivial for path compression to maintain the true height of the tree.  Anyway, an upper bound is good enough.


--------------------------------------------------
insert(v):
  parent[v] = v
  rank[v] = 0

find-set(v):
 if parent[v]!=v:
   parent[v] = find-set(parent[v]) // "parent[v] =" is path compression
 return parent[v]

merge-sets(u,v):
  rootOfU = find-set(u)
  rootOfV = find-set(v)

  // naive variant
    parent[rootOfU] = rootOfV

  // union by rank optimization
    if rank[rootOfU]<=rank[rootOfV]: parent[rootOfU] = rootOfV
    else                           : parent[rootOfV] = rootOfU
    if rank[rootOfU]==rank[rootOfV]: rank[rootOfV] += 1
--------------------------------------------------


References:

- https://web.stanford.edu/class/cs166/lectures/16/Small16.pdf


== Augmenting data structures

1. Choose an underlying data structure DS.
2. Determine additional information AI to maintain in DS.
3. Verify that we can maintain AI for the basic operations on DS.
4. Develop new operations.

Let +f+ be an attribute that augments a red-black tree +T+, and suppose that the value +x.f+ for each node +x+ only depends on only the information in the nodes +x+, +x.left+ and +x.right+. Then we can maintain +f+ in all nodes of +T+ during insertion and deletion without affecting the +O(lg n)+ performance of these operations.


== Sorting algorithms
Properties of sorting algorithms.  See also properties of algorithms in general.  Comparison-based sorting algorithm are asymptotically optimal when they run in +O(n lg(n))+ time.

Stable:: Stable sorting algorithms maintain the relative order of records with equal keys
Adaptability:: Whether or not the presortedness of the input affects the running time.
internal/external sorting:: Internal: all the data fits into main memory. External: the input data does not fit into main memory, and parts of it must reside on secondary storage.

=== Overview

To sort arrays:

* Bubble sort, Insertion Sort and Selection Sort, having +O(n^2^)+, are bad. However insertion sort is online, stable, adaptive and has a small constant factor (also due to being CPU cache friendly), so it's often used for the base case of the recursive +O(n log n)+ algorithms. Bubble sort has tiny code size.
* Quick sort is great when it works, but unreliable (+O(n^2^)+ worst case), not stable, +O(log n)+ space for stack. Time complexity has a relatively small constant factor since it's CPU cache friendly.
* Merge sort is reliably good, stable, highly parallelizable, but requires +O(n)+ auxiliary space;
* Heap sort is reliably good, but unstable, and also about a factor of 4 slower than quick sort's best case.
* Introsort (hyprid of quick sort and heap sort) almost same as quicksort, now having the good +O(n log n)+ worst case, but constant factor is between quick sort and heap sort, i.e. worse than quick sort's average case.

To sort linked lists:

* Copy it to an temporary array, sort that, copy the array back to the linked list.  Main reason: array has much better locality than a linked list where the nodes are scattered within memory.
* Variant of merge sort

To sort strings:

* *to-do*

To sort integers:
* *to-do* see e.g. at 6:40 https://www.youtube.com/watch?v=pOKy3RZbSws&list=PLUl4u3cNGP61hsJNdULdudlRL493b-XZf&index=14


References:

- https://en.wikipedia.org/wiki/Sorting_algorithm#Comparison_of_algorithms
- Non comparison sorts with integers: http://en.wikipedia.org/wiki/Integer_sorting


=== Insertion sort / sort by insertion
Time: +O(n^2^)+ worst case & average case, +O(n)+ best case.  Auxiliary space: +O(1)+.  Adaptive.  Stable.  In-place.  Online.  Brief: In each outer iteration, the next element from the yet unsorted part is inserted into its correct position in the sorted part. More detailed: The input is logically divided into a sorted part at the left, initially empty, and an unsorted part at the right, initially the complete input.  In each outer iteration, insertion sort removes (see following swap) the leftmost element from the unsorted part.  In an inner iteration it drags the element to the location the elements belongs to within the sorted part by searching to the left and swapping elements on the way.  Often used for small arrays (since time complexity has a small constant factor).


=== Selection sort / sort by selection
Time: +O(n^2^)+ worst case & average case & best case.  Auxiliary space: +O(1)+.  Brief: In each outher iteration, select the min element from the yet unsorted part and append it to the sorted part. Detailed: Divide the input logically into a sorted part (initially empty) followed by an unsorted part (initially the whole input).  In each iteration search the smallest element in the unsorted part, swap it with the leftmost element of the unsorted part, then increment the pointer dividing the sorted / unsorted sub lists.


=== Bubble sort / sort by exchange
The input is devided logically into an unsorted part to the left, initially the whole input, followed by an sorted part, initially empty.  In each inner iteration, a sliding window of length two elements traverses the unsorted list from left to right, advancing in one element steps.  At each step, the two elements in the sliding window are swapped if needed to ensure the right element is larger than the left element.  The result of one inner iteration is that the sorted part gets one element added to its left side.  The process is repeated until all is sorted. As an optimisation, if an inner loop makes no swaps, it means the `unsorted' part is actually sorted and we can terminate early.


[[quick_sort]]
=== Quick sort
Time: +O(n^2^)+ worst case, +O(n lg(n))+ average case, best case: (simple partition: +O(n lg(n))+, 3way partition and equal keys: +O(n)+).  Auxillary space: worst case: (naive: +O(n)+, Sedgewick +O(log n)+).  Not stable in naive implementation.  Hidden factor in time complexity in practice quite small.  The +O(n^2^)+ worst case running time might be a problem when input size is large and used in an real-time system or system concerned with security (because malicious user potentially can trigger worst case behaviour).


=== Merge sort
Time: +O(n lg(n))+ worst case & average case & best case.  Space: +O(n)+ auxiliary memory.  Stable.  Good locality of reference.  Parallelizes well.  External sorting possible.  1) Divide the sequence into two equal length subsequences 2) sort the two sequences using recursion, recursion stops at a sequence of one element 3) merge the two sequences (see below).  Discussion: Good for sequentially accessible data.  Highly parallelizable (+O(log n)+).  Variant: _natural merge sort_:  Time: +O(n)+ best case, the rest remains equal.  Exploits any naturally occurring runs in the input.  Variant: _external merge sort_: Motivation: input data does not fit into memory.  Divide the input data into N blocks, each block fitting into memory.  Sort each block with any sorting algorithm and write the result to disk (N files).  Then with ``externally merge sorted sequences'' merge the blocks/files.  Variant _merge sort for linked lists_: *to-do*


=== Heap sort
Time: +O(n lg(n))+ worst case & average case & best case.  Auxillary Space: +O(1)+.  In-place.  Not stable.  In practice often somewhat slower that quick sort, however it has a better worst case run time.  Brief: As selection sort, however the min element is found via an binary min heap. Detailed: The input is logically divided into an unsorted part, initially the whole input, followed by a sorted part, initially empty.  The unsorted part is heapified into a max heap.  In each iteration, the first (i.e. max) element is swapped with the last,  thus appending a new element to the left side of the sorted part, and thus also shrinking the heap / unsorted part by one.  Using the heap's sift down operation (or just heapify again the unsorted part), the heap property is re-established.


=== Bucket sort
Time: +O(n^2^)+ worst case, +O(n+k)+ average-case and best case.  Auxiliary space: +O(n+k)+.  Stable.  Not comparison-based; it assumes that the elements' values are uniformly distributed, for simplicity, without loss of generality, assume in [0,1).  Make an array of k `buckets', where each bucket is a sequence of elements, initially empty.  For each element in the input, insert it into the bucket having the array index k*elementvalue.  Then sort each of the buckets with another sort.  Then produce the final output by concatenating the buckets.  Note: the time complexity gets worse if the data is not uniformly distributed as assumed, since certain bucket sequences get much longer than other.


=== Counting sort (aka histogram sort)
+Θ(k+n)+ time complexity, +Θ(k+n)+ or +Θ(k)+ space complexity, depending on whether the _required_ output array (sorting in-place only with the input array is not possible) is taken into account or not.  Not comparison-based; it assumes that each element is an integer in the range +0--k+, where +k=O(n)+ (If you start off with non-integers, you might be able to map to integers).  Stable sort.  Algorithm: With an array a of size k, for each input element x, increment a[x]. Then for each item (index i) in the array, print the number i a[i] times. 

--------------------------------------------------
k=5, input={3,4,2,2,4}
             1s   2s  3s  4s  5s
a           { 0,  2,  1,  2,  0 }
output      {     2,2,3,  4,4   }
--------------------------------------------------


=== Radix sort
Not comparison-based.  Stable sort.  Given n d-digit numbers/strings, each digit can take up to k possible values.  Time: +Θ(d(n+k))+ (provided the internally used stable sort has +Θ(n+k)+), however the hidden constant factor is in practice quite large relative to other sort algorithms.  First sort/group after least significant digit, then after second least, .... The inner sort must be a stable sort, typically counting sort or bucket sort.


=== Merge sorted sequences
Time: +Θ(n)+.  Space: +O(n)+. Imagine n cards within m sorted piles of cards face up.  Take the smallest card yous see and put it on the sorted pile.


=== Externally merge sorted sequences
Given N sorted sequences on disk which do not fit all together into memory.

For each, make a buffered (in memory) input stream.  Make an (buffered) output stream for the result.  Now there are N input streams and 1 output stream and the algorithm works as described in ``merge sorted sequences''.  You might want the I/O to be in separate threads, so the actual algorithm can run while there is IO filling parts of in input stream with new data, or flushing part of the output stream. However, if there are only B blocks/pages of memory available but there are more than B-1 input streams, multiple passes have to be made.


[[BST_sort]]
=== BST sort
Take an implementation of the BST ADT, insert all elements of the input, then do an in-order walk.

When the <<naive_BST>> is used, BST sort does same comparisons as <<quick_sort>>, but in a different order. (where quick sort uses first element as pivot, and then does stable partitioning). When the input is randomly permuted before building the BST, _randomized BST sort_ does again the same comparisons as randomized quick sort choosing its pivot randomly.

=== Partial sorting

Sort the k smallest elements.  Opposed to all elements, as in total sorting.

_partial heapsort_: Heapify to a min heap, then do m extractions.

_quickselsort_: Use quickselect to find k-th smallest element. The way that
algorithm works will leave it at the k-th position. Sort the elements [0,k).

|=====
|                    | time av.       
| partial heapsort   | O(n + k log n)
| quickselsort       | O(n + k log k)
|=====


[[order_statistics]]
== Order statistics / medians / selection problem

_Selection problem_: Find the i-th order statistic, i.e. find the i-th smallest element out of an, typically unsorted, set of elements.

The _i-th order statistic_ is the i-th smallest element out of a set of elements.  For example, the _minimum_ of a set of elements is the first order statistic, and the _maximum_ is the n-th order statistic.  A _median_, informally, is the “halfway point” of the set.  When n is odd, the median is unique, occurring at i D .n C 1/=2.  When n is even, there are two medians, occurring at i D n=2 and i D n=2C1.  Thus, regardless of the parity of n, medians occur at i D b.n C 1/=2c (the _lower median_) and i D d.n C 1/=2e (the _upper median_).  For simplicity in this text, however, we consistently use the phrase “the median” to refer to the lower median.

Overview:

|======
|                                  | time av.               | time worst  
|via sort                          | O(sort(n))             | O(sort(n))
|quick select / random             | O(n) almost certain    | O(n^2)
|quick select / median of 3        | O(n)                   | O(n^2)
|quick select / median of medians  | O(n) high const factor | O(n) high const factor
|Introselect                       | O(n)                   | O(n) high const factor
|Floyd–Rivest algorithm            | O(n)                   | ? *to-do*
|======

=== Trivial algorithms

Finding the minimum, i.e. i=1, element can be trivially solved in +O(n)+ time and +O(1)+ auxillary space by iteratively searching through the array for the smallest element respectively.

If the input is preprocessed sorting it first, then we can just access the i-th element in the sorted collection in +O(1)+. Recalling sorting algorithms, the cost of the preprocessing is typically +O(n log n)+, but can be +O(n)+ for certain input. Obviously this might be a good strategy if the collection is static and thus has to be preprocessed only once, and there are more than +O(log n)+ queries (assuming +O(n log n)+ sorting and +O(n)+ for the competing selection algorithm).


=== Quick select

A <<decrease_and_conquer>> algorithm: In each recursion step, choose a pivot and partition the input (of the current recursion step) into a part smaller than the pivot and larger than the pivot respectively.  That delivers the position / index of the pivot, i.e. if all the values were sorted, the pivot would be at the same position.  Then recurse into the left/right part depending on whether i is smaller/larger than the pivot's index.  The base case is either if i is equal the pivot after partitioning, or when the input size is 1.

The best possible pivot is the median (apart from the i-th element, which would directly deliver the solution), since it halves the input.  Like quicksort, the quickselect has good average performance, but is sensitive to the pivot that is chosen.  See the next chapters on strategies how to choose the pivot.


==== Quick select / variant random pivot random

See the previous `common core' chapter. The pivot is chosen randomly.


==== Quick select / variant median of 3

The pivot is the median of the first, middle and last element. Also sort these three elements.


[[median_of_medians]]
==== Quick select / variant median of medians (aka BFPRT)

See the previous `quick select core' chapter. The chosen pivot is an approximate median (remember that the real median would be optimal). It is guaranteed being between 30th and 70th percentiles.

Algorithm to find pivot in array A: Make groups of five elements and find median in each of those (e.g. via insertion sort and taking 3rd element). Then recursively find median of those medians.

Trivia: BFPRT stands for the name of its inventors Blum-Floyd-Pratt-Rivest-Tarjan

*to-do*: Time, auxillary space?


=== Introselect

The idea is to start out with an decrease and conquer selection algorithm that has very good average performance but bad worst case performance, and if it on the fly remarks that it is making bad progress (i.e. steers towards the worst case), switch to choosing a selection algorithm that has optimal worst case perforamnce.

Concretely, starts out with the quick select variant which uses a random pivot, and potentially switches to the median of medians quick select variant.

Examples of possible switching strategies:

- If the sum of partitions so far exceeds the original input size times a constant factor. Only one variable needed to track sum of partitions so far.
- If at any point the last k partitions did not half the input size, where k is some small positive constant. 

Trivia: Introselect is short for introspective selection.

=== Order statistics tree

1) Augment a binary search tree by augmenting each node with the size of its subtree.

Select(i): O(log n) [however that does not include the cost of creating the tree!!]

2) Via min max heap

*to-do*

=== Floyd–Rivest
Functionally equivalent to quickselect, but runs faster in practice on average.

*to-do*


[[string_searching]]
== String searching / matching algorithms

Problem: Find all/some occurences of a pattern P in a given text T.  Let _Σ_ be an alphabet (finite set), _T_ a string of length _n_, _P_ a string of length _m_.  Both the pattern and searched text are vectors of elements of Σ.

_naive string search_: iteratively check at each location in the searched-text.  Time: +O((n-m+1)m)+ worst case, +O(n)+ average case (note that m<=n).

Comparision *to-do*: Small/big T, small/big P, small/big Σ, repeatedly searching in same T, repeatedly searching same P, ...
- http://programmers.stackexchange.com/questions/183725/which-string-search-algorithm-is-actually-the-fastest
-


=== Rabin-Karp
Time: +Θ(m)+ pre-processing, +Θ((n-m+1)m)+ worst case running time,  +O(n+m)+ expected running time.  *to-do*: I don't see why the naive approach should have a worse expected running time, or a worse constant factor if equal

Compute a hash of the pattern.  Iteratively move a window over the search text until the left edge of the window hits the end of the search text.  The window has the same length as the pattern.  In each iteration compute a rolling hash of the window.  If the window-hash matches the pattern-hash, do a regular string comparison between the window and the pattern, and if they still match, the pattern is found.

Popular rolling hash functions for Rabin-Karp:

--------------------------------------------------
static const int q = ...; // a prime where q*radix<INT_MAX
static const int h = pow(d, m-1) % q;

int find(const string& text, const string& pattern) {
   int radix = ...; // aka d.  size of alphabet, e.g. 127 or 255
   int textlen = text.length(); // aka n
   int patternlen = pattern.length(); // aka m
   int patternhash = hash(pattern, m); // aka p
   int texthash = hash(text, m); // aka ts
   for ( int s=0; s<=textlen-patternlen; ++s ) {
     if (patternhash==texthash && text.issubstring(s,pattern))
       return s;
     if (s<textlen-patternlen)
       texthash = rollinghash(texthash, text[s+1], text[s+patternlen+1]);
   }
   return -1;
}

int hash(const string& str, int len) {
    int acc = 0;
    for ( int i=0; i<len; ++i ) acc = (radix * acc + str[i]) % q;
    return acc;
}

int rollinghash(int hash, char ch_out, char ch_in) {
    return (radix*(hash - ch_out*h) + ch_in) % q;
}
--------------------------------------------------


=== Knuth-Morris-Pratt
Time O(n+m) worst case [O(m) preprocessing], O(m) auxillary space

*to-do*


=== Boyer-Moore
O(n+m) worst case

*to-do*


=== Misc

- <<suffix_tree>>
- _FSA_ / _DFA_: *to-do*


== Graph (incl. tree) algorithms

See also <<graph_theory>> and <<graph_adt>>.

[[BFS]]
=== Breadth-first search (BFS) / traversal

An algorithm for traversing or searching a graph in breadth first order in +O(V+E)+ (= +O(b^d+1^)+) time and +O(V)+ space.  Typically used to traverse a connected graph starting from a single source, thus that's what is shown here. Traversing a disconnected graphs, which implies multiple sources, is theoretically also possible, but not shown here. See the outer skeleton of DFS for the general idea how that would be done.

Intuitively the algorithm is: Starting at the source vertex, a spanning tree is built, level by level.  A queue contains neighbors (not yet part of the spanning tree) of the spanning tree.  In other other words, the queue contains the vertices to be added next to the spanning tree, ordered after their level in the spanning tree.  A visited/unvisited attribute per vertex ensures that a node is only added once to the queue.

Overview version:
--------------------------------------------------
Breadth-First-Search(Graph, source:vertex):
  // 'visited' means 'is in queue or in spanning tree'
  source.visited = true            
  create queue and enqueue root // queue contains neighbors of spanning tree
  while queue is not empty:
    current = queue.dequeue() // add current to spanning tree
    for each neighbor:
      if !neighbor.visited:
        neighbor.visited = true
        queue.enqueue(neighbor)
--------------------------------------------------

Time complexity +O(V + E)+ (each vertex is enqued/dequed at most once, and each edge is looked at twice (from each of its two sides)). Auxillary space complexity is +O(V)+ (Each vertex needs an attribute to know wheter it was visited. Beside that, in the worst case, the queue contains all vertices).  For graphs which are implicitly defined or very large, time complexity is better given as +(b^d+1^)+, whereas b is the branching factor and d is the distance (weights being 1) up to which we search.

To be able to know whether a given node (aka vertex) was already visited typically each node gets a `color' attribute and/or a distance attribute attached.  If it is known that the graph is acyclic that color attribute is not needed since it's inherently not possible for that algorithm to visit a node twice.  There are multiple common naming schemes for the colors. The three color schemes have no advantage over the two color schemes other than some people find the algorithm easier to understand / visualize. 

[options="header"]
|=====
   |relation to distance      |                       |                      |       |
   |NIL / infinite            |unvisited/undiscovered | unvisited            | white | not seen at all
.2+|not NIL / not infinite .2+|visited/discovered     | tentative (neighbor) | gray  | not visited but neighbor of visited; member of Q
                                                      | visited              | black | visited
|=====


Detailed version:
--------------------------------------------------
Breadth-First-Search(graph, source:vertex [, dest:vertex]):
  init(graph, source, Q) # Q contains vertexes adjacent to visited notes
  while Q is not empty:
    current = Q.dequeue()
    [current.color = visited] // not needed in the two-colors schemes
    [do auxillary visit action with current]

    for each neighbor that is adjacent to current:
      if neighbor.color == unvisited // or: neighbor.distance == NIL
        neighbor.color = tentative // or: neighbor.distance = current.distance + 1
        [neighbor.parent = current]
        [if source==dest: return] // if its only about finding path source->dest
        Q.enqueue(neighbor)

init(graph, source:vertex):
  for each vertex v in graph:
     v.color = undiscovered // or: v.distance = NIL
    [v.parent = NIL]

  source.color = tentative // or: source.distance = 0
  create empty queue Q
  Q.enqueue(source)
--------------------------------------------------

Applications:

- Solves the single-source <<shortest_path_problem>> where all edge weights are equal / absent, i.e. where for all paths the path weight equals the path distance.



DFS does not really produce a spanning tree but a spanning forest. The book says `predecessor subgraph'.


==== Tree level order traversal: Recursive approach


==== Tree level order traversal: Iterative approach

Is basically a BFS, simplified by the fact that a tree is an acyclic graph, and that typically the distance is not something that we want to know, and thus neither the color nor the distance attribute is needed.

--------------------------------------------------
Tree-Level-Order-Traversal(root):
  create empty queue Q
  Q.enqueue(root)

  while Q is not empty:
    current = Q.dequeue()
    for each child of current:
      Q.enqueue(n)
--------------------------------------------------


[[DFS]]
=== Depth-first search (DFS) / traversal

An algorithm for traversing or searching a graph in depth first order.  In +Θ(V+E)+ time and +O(V)+ space.  Typically used to traverse the complete, possibly disconnected graph.  As opposed to search only a connected graph starting from a single source; however that is also possible.


==== Recursive algorithm

Basic algorithm for a connected graph:
--------------------------------------------------
DFS-visit(graph, source:vertex):
  // visited ⇔ is part of spanning tree
  mark source as visited    // not needed for tree
  for each neighbor of source:
    if !neighbor.visited:   // not needed for tree
      DFS-visit(graph, source)
--------------------------------------------------

More detailed algorithm for a general graph:
[[DFS_all_source]]
--------------------------------------------------
DFS-all-source(graph):
  for each vertex in graph
    set color=unvisited and parent=NIL
  for each vertex v in graph:
    if v.color == unvisited:
      DFS-visit(graph, v)

DFS-single-source(graph, source:vertex):
  for each vertex in graph
    set color=unvisited and parent=NIL
  DFS-visit(graph, source)

DFS-visit(graph, current:vertex):
  current.color = tentative
  [do auxillary pre-order action with current]
  for each neighbor adjacent to current: // aka explore edges (current,neighbor)
    [if neighbor.color is tentative: graph_has_cycles = true]
    if neighbor.color is unvisited:
      [neighbor.parent = current]
      DFS-visit(graph, neighbor)
  [do auxillary post-order action with current]
  [push to front of topo sorted sequence]
  [current.color = visited]              // not needed in the two color variants
--------------------------------------------------

Analysis DFS-all-source: Time complexity: +Θ(V+E)+. Rational: Each vertex is visited (is current) once -> +O(V)+.  Each outgoing edge of each current (i.e. each node) is looked at -> +O(E)+. Space complexity: +O(V)+. Rational: Each vertex has at least the color attribute attached. Also in the worst case, when the graph is a list, there are as many recursion calls (stack pushes) as vertices.


==== Iterative algorithm

Intuitively the algorithm is: Starting at the source vertex, a spanning tree is build, prefering to grow the tree in depth rather than breadth.  A stack maintains the neighbors of the current nodes' spanning tree anchestors.  Ordered after spanning tree depth, except for those having the `is-in-spanning-tree' flag set; those are semantically not really part of the stack anymore.  When a vertex is popped from the stack, if it is not already part of the spanning tree, it is made the current node and is made part of the spanning tree.  When the current vertex adds a neighbor to the stack he means `I want that neighbor as my spanning tree child'.  Due to the stacks FILO policy, that `takes away' that neighbor from any spanning tree anchestor who also wanted that neighbor as its child.  I.e. the stack can contain a vertex multiple times.  When a given vertex is popped the first time from the stack, his parent wins, and it is added to the spanning tree.  Wannabe parents of further occurences of the same vertex further below in the stack loose.  To prevent adding the vertex again to the tree, each vertex needs an attribute which tells whether or not it's already part of the tree.


Basic algorithm 1 for a connected graph:
--------------------------------------------------
DFS-visit(G:graph, root:vertex):
  root.isInST = false // most authors call the flag 'visited'
  create stack and push root
  while stack is not empty:
    current = stack.pop()
    if !current.isInST:
      current.isInST = true // i.e. visit current vertex
      for each neighbor:
        // The if-guard is not required, but it would be silly to omit it.
        if !neighbor.isInST:   
          // Announce that the current vertex wants neighbor as its spanning
          // tree child. That takes 'take away' that neighbor from any
          // anchestor of the current vertex who also wanted it as its child.
          stack.push(neighbor)  
--------------------------------------------------

Basic algorithm 2, which is closer to the detailed that follows, for a connected graph:
--------------------------------------------------
DFS-visit(G:graph, root:vertex):
  while stack is not empty:
    current = stack.top()
    if current.color == unvisited:
      current.color = visited
      for each neighbor of current:
        stack.push(neighbor)
    else
      stack.pop() 
--------------------------------------------------

The recursive DFS saves on its call stack for each recursion the pair 1) the current vertex and 2) for that current vertex the current neighbor in the sequence of neighbors it is iterating over. The following iterative solution saves neighbors on the stack, but not the `current neighbor'. E.g. when starting, _all_ neighbors of the src node are pushed immediately onto the stack, and only after that the next iteration begins; that is unlike the recursive solution, which only explores the 2nd neighbor of the src after the complete DFS tree of the first neighbor has been explored. 

Note that DFS-visit is only part of DFS-all-src, *to-do* make it correct where to place init

--------------------------------------------------
DFS-visit(G:graph, root:vertex):
  init(G, root, S)
  while S is not empty:
    current = S.top() // not pop
    if current.color = undiscovered
      [do auxillary pre-order action with current]
      current.color = tentative !!! bad wording. we definitely added to the DFS, but we're not yet finished with this vertex
      for each neighbor of current:
        if neighbor.color = undiscovered: <3>
          [neighbor.parent = current] <4>
          S.push(neighbor) <4> !!! tentatively adding to DFS does _not_ change color 
    elif current.color = tentative
      [do auxillary post-order action with current]
      current.color = discovered <2>
      [push to front of topo sorted sequence]
      S.pop()
    elif current.color = discovered <1>
      S.pop()

init(G:graph, root:vertex, S:stack)
  for each vertex v in G:
    v.color = undiscovered
    [v.parent = NIL]
  create empty stack S
   S.push(source)
--------------------------------------------------

<1> Skipping vertices that are already in the DFS tree. There is no direct equivalent in the recursive DFS version because that
<2> (Definitly) adding current vertex to the DFS tree
<3> `If guard' is only required when parent pointers needs to be set. The guard prevents from modifying the now immutable parent pointer of a vertex which already is in the DFS tree.  If the parent pointer is not needed, keeping the if nevertheless may or may not be an optimization: you save `needlessly' adding vertices to the stack (they will be skipped by ① anyway), but pay with one more conditional branch.
<4> Tentatively add neighbor to DFS tree

When there are multiple paths to a node, a node can occur multiple times in
the stack. Since we want depth-first, we want to follow the longer path (take
the vertex instance that is higher up in the stack), and discard the shorter path (the
instance further down in the stack, discarded by the continue case ①). E.g. in
the following graph, A being the root, DFS pushes A as part of the init, pops
A and marks it as discovered, pushes C and B onto the stack, pops B and marks
is as discovered, pushed C _again_, pops C and marks it as discovered, pops C
(the original push) again and skips it since its marked as discovered.

--------------------------------------------------
        A
      /   \
     V     V
     B---->C
--------------------------------------------------

*to-do*: symmetry between DFS and BFS: http://stackoverflow.com/questions/5278580/non-recursive-depth-first-search-algorithm


[[DFS_forest]]
[[DFS_edge_classification]]
==== Edge classification / DFS forest

These edge properties are edge properties of the spanning forest (aka DFS forest) that results from a particular DFS run, not of the graph per se.

The trees in the DFS forest produced by the algorithm above are in-trees (edges point towards root), i.e. edge direction is reversed relativ to the underlying graph.  When in the following ancestry terms are used, they refer to the edges in the DFS tree, not the edges in the underlying graph.

--------------------------------------------------
        A----->B  directed edge in graph
         <·····   directed edge in DFS forest
   parent      child  
  current      neighbor
--------------------------------------------------

_Tree edges_: node->child / neighbor is univisited (aka white). I.e. an edge in the DFS forest.

_Back edges_: node->anchestor / neighbor is exploring (aka gray). A self-loop is considered a back edge.

_Forward edges_: node->descendant(non-child) / neighbor is finished (aka black) and node.starttime<neighbor.starttime.

_Cross edges_: node->neither-anchestor-nor-descendant / neighbor is finished (aka black) and neighbor.starttime<node.starttime. Between two non-ancestor-related sub-trees (i.e. between trees in the depth-first forest or between sub-trees within a given tree).

--------------------------------------------------
      ---A<--
     /  / \  \
     | V   V  |
for  | B   C  |back edge
ward | |   |  |
edge | V   V  |
     \>D<--E-/
       cross edge
--------------------------------------------------

Applications:

- <<cycle_dedection>>: A graph is acyclic iff there are no back edges.


==== Tree (pre-/in-/post-) order traversal: Recursive approach

Trivial

==== Tree (pre-/in-/post-) order traversal: Iterative approach

Is a DFS, but since a tree has no cycles, the color attribute is not needed.

--------------------------------------------------
void traverse(Node* n) {
  MyStack<Node*> parents; // pop returns top element and removes it
  Node* prev = NULL; // prev is always non-NULL except at the beginning
  parents.push(prev);
  for ( Node* next = NULL; n ; prev = n, n = next) {
    // came from top
    if ( prev==parents.top() ) {
      preorder_visit(n);
      if (n->left) { // go down left
        parents.push(n);
        next = n->left;
      else if (n->right) { // skip left, go down right
        inorder_visit(n);
        parents.push(n);
        next = n->left;
      } else { // skip left, skip right, go up
        inorder_visit(n);
        postorder_visit(n);
        next = parents.pop();
      }
    }

    // came from left
    else if (prev == n->left) {
      inorder_visit(n);
      if (n->right) { // go right
        next = n->right;
      } else { // skip right, go up
        postorder_visit(n);
        next = parents.pop();
      }
    }

    // came from right
    else {
      postorder_visit(n);
      next = parents.pop(); // go up
    }
  }
}
--------------------------------------------------


=== (Greedy) best-first search
*to-do*:


[[A_star]]
=== A*
A* is a best-first-search algorithm which solves the single-pair <<shortest_path_problem>>, not allowing negative weights.  A* uses an heuristic, involving the function h, to find the next node to be added to the shortest path tree.  It can also be used to solve the single-source shortest-path problem, but is not intended for it, since h is typically optimized for a specific destination node.  A* is a generalized version of <<Dijkstra>>'s algorithm: if h returns always 0 (i.e. is independent of a destination node) then A* equals Dijkstra.

How the algorithm works intuitively (use case `monotonic h'):  Construct the shortest path tree (_SPT_) by adding source to the SPT, and then iteratively adding a vertex to the SPT.  The vertex v to be added to the SPT is the one with minimal `estimated shortest path length from source to destination via v'. That is correct due to the monotonic h and the fact that any sub path of a shortest path is itself a shortest path.

Each vertex gets attached two additional attributes: shortest path tree parent (*+spt_parent+*) and shortest path weight from source to this node (*+spwfs+*).  While a node is in the queue, these are tentative values. Once a node is dequed and thus really added to the shortest path tree, they remain at that final value.  In case of +spwfs+ its value is an upper bound on the true value.

As parameter the algorithm takes an heuristic function *h*, or more verbosely, +*estimated_spw_to_dest*(from:vertex)+, which shall return an estimated shortest path weight from the given vertex to the implicit destination vertex. More on h later.

From h another function is derived: ++function vertex::**k**() = .spwfs + estimated_spw_to_dest(this);++.  It returns the estimated shortest path weight from the implicit source vertex to the implicit destination vertex via vertex v.  k() is used as the key for the min priority queue.

Algorithm which assumes h is monotonic:

When dest==NIL the algorithm solves the single-source shortest path problem.  The solution is the shortest path tree given by the .spt_parent attribute of each vertex, along with the .spwfs attribute of each vertex.

When dest!=NIL the algorithm solves the single-pair shortest path problem.  The solution is the shortest path from source to dest with a weight of dest.spwfs and given by a linked list from dest to source, starting at dest.spt_parent.

--------------------------------------------------------------------------------
function A*(graph, source:&vertex, dest:&vertex,
        estimated_spw_to_dest:function(:vertex)->double )
    create min priority queue outsideSptQ  where key is vertex::k() <1>
    for all vertices v in graph:
        [v.spt_parent = NIL]
        v.spwfs       = (v==source ? 0 : INFINITE)
        outsideSptQ.enqueue(v)

    while outsideSptQ not empty:
        current = outsideSptQ.deque() <2>
        [if current == dest return]
        for each neighbor of current:
            relax(current, neighbor, outsideSptQ) <3>

function relax(u:vertex, v:&vertex, Q:MinPriorityQ)
    alternate_spwfs_for_v = u.spwfs + weight(u,v)
    if alternate_spwfs_for_v < v.spwfs:
        Q.decreaseKey(v, alternate_spwfs_for_v)
        [v.spt_parent = u]
--------------------------------------------------------------------------------

Optimizations and notes to the above algorithm:

<1> Optionally adapt the min priority queue implementation slightly: If +dequeue+ returns NIL if all remaining keys are infinite, we can sooner terminate.  Also the implementation might take advantage of the knowledge that many keys are infinite, especially at the beginning.
<2> Greedely choose vertex with smallest k and add it to the SPT. +current.spwfs+ is now at its final true value, i.e. no longer an estimate / upper bound.
<3> Note that +neighbor+ could already be part of shortest path tree and thus could be skipped.  However it's not worth explicitly checking that (in the presented implementation there would also not be a trivial way to do so), since the if statement within +relax+ implicitely ensures that not much is made with +v+.

Properties of the A* algorithm:

- complete (will always find a path if one exists)
- optimal (*to-do*: i think this explains optimal wrongly: finds the shortest path), but only if the provided heuristic function is admissible.  However see <<bounded_relaxation>> later in the A* sub chapter.
- *to-do:* I don't trust the time complexity, space complexity noted in Wikipedia,  I guess they forget to account for the costs of the min priority queue and of h.  If h is const, then there two options: call it once for every vertex at the beginning and store the result, or call it every time when needed.  Depending on the problem one or the other will be more optimal.
- *to-do:* Does it work for the case when the single-source shortest path problem is tried to be solved?
- Is an best-first search, but not greedy

About the heuristic function h aka +estimated_spw_to_dest(from:vertex)+:

- Responsibility: Returns an estimated shortest path weight from the given vertex v to the implicit destination vertex.
- Should be an admissible (it must not overestimate) heuristic.  If it fails to be admissible, A* is no longer optimal.  However see also <<bounded_relaxation>>.
- Should be monotone (aka consistent: For every edge (x,y): h(x) ≤ edge_weight(x,y)+h(y)).  A* can be implemented more efficiently by not making a node the current more then once (remember the above algorithm is for monotonic h).  Running such an A* algorithm on graph G is the same as running Dijkstra's algorithm on an alternate graph G' where edge_weight'(x,y) = edge_weight(x,y) + (h(y)-h(x)).
- *to-do*: must be constant (during the run-time of the algorithm)?
- Examples how to implement h: euclidean distance.

[[bounded_relaxation]]
_Bounded relaxation_: While the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path.  It is possible to speed up the search at the expense of optimally by relaxing the admissibility criterion.

Relation between different common naming schemes. spt_parent and and spwfs are my own abbreviations/terms; I prefer them over just parent and distance respectively since they more precisely say what they mean exactly.

[options="header"]
|=====
   |scheme 1  | scheme 2       | scheme 3 | spwfs' value           | is in outsideSptQ
.2+|unvisited | unvisited      | white    | INFINIT, upper bound   | yes 
              | tentative/open | gray     | < INFINIT, upper bound | yes
   |visited   | closed         | black    | final value            | no
|=====

[options="header"]
|=====
|scheme 1   | scheme 2       | comments
|spwfs      | d or distance  | actually an upper bound
|spt_parent | π or parent    |
|=====


[[Dijkstra]]
=== Dijkstra's algorithm

A greedy (can also be seen as dynamic programming) algorithm which solves the single-source <<shortest_path_problem>>, not allowing negative weights, in +O(E+V*lg(V))+ time and +O(V)+ space, assuming Fibonacci heaps are used. That notably includes directed or undirected graphs and graphs with cycles.

Terms and abbreviations: ubspwfs = upper bound on shortest path weight from source (in literature often called distance or simply d), SPT = shortest path tree. See also <<A_star>>.

Intuition: A spanning tree is built, more specifically a SPT (see terms).  Vertices are added in order of how close to the SPT they are.  Each vertex is assigned an ubspwfs (see terms) attribute.  Out of the vertices not yet in the SPT, iteratively greedely choose the vertex with the smallest ubspwfs.  I.e. the vertex which is closest to the SPT.  Typically a min priority queue with ubspwfs as key is used to find that vertex.  Add the found vertex to the SPT and update the ubspwfs for all it's neighbors.

----------------------------------------------------------------------
Dijkstra(graph, source:vertex)
  create empty min priority queue sptNeighborsQ with ubspwfs as key
  for each vertex v in graph:
    v.ubspwfs = (v==source ? 0 : INFINITE)
    sptNeighborsQ.enque(v)
  while sptNeighborsQ not empty:
    // greedely choose vertex nearest to SPT and add it to SPT
    current = sptNeighborsQ.deque()
    for each neighbor of current:
      relax(current, neighbor)
----------------------------------------------------------------------

Detailed description: Each node has an attribute ubspwfs. It is initially infinite, and each relaxation step potentially makes it smaller, i.e. potentially tightens the upper bound, until it reaches its final, true smallest value. It can be proven that the smallest ubspwfs in sptNeighborsQ, i.e. the one that the next dequeue call will return, has arrived at its true final spwfs value, and will never change again. Thus by choosing the vertex with the smallest ubspwfs, the algorithm always adds a vertex to the SPT which is not only closest, but in fact for which the spfws is known. Since the relaxation sets the spt_parent always to the current vertex, i.e. a node which already is in the SPT, when a vertex is dequed, its spt_parent is in the SPT, and thus it is implicitely added to the SPT.

----------------------------------------------------------------------
Dijkstra(graph, source:vertex [,dest:vertex])
  create empty min priority queue sptNeighborsQ with ubspwfs as key
  for each vertex v in graph:
    [v.spt_parent = NIL]
    v.ubspwfs   = (v==source ? 0 : INFINITE)
    sptNeighborsQ.enqueue(v)
  while sptNeighborsQ not empty:
    // greedely choose vertex nearest to SPT and add it to SPT
    current = sptNeighborsQ.deque()
    [if current==dest return]
    for each neighbor of current:
      relax(current, neighbor, sptNeighborsQ)

relax(u:vertex, v:vertex, Q:MinPriorityQueue)
  alternate_ubspwfs_for_v = u.ubspwfs + weight(u,v)
  if alternate_ubspwfs_for_v < v.ubspwfs:
    Q.decreaseKey(v, alternate_ubspwfs_for_v)
    [v.spt_parent = u]
----------------------------------------------------------------------

Proof: Note that the shortest path problem has optimal substructere, see <<shortest_path_problem>>. *to-do*: Understand proof that the (upper bound) spwfs of the front vertex ofs sptNeighborsQ has arrived at it's true final value.

Notes:

- relax is called for all neighbors, even if they are already part of the SPT. Since their spwfs is already at the true final value, which can be proven, it will never be decreased anymore, so always calling relax doesn't hurt.
- Rational why negative weights are not allowed: The Dijkstra algorithm relies upon that adding a vertex to a shortest path does not decrease the shortest path's weight.
- Dijkstra can be seen as dynamic programming algorithm. DP(i) is the spwfs for vertex i: DP(i) = min~all predecessors j of i~{DP(j) + weight(j,i)}, with base case DP(startvertex)=0. See also https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm#Dynamic_programming_perspective.

Relation to other algorithms: Dijkstra is a special case of the A* in that that A*'s h function is constant 0, but is a generalization of A* in that that it not only can solve the single-pair shortest path problem but also the single-source shortest path problem. Algorithm when implemented in terms of A*:

----------------------------------------------------------------------
function Dijkstra(graph, source:vertex)
    return Base(graph, source, NIL, lambda(v:vertex){0})

function Dijkstra(graph, source:vertex, dest:vertex)
    return Base(graph, source, dest, lambda(v:vertex){0})
----------------------------------------------------------------------

Analysis: Time +O(E+V*lg(V))+ if a Fibonacci heap is used to implement the min priority queue.  Using a binary heap it's +O((E+V)*lg(V))+, with an array it's  +O(E+V^2^)+.  Auxillary space: +O(V)+ -- however, the same space amount is also used for an answer which includes all shortest path weights and the shortest path tree.  If the answer must only include the weight of one target node the time and auxiliary space complexity remains the same.

Trivia: Etymology of the term `relaxation': There are two variants. 1) It comes from mathematics where relaxation means relaxing a constraint. Here the constraint is v.spwfs<=u.spwfs+weight(u,v). The smaller v.spwfs is, the less `pressure' there is to satisfy this constraint, thus the constraint is relaxed. 2) Think of the upper bound as an extended spring. Making the upper bound smaller relaxes the spring. The true value is reached when the spring is in its resting position. However from another viewpoint one could find it strange to call tightening an upper bound a relaxation.


=== Bidirectional search
It runs two simultaneous searches: one forward from the initial state, and one backward from the goal, stopping when the two meet in the middle.

*to-do*

[[Bellman_Ford]]
=== Bellman-Ford
A dynamic programming algoritm solving the shortest-path single source problem in O(|V|·|E|). Allows for negative weights (which Dijkstra doesn't) and can report negative cycles.

--------------------------------------------------
fun bellman-ford(G, source:vertex)
  init(G, source)
  |V| times:
    for each edge e in G:
      relax(e)
  [dedect-negative-cycles]

relax(e:Edge)
  for both directions of e:
    alternate_distance = e.v_from.distance + e.weight
    if alternate_distance < e.v_to.distance
      e.v_to.distance = alternate_distance
      [e.v_to.parent = e.v_from]

init(G)
  for each vertex v in G:
    v.distance = inf
    [v.parent = NIL]
  source.spwfs = 0

dedect-negative-cycles(G)
  for each edge e in G:
    if relax(e) would relax:
      abort, negative cycle found
--------------------------------------------------


[[floyd_warshall]]
=== Floyd–Warshall algorithm
A dynamic programming algorithm which solves the <<shortest_path_problem>> all pairs problem in O(|V|^3^). Negative weights are allowed, negative cycles are dedected.

Basic idea: You have a matrix C (V×V) storing shortest path for all vertex
pairs. Initially, no intermediate vertices are allowed, i.e. C corresponds to
the graph in its adjacency matrix representation. Then you iterate |V| times:
in each iteration, one more intermediate vertex is allowed, thus relaxing each
cell.

----------------------------------------------------------------------
Floyd-Warshall(G:GraphAsAdjacencyMatrix)
  C = G
  for k=1 to |V|:
    for i=1 to |V|:
      for j=1 to |V|:
        C[i][j] = min(C[i][j], C[i][k] + C[k][j]) // relaxation
----------------------------------------------------------------------

Explanation: c~ij~^k^ is a cell in the matrix in iteration k. It represents
the shortest path from v~i~ to v~j~, choosing intermediate vertices only from
the set {v~1~, ..., v~k~}. It's value is the min of:

- c~ij~^k-1^, i.e. directly going from v~i~ to v~j~, choosing intermediate
  vertices only from set {v~1~, ..., v~k-1~}.
- c~ik~^k-1^ + c~kj~^k-1^, i.e. going via v~k~, and both v~i~ to v~k~ and v~k~ to v~j~ choosing intermediate
  vertices also only from set {v~1~, ..., v~k-1~}.

*to-do* cycle dedection


[[Kruskal]]
=== Kruskal's algorithm

A greedy algorithm solving the <<MST_problem>> for a connected weighted graph in O(sort(E)+(E+V)α(V)) time, assuming an asymptotically optimal disjoint-set data structure is used internally, and where α is the very slowly growing inverse function of the Ackermann function.

Intuition: Start out with each vertex being its own tree.  They represent the tentative, yet disconnected, `MST'.  Then iteratively add the smallest weighted edge to the `MST' iff that does not produce a cycle in the `MST', since a by definition a tree is not allowed to have cycles.  Typically that cycle detection is done via the help of a <<disjoint_set>> data structure.  There would be a new cycle if the start and the end vertex of a given edge are in the same tree, i.e. in the same set.

--------------------------------------------------
KRUSKAL(G):
  foreach vertex v:
    MAKE-SET(v)
    v.MSTParent = NIL
  order edges by weight, increasing
  foreach edge (u, v) in that ordered edge sequence:
    if FIND-SET(u) ≠ FIND-SET(v):
      add edge to MST, e.g. u.MSTparent = v
      MERGE-SETS(u, v)
  starting at any vertex, follow MSTparent pointers to find MST's root
--------------------------------------------------

Proof: It's not trivial to see why this greedy algorithm is optimal, see http://tandy.cs.illinois.edu/Kruskal-analysis.pdf.

Analysis: O(sort(E)+(E+V)α(V)) time.  Note that the running time depends heavily on the used sort algorithm used to sort the edge weights.  So if the weights are integers, a non-comparative sorting algorithm such as radix sort can be used.

References:

-  http://tandy.cs.illinois.edu/Kruskal-analysis.pdf


[[Prim]]
=== Prim's algorithm

A greedy algorithm solving the <<MST_problem>> for a connected graph in O(E + V log V) time (i.e. the same as Dijkstra's algorithm), assuming a Fibonacci heap is used.

Intuition: Start with any vertex as being the root of the MST.  There's a cut between the nodes in the (so far) MST and the rest of the nodes.  Iteratively greedely add a light edge.

To be able to do that, each vertex as an attribute distanceToMst.  It is the distance to the MST using only one edge, initialy infinite.  Vertices in the MST have distanceToMst set to zero.  Thus distanceToMst is larger than zero and less than infinite iff the vertex has an edge crossing the cut.  Thanks to a min piority queue having distanceToMst as key, we always know a light edge.

Basic algorithm. Note that it's quite similar to Dijkstra's shortest path algorithm.
--------------------------------------------------
MST-Prim(G:graph):
  for each vertex v:
    v.distanceToMst = infinit
    v.parent = NIL
  choose any vertex as root
  root.distanceToMst = 0
  create min priority queue outsideMstQ out of all vertexes, distanceToMst being key  
  while outsideMstQ not empty:
    current = outsideMstQ.deque() // greedely add vertex to MST
    current.distanceToMst = 0
    for each neighbor of current:
      prim-relax(current, neighbor, outsideMstQ)

prim-relax(u:vertex, v:vertex, Q):
  if weight(u,v)<v.distanceToMst
    Q.decreaseKey(v, weight(u,v))
    v.parent = u
--------------------------------------------------

Analysis:

- Initialization loop is done V times, and costs O(V) overall

- Overall prim-relax is called twice for each edge, i.e. O(E) times. prim-relax costs O(decrease-key), which is O(1) for a Fibonacci heap, so its O(E) overall.

- Outer loop is over all vertices, i.e V times. O(dequeue) is O(log V), so overall its O(V log V).

- All together its thus O(E + V log V)


References:

- Book ``Introduction to algorithms'', subchapter ``Prim's algorithm''.


[[ford_fulkerson_method]]
=== Ford-Fulkerson method

The ford-fulkerson method is a greedy method which solves the <<maximum_flow_problem>>.  It's called a method opposed to algorithm since it's a template on which the more concrete algorithms are built.  *to-do*: why is it called a greedy algorithm? a) It does not choose the local best solution, it just picks _a_ solution b) it arguably undoes previous decisions by allowing `removing' already existing flow from an edge.

Overview: Search some path in the residual network, augment the current flow with the found path. Iterate until there is no augmenting path in the residual network.

--------------------------------------------------
ford-fulkerson-method(G:flownetwork, source:vertex, sink:vertex)
  flow = 0
  while there exists an augmenting path p in the residual network:
    augment flow along p
  return flow
--------------------------------------------------

If graph is undirected, see <<maximum_flow_problem>> for a transformation. If the capacities are rational numbers opposed to integers, transform by appropriately scale all capacities to turn them into integers.

Sending flow along the residual graphs backward edges means conceptually `undoing' decisions of previous iterations.

References:
- See <<flow_network>>
- Book ``Introduction to algorithms'', subchapter ``The Ford-Fulkerson Method''.


[[ford_fulkerson_algorithm]]
=== Ford-Fulkerson algorithm

An implementation of the <<ford_fulkerson_method>> which solves the <<maximum_flow_problem>> in +O(Ef)+.

--------------------------------------------------
ford-fulkerson-algorithm(G:flownetwork, source:vertex, sink:vertex)
  for each edge e in G:
    e.flow = 0
  while there exists an augmenting path ap in the residual network:
    rcp = min capacity of ap's edges // residual capacity of path ap
    for each edge re in ap:
      be e the corresponding edge in G
      if re is an forward edge: e.flow += rcp
      else (i.e. backward)    : e.flow -= rcp
  return flow
--------------------------------------------------

Analysis: In the worst case, mind that all capacities are integers, the residual capacity is 1, so there are +O(f)+ iterations. Each iteration takes +O(E)+, so the overall cost is +O(Ef)+, i.e. pseudo-polynomial. The reason is that the algorithm allows that in each iteration, a silly augmenting path is choosen, while there might be much better augmenting paths.

References:

- See <<flow_network>>


[[edmonds_karp]]
=== Edmonds-Karp algorithm

An specialiation of the <<ford_fulkerson_algorithm>> which solves the <<maximum_flow_problem>> in ++O(VE²)++. It is a specialication by specifying that the augmenting path is to be found by a breadth-first-search (O(V+E)=O(E)) (i.e. searching the shortest path where each edge has weight 1).

Analysis: Each iteration is +O(E)+. It's proofable that the number of iterations
is +O(VE)+. Thus the overal running time is +O(VE²)+.  The mentioned proof is based upon that once a critical edge is removed from residual network, it proofably can reappear only a certain mount of times. Also, the shortest path distance in the residual network from source to sink proofably increases monotonically after each augmentation / iteration.

Intution in what way we want to improve the ford-fulkerson algorithm: If in the residual graph there is no augmenting path, i.e. source and sink are disconnected, then we found the maximum flow. Thus we strive for increasing the distance (edge weights 1) between the source and the sink.

References:

- See <<flow_network>>
- CMU, Course 15-451/651 (Algorithms) Fall 2013, Lecture "Network Flows II: Edmonds-Karp 1, Edmonds-Karp 2, and blocking flows": https://www.cs.cmu.edu/~avrim/451f13/lectures/lect1008.pdf[lecture notes]


[[push_relabel]]
=== push-relabel algorithms

Is an algorithm which solves the <<maximum_flow_problem>>. Is considered one of the most efficient maximum flow algorithms. The generic algorithm has +O(V²E)+, the variant based on the highest label node selection rule has +O(V²√E)+.

Concepts: A _preflow_ is like a flow, only that the flow conservation constraint is relaxed: The flow into a vertex may exceed the flow out of a vertex.  The _excess flow_ of a vertex is the difference between flow in and flow out. Note that in most physical analogies, the capacity is a quantity per time, and so is excess. An excess of 3 thus means that 3 units accumulate per unit of time in the given vertex.  Most authors describe that figuratively, each vertex has a reservoir of infinite size.  Alternatively figuratively an excess flow means that a vertex is leaking.  In a regular flow all excess flows are 0.  In a preflow they are ≥ 0. An vertex other than source or think is said to be _overflowing_ (aka _active_) if its excess flow is > 0.  Each vertex as a _height_, which is an integer. The source has always height |V|. The sink has always height 0.  All other vertices start at height 0 and potentially raise over time.  They never decrease.  In the residual graph, edges can go downhill by at most 1. They can be flat an go uphill.  An edge in the residual graph is _admissible_ if it's downhill. Since edges in the residual graph can go downhill only by at most one (see above definition of height), admissible edges go downhill exactly by one. Recall that in the residual graph, all capacities are > 0.  To _push (excess) flow_ means decreasing the excess flow of a vertex by sending flow through an outgoing unsaturated downhill edge in the residual network.  That increases the excess flow in the receiving vertex by the same amount.  To _relabel_ a vertex means to increase its height to one unit more than the lowest of its direct successors to which there's an unsaturated pipe.

Intuition: The algorithm starts by setting height of source to |V|, and all other vertices to 0, flooding all out edges of the source to their capacity. The residual graph now is almost identical to the original graph, only that source's edges are reversed. All direct successors of the source are overflowing. We iteratively try to turn all remaining overflowing vertices into non-overflowing vertices, in which case the preflow proofably is the maximum flow. We do that by moving (part of) the excess flow to an direct successor vertex. More concretely by pushing flow downhill through an admissible edge in the residual graph. And if that is not possible because the overflowing vertex has no admissible outgoing edges, by relabeling it.  Ultimatively all excess flow is pushed either to the sink or to the source,  which by definition have no excess flow.  In other words, the algorithm tries to turn the initial preflow into a flow (no more overflowing vertices), which proofably will then be a maximum flow.

*to-do*: why to encode in the description and in the pseudo code that source and think don't have an excess flow?


--------------------------------------------------
push-relabel(G:flownetwork)
  init(G)
  while there is an overflowing node u
    If u has an admissible outgoing edge v in the residual graph: push(u, v)
    else: relabel(u)

// precondition:
//   u is overflowing
//   residual edge (u,v) is admissible
push(u:vertex, v:vertex)
  oe = edge(u,v) in original graph
  re = edge(u,v) in residual graph
  df = min(u.excess, re.capacity)
  if e is an forward edge: oe.flow += df
  else                   : oe.flow -= df
  u.excess -= d
  v.excess += d

// precondition:
//   u is overflowing
//   u has no admissible outgoing edge in the residual graph
relabel(u:vertex)
  u.height = 1 + (minimal height among u's direct successors in residual graph)
--------------------------------------------------

Compare and contrast the push-relabel approach with the augmenting path approach: Augmengting path maintains the invariant feasability, i.e. the conservation and the capacity constraints. The goal is to reach a residual graph where the source and the sink are disconnected; i.e. we are trying to get better and better flows. Push-relabel is the exact opposite: The invariant is that in the residual graph source and sink are disconnected. The goal is restoring feasability, or in other words, transform the preflow into a flow. I.e. push-relabel first relaxes feasability, and then tries to restore it.


References:

- See <<flow_network>>
- CMU, Course 15-451/651 (Algorithms) Fall 2013, Lecture "Network Flows III: Push-relabel flow algorithms, and Min-Cost Max-Flow.": https://www.cs.cmu.edu/~avrim/451f13/lectures/lect1010.pdf[lecture notes]
- Book ``Introduction to algorithms'', chapters ``Push-relabel algorithms'' and ``The relabel-to-front algorithm''.


[[online_algorithms]]
== Online algorithms



=== Page replacement algorithms

There's (slow) data, k faster-than-data pages. A cache _miss_ is when a requested page is not in the cache. Updating cache from data is called _evicting_ (kicking out a page from the cache) and _fetching_ data into the now free page.

No deterministic algorithm is k-competitive.

Page replacement strategies:

- LRU (least recently used): Evict least recently used. Is an conservative algorithm.
- FIFO (first in first out): Evict oldest-loaded page. Is an conservative algorithm.
- LFU (least frequently used): Evict the page that has been requested least frequently.
- MIN or OPT (offline optimal): Evict page to be used farthest in the future. A greedy algorithm. Proofably optimal. Cost +O(n/k)+ (n accesses, k pages). Proof: Only very k accesses there's an evict.

Applications:

- Cachings at all levels of the memory hierarchy


== Misc algorithms

=== Horner's method / Horner's scheme

Task: Evaluate a polynomial P(x)=a~0~ + a~1~x + ... + a~n~x^n^ at x=x~0~.  Solution: Since the polynomial can be rewritten as a~0~ + x (a~1~ + x(a~2~+...+x(a~n~)...)) we can solve it beginning at the deepest level and iteratively go outward: b~n~=a~n~, b~n-1~=a~n-1~+x~0~b~n~, ..., b~0~=a~0~+x~0~b~1~ with b~0~ being the solution.  In code, with b~i~ stored in ++acc++umulator:

--------------------------------------------------
double polynomial(double x, const vector<double>& coefficients) {
    double acc = 0;
    for (int i=coefficients.size()-1; i>=0; --i) {
        acc = coefficients[i] + x * acc;
    }
    return acc;
}
--------------------------------------------------


[[hash]]
=== Hashes
A hash function +h(k,m)+ maps a key +k+ to an integer in the range [0,m), +m+ being an integer, and +U+ is the universe of possible keys. Typically k is an integer of the size of a CPU word. Thus prehashing is used to map any originial key of any size to [0,k).  

References:

- https://courses.csail.mit.edu/6.851/spring12/lectures/L10.html

====  Totally random hash(table) (aka simple uniform hashing, SUHA)
An ideal hash is _totally random_ (aka assumption of _simple uniform hashing_, _SUHA_): Every k from U has the probability 1/m to be mapped to slot s, for every slot s in [0,m). But that requires Θ(U log m) space (a table of U entries, each storing the hash functions's output, i.e. each entry needs Θ(log m)space), which is in general too much.

A _totally random hash table_ T is the lookup table implementing a totally random hash function, i.e. h(x)=T[x].


==== Universal familiy
A family of hash functions H is _universal_ if for every h∈H, and for all x≠y ∈ U, Pr~h~{h(x) = h(y)} = O(1/m).

Examples:

- ++h(x) = [(ax) mod p] mod m++ for 0<a<p, wheras p is a prime. p should be larger than m, else the higher slots are unused. ax can also be vector dot product. Note that this is not exactly the same as the division method.

- ++h(x) = (a · x) >> (lg u − lg m)++. Use lg m high order bits of product ax.

==== Strong universal family
A famility H of hash functions is _strong universal_ if for all x,y ∈ U, x≠y, Pr~h~{h(x) = h(y)} ≤ 1/m.

[[k_wise_independent]]
==== k-Wise Independent family
A family H of hash functions is k-wise independent if for every h ∈ H, and for all
distinct x~1~, ..., x~k~ ∈ U, Pr{h(x~1~)=t~1~ and ... h(x~k~)=t~k~} = O(1/m^k^).

Example pair-wise independent (k=2):

++h(x) = [(ax + b) mod p] mod m++ for 0<a<p and for 0≤b<p. Here, again, p is a prime greater than |U|.

==== Simple tabulation hashing
View key x as vector of characters x~1~, ..., x~c~. We create a totally random hash table T~i~ on each character. ++h(x)=T~1~(x~1~) xor ... xor T~c~(x~c~)++

Is <<k_wise_independent,3-wise independent>>.

==== Division method
+h(k,m)=k mod m+. In practice not so bad if m is prime and not close to a power of two. Still pretty `hackish'.  Rational for m being prime: When m has common factors with k, the effectively used table size gets divided by the product of those factors: ++k=k%m=(a*f1*f2)%(b*f1*f2)=a%b)++


==== Multiplication method
++h(k,m,a,w) =((ak) % 2^w^) >> (w-r)++, where as m=2^r^, i.e. +r=lg(m)+, and the machine stores integers in words of size w bits. a should be odd and not close to a power of two, between 2^r-1^ and 2^r^.

Intuitively: The multiplication mixes up bits, especially in the area from bit (w-r) to w, so we take that area: +% 2^w^+ cuts away the part left of bit w, the shift right cuts way the bits right of bit (w-r).


[[graph_theory]]
== Graph theory
This chapter explains graphs as a mathematical concept. For the corresponding abstract data type, see <<graph_adt>>

A _graph_ G=(V,E) is given by a set of _vertices_ V (aka _nodes_ or _points_) and a set of _edges_ E (aka _lines_), each edge being an pair of elements from V.  The _order_ of a graph G, denoted by |G| is the number of its vertices, i.e. |V|.  The _size_ of a graph G, denoted by ‖G‖, is the number of its edges, i.e. |E|.

An _undirected graph_ is one in which edges are an unordered pairs; the edge ++(a,b)++ is identical to the edge ++(b,a)++.  An _directed graph_ (aka _digraph_) is one in which edges are ordered pairs.  Such an directed edge is also called _arc_, _directed edge_ or _arrow_.  An _outgoing edge_ (_incoming edge_) of vertex v is a directed edge starting (ending) at v. A _successor_ (_predecessor_) is vertex that comes after (before) a given vertex in a directed graph. A _direct successor_ (_direct predecessor_) is an adjacent successor (predecessor).  A _loop_ is an edge which starts and ends on the same vertex.  A _link_ is an edge with two different ends.  _parallel edges_ (aka _multiple edges_ or a _multi-edge_) share the same pair of vertices, and in case of directed graphs, point in the same direction.  _antiparallel edges_ share the same pair of vertices and point in opposite directions.  In an undirected graph the two vertices of an edge are said to be _adjacent_ (or _neihbors_) to each other; in an directed graph only the destination vertex of an edge is adjacent to the source vertex.  Two edges are called _incident_ (or _adjacent_) if they share a vertex.  Pairwise non-adjacent vertices or edges are called _independent_.

The _degree_ (aka _valency_) d~G~(v)=d(v) of a vertex v in an undirected graph is the number |E(v)| of edges incident with v, with loops counted twice. In directed graphs, _out degree_ (_in degree_) is the number of outgoing (incoming) edges. A vertex of degree 0 is called _isolated_ (or _disconnected_), a vertex of degree 1 is called an _end-vertex_.  The number δ(G) := min { d(v) | v ∈ V } is the minimum degree of G, the number Δ(G) := max { d(v) | v ∈ V } its maximum degree. If all the vertices of G have the same degree k, then G is _k-regular_, or simply _regular_.  The _average degree_ d(G) of G is the average degree of G's vertices.

A _simple graph_ is an undirected graph that has no loops (mind: a loop is not a cycle) and no parallel edges.  A _trivial graph_ is a graph of order 0 or 1.  A _multigraph_ is one where +E+ is a multiset (as opposed to just a set).  Alternatively: a multigraph is allowed to have parallel edges.  A _sparse_ graph is one for which +|E|+ is much less than ++|V|^2^++.  A _dense_ graph is one for which +|E|+ is close to ++|V|^2^++.  A _null graph_ is one with no edges. The _empty graph_ ∅=(∅,∅) has no vertices and no edges.

A _vertex cover_ is a set of vertices such that each edge of the graph is incident to at least one vertex of the set.  An _independent set_ (aka _stable set_) is a set of vertices, no two of which are adjacent.  Given a simple Graph G, it's _complement_ graph has the same vertices, but only an edge between any two vertices iff they are not adjacent in G.

A _walk_ is a set of consecutive edges.  A _closed walk_ is one where the first and last vertices are the same, an _open walk_ is one where they are different.  A _trail_ is a walk with distinct edges.  A _circuit_ is a closed trail.  A _path_ is a walk with distinct vertices (which implies distinct edges), except possibly the first and the last.  Some authors call that a _simple path_, and use the term path as a synonym to walk.  Given a subgraph H of graph G, we call a path P an _H-path_ if it meets H only exactly at its two end vertices, i.e. no other common vertices, and no common edges.  A _cycle_ is a path in which the first and last vertices are identical.  An _open path_ is a path in which the first and last vertices are distinct.  Two paths are _vertex-independent_ (aka _vertex-disjoint_) if they do not have any vertices in common (*to-do* some sources are clear that they only mean inner vertices, other sources seem to include also end vertices).  Two paths are _edge-independent_ (aka _edge disjoint_) if they do not have any edges in common.  The _path weight_ is the sum of the weights of its constituent edges.  The _length_ of a path is sometimes defined as the number of edges on the path or as synonym to path weight.  The [[shortest_path]]_shortest path_ from vertex u to v is any path with minimal path weight.  See also <<shortest_path_problem>>.  The _shortest path weight_ is the path weight of the shortest path; defined to be infinite if there is no path, and often defined as -infinite it contains a negative-weight cycle.  Any sub path of a shortest path is itself a shortest path.  The _distance_ between two vertices is the length of the shortest path.  A _(pre/in/post)order tree walk_ does the key action with the current node's payload before/between/after recursively calling the children.

Given a graph G, a _subgraph_ of G is a graph, each of whose vertices and edges belong to G.  Given a graph G, an _induced subgraph_ is a subgraph of G, whose edge set consists of all G's edges that have both endpoints in the subgraph.  A graph is _edge-maximal_ with a given graph property if G has the property, but no Graph resulting from adding an (non-parallel) edge does.

[[DAG]]
A _directed acyclic graph_ (_DAG_) is a graph with no directed cycles.  Given a directed edge C->P, C is the _child_ and P the _parent_. Nodes reachable from C are C's _ancestors_, nodes which can reach P are P's _descendants_.  Whether or not a node is it's own ancestor/descendant is not defined across literature. CLRS say yes.  The _lowest common ancestor_ (_LCA_, aka, less accurately, _least common anchestor_) of two nodes x and y is the first common ancestor in topo order.

[[tree_graph_theory]]
See also <<tree_ADT>>. A _tree_ is an undirected graph in which any two vertices are connected by exactly one path (which then naturally is a simple path).  Alternatively: an undirected connected acyclic graph.  Alternativly:  An undirected connected graph where every edge is a bridge.  Thus it's always undirected, acylic, connected and bipartite.  Note that an arborescence is not a connected graph (consider a root with two childs; the two childs are not connected).  In informatic context, the term tree is often used for what we call here an directed rooted tree.  A _rooted tree_ is a tree in which one vertex has been designated the root.  An _arborescence_ (aka _directed rooted tree_ or _out-tree_) is a directed rooted tree in which all edges point away from the root.  An _anti-arborescence_ (aka _in-tree_) is an arborescence where all edge directions are reversed.  A _forest_ is a disjoint union of trees.  Alternatively: an acyclic undirected graph.  An _ordered tree_ (aka _plane tree_) is a rooted tree in which an ordering is specified for the children of each vertex.  A _poly tree_ is a directed acyclic graph which would be a tree if it's edges were undirected.  A _spanning tree_ of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G.  A [[MST]]_minimum spanning tree_ (aka _MST_) of is a spanning tree of G with the minimal total weighting for its edges.  See also the <<MST_problem>>.  The equivalent to a MST in an directed graph is a _spanning arborescence of minim weight_ (aka _optimum branching_).

Common graph problems:

- _single-pair shortest path_ problem (from single source to a single destination)
- _single-source shortest path_ problem (from a single source to all others)
- _single-destination shortest path_ problem (from all others to a destination)
- _all-pairs shortest path_ problem (from all to all)
- <<maximum_flow_problem>>
- The chapter <<NP_complete>> also lists some graph problems.

References:

- http://en.wikipedia.org/wiki/Glossary_of_graph_theory


[[hamiltonian_cycle]]
[[hamiltonian_path]]
=== Hamiltonian path / cycle

An _Hamiltonian path_ (aka _traceable path_) is a path in an undirected or directed graph that visits each vertex exactly once.  A _Hamiltonian cycle_ (aka _Hamiltonian circuit_, _vertex tour_ or _graph cycle_) is a Hamiltonian path that is a cycle.  A graph that contains a Hamiltonian path is called a _traceable graph_.  A graph is _Hamiltonian-connected_ if for every pair of vertices there is a Hamiltonian path between the two vertices. A graph that contains a Hamiltonian cycle is called a _Hamiltonian graph_.

Properties:

All Hamiltonian graphs are biconnected, but a biconnected graph need not be Hamiltonian.

A tournament (with more than two vertices) is Hamiltonian iff it is strongly connected.

Common problems: <<hamiltonian_path_problem>>, <<hamiltonian_cycle_problem>>, both NP-complete.


[[eulerian_circuit]]
[[eulerian_trail]]
=== Eulerian trail / circuit

An _Eulerian trail_ (aka _Eulerian path_ or _Euler walk_) is a trail in an undirected or directed graph (including multigraphs) that uses all the Graph's edges.  If such as walk exists, the graph is called _traversable_ (aka _semi-eulerian_).  An _Eulerian cycle_ (aka _Eulerian circuit_, or _Eulerian tour_) is a circuit that uses all the Graph's edges.  If such a cycle exists, the graph is called _Eulerian_ or _unicursal_.

Properties:

An connected undirected graph has an Eulerian cycle if and only if every vertex has even degree.

An connected undirected graph has an Eulerian trail if and only if exactly zero or two vertices have odd degree.

A directed graph has an Eulerian trail if and only if at most one vertex has (out-degree) − (in-degree) = 1, at most one vertex has (in-degree) − (out-degree) = 1, every other vertex has equal in-degree and out-degree, and all of its vertices with nonzero degree belong to a single connected component of the underlying undirected graph.

A connected directed graph has an Eulerian cycle if and only if its strongly connected and every vertex has equal in degree and out degree. Equivalently, a directed graph has an Eulerian cycle if and only if it can be decomposed into edge-disjoint directed cycles and its strongly connected.

Problems:

- Finding Eulerian trails:
  * Fleury's algorithm find's Eulerian trails or cycles in +O(E²)+
  * Hierholzer's algorithm find's Eulerian cycles linear time: +O(E)+.

Applications:

- Eulerian trails are used in bioinformatics to reconstruct the DNA sequence from its fragments.
- They are also used in CMOS circuit design to find an optimal logic gate ordering.
- There are some algorithms for processing trees that rely on an Euler tour of the tree (where each edge is treated as a pair of arcs).

Trivia: Was first discussed by Leonhard Euler while solving the famous _Seven Bridges of Königsberg_ problem in 1736.


=== Connectivity

Two vertices v and u are called _connected_ if there is a path from v to u, otherwise they are called _disconnected_.  A _connected graph_ is one where every pair of vertices is connected.  A directed graph is considered a connected graph if for every pair of vertices there is a path in either direction.  A _weakly connected_ graph is a directed graph which would be a connected graph if the edges were taken to be undirected.  A directed graph is _strongly connected_, if for every pair there's a path in both directions.  A _connected component_ (or just _component_) of an undirected graph is a subgraph which is connected and is not connected to any vertices of the supergraph.  A directed graph is called _strongly connected_ (or _strong_) if it contains a path from u to v and from v to u for every pair of vertices u and v.  A _strongly connected component_ of a directed graph G is a subgraph that is strongly connected, and is maximal with this property: no additional edges or vertices from G can be included in the subgraph without breaking its property of being strongly connected.  If each strongly connected component is contracted to a single vertex, the resulting graph is a directed acyclic graph, the _condensation_ of G.  A _cut_ is a partition of V.  A _vertex cut_ (aka _separating set_) of a connected graph G is a set of vertices whose removal renders G disconnected.  An _articulation point_ (aka _cut point_, _separating vertex_ or _cut vertex_) is a vertex in a connected (sub)graph whose removal would disconnect the (sub)graph.  A _bridge_ is an edge in a connected (sub)graph whose removal would disconnect the (sub)graph.  An edge can only be a bridge if it is not contained in an cycle.  A set of edges _respects_ a cut if no edge in that set croses the cut.  An edge is a _light edge_ crossing a cut if its weight is the minimum of any edge crossing the cut.  The _connectivity_ (or _vertex connectivity_) κ(G) is the size of a minimal vertex cut, or in other words, the minimum number of nodes to be removed to disconnect the graph.  A graph is called _k-connected_ (or _k-vertex-connected_) if its vertex connectivity is k or greater, or in other words, if it remains connected whenever fewever than k vertices are removed.  2-connectivity is also called _biconnectivity_, and 3-connectivity is also called _triconnectivity_.

A _complete graph_ with n vertices, denoted K~n~, is a simple undirected graph in which every pair of distinct vertices is connected by a unique edge.  A _tournament_ is a directed graph obtained by assigning a direction for each edge in an complete graph.  A _clique_ in an undirected graph is a subset of its vertices such that every two vertices in the subset are connected by an edge.  A _biconnected graph_ is a connected graph that has no articulation points.  A _biconnected component_ (aka _block_ or _2-connected component_) is a maximal (*to-do*: what does maximal mean here?) biconnected subgraph.  Note a biconnected component, unlike connected components, can be connected to other parts of the supergraph.  A _block graph_ (aka _clique tree_) is an undirected graph in which every biconnected component (aka block) is a clique.

_Menger's Theorem_: Vertex connectivity: k-connected vertices are connected by k internally vertex-disjoint paths.

Motivation: Connectivity measures fault tolerance of a network. E.g. how many connections can fail without cutting off communications.


[[flow_network]]
=== Flow networks

A _flow network_ (aka _transportation network_) is a directed connected simple graph with no antiparallel edges.  One vertex is denoted the _source_ and one vertex is denoted the _sink_.  Each edge has a _capacity_ ≥ 0 and a _flow_. The _capacity constraint_ dictates that the flow is ≥ 0 and cannot exceed the capacity.  The _flow conservation constraint_ dictates the the amount of flow into a node must equal the amount of flow out of it, unless the node is the source or the sink.

The _residual graph_ G~f~ induced by flow f conceptually has two edges for every edge in G. The forward (same direction as in G) edges capacity is c~e~-f~e~, the backward edges capacity is f~e~. However edges in G~f~ with capacity 0 are removed. The residual graph is almost a flow network just like the original graph, with the small difference, that now antiparallel edges are allowed.  An _augmenting path_ p is a path in the residual graph vom source to sink. The _residual capacity_ of such a path p is the minimal edge capacity on p.  An edge on p is said to be _critical_ if its capacity equals the residual capacity of p.

A __s-t edge cut__ is a partition of the graph's vertices into set S and T, S containing source and T containing sink.  The __edge-cut capacity__ is the sum of the capacities of edges from S to T, thus ignoring edges from T to S. The _net flow_ f(S,T) across the cut (S,T) is the sum of the flows of edges from S to T, minus the sum of the flows of edges from T to S.

Lemma: Given a flow in a flow network, the flow equals the net flow of any s-t cut. Figuratively: Any cut partitions the network in two networks, one containing the source, the other the sink.  The two groups are connected by two pipe bundles.  It's intuitive that the flow of these two pipe bundles equals the flow from source to sink.

[[max_flow_min_cut_theorem]]
__Max-flow min-cut theorem__: The following three are equivalent, given a graph G and a flow f: 1) f is a maximum flow in G 2) The residual network induced by f contains no augmenting path 3) There is a cut such that it's capacity equals the flow f. In other words, the maximum flow equals the capacity of the cut with the minimal capacity. Intuitively that becomes apparent having the previous lemma in mind: Since the capacity of a s-t cut is an upper limit of G's maximum flow, the maximum flow is bounded by the cut with the minimal capacity.

Convertions: A multi-source multi-sink flow network can easily be transformed to a single-source single-sink flow network by adding a consolidated source and a consolidated sink. Given an antiparallel edge pair, replace one of the two edges by figuratively inserting a new vertex in its middle, splitting the edge in two edges.  Given an undirected graph, replace each edge with an antiparallel edge pair, and then replace those as described bevore.

Problems: <<maximum_flow_problem>>

References:

- MIT course 6.046J, Design and Analysis of Algorithms (Spring 2015), Lecture 13 "Incremental Improvement: Max Flow, Min Cut":  https://www.youtube.com/watch?v=WwMz2fJwUCg&t=603s[video], https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-notes/MIT6_046JS15_lec13.pdf[lecture notes]

- CMU, Course 15-451/651 (Algorithms) Fall 2013, Lecture "Network Flows and Matchings I": https://www.cs.cmu.edu/~avrim/451f13/lectures/lect1003.pdf[lecture notes]

- Book ``Introduction to algorithms'', chapter ``flow networks''.


== Computational Geometry

A subset S of the plane is called _convex_ iff for any pair of points p,q ∊ S the linesegment pq is completely contained in S.  Given a set S, the _convex hull_ 𝒞ℋ(S) is the smallest convex set that contains S.  Alternatively, the convex hull is the unique convex polygon whose vertices are points from S and that contains all points of S.

__Determining the orientation of an ordered point triple (p1, p2, p3)__: Intuition: If slope(p2, p3) > slope(p1, p3), the orientation is clockwise. Using the formula for slope, Δy/Δx, and simple transformations delivers the formula used in the pseudo code below. Alternatively, one can look at `cross product' (when defined as just the determinant, rather than a vector) of (p1-p2) ⨯ (p3-p1).

----------------------------------------------------------------------
enum Orientation { colinear, clockwise, counterclockwise  }
orientation(p1: point, p2: point, p3: point): Orientation
  val = (p2.y - p1.y) * (p3.x - p2.x) -
        (p2.x - p1.x) * (p3.y - p2.y);
  if val==0: return colinear;
  if val>0 : return clockwise;
  else     : return counterclockwise;
----------------------------------------------------------------------

_Order points p2 and p3 around a point p1_: From the viewpoint of point p1, determine wether point p2 is right of point p3.  Or equivalently, from the viewpoint of p1, determine  wether line (p1, p2) is right of line (p1, p3).

----------------------------------------------------------------------
is2ndRightOf3rd(p1: point, p2: point, p3: point): bool
  return orientation(p1, p2, p3) == counterclockwise
----------------------------------------------------------------------

_Determine whether two line segments intersect_: Assumption for simplicity: no three point lie on a common line.  The general case is not significantly more complex.  Two line segments (p1, p2) and (q1, q2) intersect iff p1 and p2 are on opposite sides of line (not segment) (q1, q2) and q1 and q2 are on opposite sides of the line (p1, p2). p1 and p2 are on opposite sides of the line (q1, q2) iff exactly one of the two triples (p1, q1, q2) and (p2, q1, q2) is in counterclockwise order.

----------------------------------------------------------------------
// returns true if segment (p1, p2) and (q1, q2) intersect
intersect(p1: point, p2: point, q1: point, q2: point): bool
  if orientation(p1, q1, q2) == orientation(p2, q1, q2): return false
  if orientation(q1, p1, p2) == orientation(q2, p1, p2): return false
 return true
----------------------------------------------------------------------

In _sweeping_, an imaginary vertical sweep line pases through the given set of geometric objects, usually from left to right.  We treat the spatial dimension that the sweep line moves across, in this case the x-dimension, as a dimension of time.  Sweeping provides a method for ordering geometric objects.

References:

- Book "Computational Geometry - Algorithms and Applications"

- Book "Introduction to algorithms", chapter "Computational Geometry".

- MIT course 2.158J / 1.128J / 16.940J "computational geometry"

  * spring 2003: https://ocw.mit.edu/courses/mechanical-engineering/2-158j-computational-geometry-spring-2003/

- MIT 6.838 "Geometric Computation"

  * fall 2003: https://people.csail.mit.edu/indyk/6.838/

- http://jeffe.cs.illinois.edu/teaching/373/notes/allnotes.pdf, Non-lecture notes at the end.


=== Convex hull

The convex hull problem asks for the set of points forming the convex hull, given a set of points.  Often the algorithms return the convex hull not only as a set of points, but as a sequence of points constituting the convex hull polygon.


==== Jarvis's algorithm

_Jarvis's algorithm_ (or _Jarvis's march_, or _gift wrapping algorithm_) finds the convex hull of a set of points in O(nh). h is the number of points on the convex hull.

Intuition: Start by finding any point which is known to be on the convex hull, e.g. the leftmost point.  An outer loop keeps adding the proper next point to the hull sequence until the hull cycle is closed.  An inner loop finds this next point the same way one finds the maximum element in a set.  Here, the `maximum' is the point which is furthest to the right from the perspective of the current point, looking at the other points.  Note that Jarvis's march resembles selection sort.  Both repeatedly find the item that goes in the solution's next slot.

Assumptions for simplicity: no three points lie on a common line, rounding error problems ignored, points.size <= 3 ignored.

----------------------------------------------------------------------
jarvis_march(points, hullpoints[out])
  leftmost = iteratively search leftmost point in points
  current = leftmost
  do
    hullpoints.append(current)
    tentativenext = any point but current
    for each point i in points
      if is2ndRightOf3rd(current, i, tentativenext)
        tentativenext = i
    current = tentativenext
  until current = leftmost
----------------------------------------------------------------------

Analysis: O(nh) time, where h is the number of points on the convex hull.  It's thus an output-sensitive algorithm. Proof: The outer iteration makes O(h) iteretions.  The inner loop makes O(n) iterations.


References:

- https://www.geeksforgeeks.org/convex-hull-set-1-jarviss-algorithm-or-wrapping/

- http://jeffe.cs.illinois.edu/teaching/373/notes/x05-convexhull.pdf

- https://www.geeksforgeeks.org/convex-hull-set-1-jarviss-algorithm-or-wrapping/


==== Graham's scan

_Graham's scan_ finds the convex hull of a set of points in O(n log n).

Assumptions for simplicity: no three points lie on a common line, rounding error problems ignored, points.size <= 3 ignored.  

Find any point on the convex hull, e.g. the left-most point.  Sort the remaining points in counterclockwise order around that left-most point.  Connect all points in counterclockwise order, starting at the left-most point, resulting in a simple polygon with n vertices.

We transform the polygon into the convex hull using the `three-penny algorithm'.  We have three pennies, which sit on three consecutive vertices of the current polygon.  Initially, the first of the three pennies is on the left-most point.  We now iteratively apply the following two rules until a penny is moved forward onto the left-most point:

* If p1 p2 p3 are in counterclockwise order, move the penny on p1 to p3's successor.  Intuitively, we `advance' on the polygon because (p1, p2, p3) could be part of the convex hull because they are in counter clockwise order.

* Else, remove p2 from the polygon, add the edge (p1, p3) to the polygon, and move the penny on p2 to p1's predecessor.  Intuitively, we found a dent in the polygon and remove that dent.

--------------------------------------------------
three-penny(p: polygon)
  create a steack, push p[0], p[1], p[3]
  for i = 3 to p.size-1
    while orientation(stack.nextToTop(), stack.top(), p[i])==clockwise
      stack.pop()
    stack.push(p[i])  
  return stack
--------------------------------------------------

Analysis: O(n log n) time. Proof: The initial sorting takes O(n log n) time.  The three penny algorithm takes O(n) time:  The first rule moves a penny to a vertex that has never seen a penny before, so the first rule is applied at most n-2 times.  The 2nd rule removes a vertex from the polygon, so the 2nd rule is applied exactly n-h times.  h is the number of points on the convex hull.

References:

- http://jeffe.cs.illinois.edu/teaching/373/notes/x05-convexhull.pdf

- Chapter "Convex Hulls" in book "Computation Geometry - Algorithms and Applications". However, there a pretty diffrent version of Graham's scan is presented.

- Chapter "33.3 Finding the convex hull" in book "Introduction to Algorithms"


=== Determining whether any pair of segments intersects

Given a set of line segments, determine whether any pair of segments intersects.

==== Sweep line

Assumptions for simplicity: No input segment is vertical.  No three input segments intersect at a single point.

The general idea is to only test those segments for intersection which are `close'.  The sweep line paradigm, moving it horizontally, ensures avoiding checking segments which don't overlap in x-direction.  Segments currently intersecting the sweep line are ordered after their intersection points with the sweep line, top to bottom.  Obviously those segments do overlapp in x-direction.  Among these x-overlapping segments, we only check those segment pairs for actual intersection, which are adjacent on the sweep line, i.e. which are adjacent in y-direction.

Create an _event point schedule_, which is a sequence of points, called _event points_. Here, the endpoints of the segments constitute the event points.  The ordering is from left to right, breaking ties left endpoint before right endpoint, breaking further ties top to bottom.

Process the event point schedule.  Imagine sweeping a vertical line from left to right, stoping at each event point.  We maintain a top-to-bottom ordered sequence of those line segments which currently intersect the sweep line.  That sequence is called _sweep-line status_.  The sequence can only change when the sweep line encounters and endpoint or an segment intersection.  When the sweep line encounters an left endpoint, the associated segment is inserted into the sequence.  We test whether the new segment intersects with any of its two neighbors.  When the sweep line encounters an right endpoint, remove its associated segment from the sequence.  We test whether the two new neighbors intersect.  Of course, whenever an intersection is found, we can terminate the algorithm.

--------------------------------------------------
any-intersections(points): bool
  eventschedule = points sorted from left to right
  // sweep line status, using isBelow (defined below) as comperator.
  sls = ∅
  for each point current_point in eventschedule:
    current_segment = segmentof(current_point)
    // for simplicity, successor / predecessor returning NIL is ignored
    if current_point is left endpoint:
      insert(sls, current_segment)
      if intersect(current_segment, successor(swt, current_segment)) or
         intersect(current_segment, predecessor(swt, current_segment)):
        return true
    else
      if intersect(successor(swt, current_segment), predecessor(swt, current_segment)):
        return true;
      remove(sls, current_segment)  
  return false
--------------------------------------------------

*to-do*: isBelow is only valid for a specific x-position of the sweep line, no? How can it then be used as comperator for the sweep line status datastructure? I believe it only works if left(s2).x <= left(s1).x <= right(s2).x.  Thus maybe first sort s1 s2 after left().x, and only then do the core comparision?

--------------------------------------------------
// returns true if leftendpoint(s1).x is below s2 at x-coordinate leftendpoint(s1).x
// precondition: leftendpoint(s2).x <= leftendpoint(s1).x <= rightendpoint(s2).x
isBelow(s1: segment, s2: segment)
  return orientation(leftendpoint(s2), rightendpoint(s2), leftendpoint(s1)) == clockwise
--------------------------------------------------

The data structure for the sweep line status should support the following operations in O(log n) time: insertion, deletion, find successor / precessor.  For example some sort of binary search tree.

Analysis: The running time is O(n log n).  The initial sorting takes O(n log n).  The sweep line stops at most at 2n endpoints, thus there are O(n) iterations.  Each operation on the sequence on the sequence requires O(log n). So the loop as a whole needs O(n log n), and the algorithm as a whole also O(n log n).

References:

- Book "Introducion to algorithms", subchapter "3.2 Determining whether any pair of segments intersects"

- http://jeffe.cs.illinois.edu/teaching/373/notes/x06-sweepline.pdf
 

=== Find all segment intersections

References:

- Book "Computational Geometry - Algorithms and Applications", chapter "line segment intersection"

- http://courses.csail.mit.edu/6.006/spring11/lectures/lec24.pdf

- https://people.csail.mit.edu/indyk/6.838/handouts/lec2.pdf

- https://www.youtube.com/watch?v=dePDHVovJlE for the case of orthogonal line segments


== Problems

=== Overview

--------------------------------------------------
Knapsack
  unbounded knapsack problem (UKP) | DP: time O(n*S) | approx: greedy algo: O(n)
    rod cutting problem: is the same problem
  bounded knapsack problem (BKP) | can be reduced to 0-1 knapsack
  0-1 knapsack problem | DP: time O(n*S)
  continuous/fractional knapsack problem | greedy algo: time: O(n*lg(n))
  coin change problem | greedy (optimal only for canonical coin systems): O(n*lg(n))

Longest common subsequence
TSP. Travelling purchaser problem
edit distance
longest path
shortest path
minimum spanning tree. directed and undirected version
cycle dedection
topo sort
sort
cutting stock problem
bin packing problem
assignment problem
bipartite matching

--------------------------------------------------


[[TSP]]
=== Travelling salesman problem (TSP)
In an weighted graph (directed or undirected), the TSP is finding the path with mimimum weight visiting each vertex exactly once and start vertex being the end vertex.

TSP is a special case of the travelling purchaser problem.  

TSP is NP-complete.

Algorithms:

- Exact: Held–Karp, a dynamic programming algorithm.
- Exact: Various branch-and-bound algorithms
- Exact: ... linear programming ...
- Approximations: *to-do*

Applications: *to-do*


=== Travelling purchaser problem
*to-do*


[[MST_problem]]
=== Minimum spanning tree problem

The problem of finding the <<MST>>, actually minimum spanning forest, in an undirected weighted possibly disconnected graph.

Algorithms:

- <<Kruskal>>'s algorithm

- <<Prim>>'s algorithm (restricted to connected graphs)

References:

- Book ``Introduction to algorithms'', chapter ``Minimum Spanning Trees''.



[[edit_distance]]
=== Edit distance
Given two strings x and y, the edit distance is the minimum cost series of
edit operations that transform x into y.  There are cost tables:
cost-deletete[c] is cost to delete char c from x, cost-insert[c] is cost to
insert char c into x, cost-replace[c1, c2] is cost to replace char c1 by
c2. Doing nothing modeled by cost-replace[c,c].

<<dynamic_programming>>:

1. suproblems: All possible suffixes of x and y.  I.e. edit distance on x[i:]
   and y[i:] for all i∊[0,|x|) and j∊[0,|y|).

2. guess: In each step, there are three choices: ① replace x[i] by y[j] (do
   nothing is modeled by replacing c by c) or ② insert (prepend) y[j] to x or
   ③ delete x[i].  The general idea is to consume the first character of x
   and/or y in order to 1) make first char of x and y equal and to 2) be left
   with a subproblem (to make progress at least one char needs to be
   consumed).

3. recurrence: 
+
-----
DP(i,j) =
  if i=|x| and j=|y|: ④ 0
  else: min(
  ① cost-replace[x[i],y[j]] + DP(i+1, j+1)  if i+1≤|x| and j+1≤|y|,
  ② cost-insert[y[j]]       + DP(i  , j+1)  if             j+1≤|y|,
  ③ cost-delete[x[i]]       + DP(i+1, j  )  if i+1≤|x|            )
-----
+
④ is the base case (aka smallest subproblem), which is the edit distance to
   transform the empty string to the empty string, which obviously is 0.

4. Description of subproblem DAG: Imagine a matrix, each cell represents a
   vertex in the DAG and thus also represents DP(i,j). It has |x| rows indexed
   by i, and |y| columns, indexed by j. Thus the top left cell/vertex is the
   original problem (edit distance to transform x into y), and bottom-right
   cell/vertex is the base case ④.  The weight of the edges are the respective
   cost-x[…] term in the DP formula of step 3.  Optionally each cell/vertex
   can have a value attribute which then is DP(i,j).
+
Example: x=FLO and y=FOO:
+
-----
             outgoing edges of each matrix-cell / DAG-vertex
   FOOε      the cells in the left-most column and bottom-most row
   0123 j    naturally don't have edges leaving the matrix
F 0R···      ☐→① insert
L 1····      ↓ ↘③ replace
O 2····      ② delete  
ε 3···④      
  i          R means root of the DAG, i.e. the original problem
-----
+
bottom-up approach: Solve the subproblems by starting in the bottom
right corner and then going left and/or up.  E.g: ++for i=|x|⋯0: for
j=|y|⋯0: …++.
+
time complexity: #subprobs=Θ(|x|⋅|y|) (number of cells). time/subproblem = Θ(1). Overall running time =
  #subprobs⋅time/subproblem = Θ(|x|⋅|y|).
+
Space complexity: Θ(|x|⋅|y|) (number of cells) for a trivial implementation.
If only a sliding window of one row or column, which ever of |x| or |y| is
smaller, is kept, the space complexity becomes Θ(min(|x|,|y|)).

5. The original problem is DP(0,0).


*to-do*:

- Most sources on the net seem to solve it in terms of making the subproblems
  prefixes, opposed to suffixes as above.  So my matrix above doesn't match
  moste of the pictures / drawings found on the net.
- Backtracing / make the operations needed available to the caller

Applications:

- computational biology: quantify similarity of DNA sequences
- correction of spelling mistakes, i.e. which correct word is the most likely


=== Longest common subsequence (LCS)
Given a set of sequences, typically two, what is (are) the longest common subsequence(s) -- The solution might not be unique, i.e. multiple subsequences of same lenght will qualify as having the longest lenght.  Note that unlike substrings, subsequences are not required to occupy consecutive positions.

In general: NP-hard.

For two sequences: Equals the <<edit_distance>> problem, with cost of insert and delete being 1 and replace being 0 for c→cʹ and ∞ otherwhise.

Applications:
- file comparison, e.g. the diff utility
- bio informatics: as a measure how similar DNA sequences are (the longer the LCS the more similar),


[[knapsack]]
=== Knapsack

0-1 knapsack problem:: Given a set of n items, each item i with a weight w[i]
(an integer) and a value v[i], determine the items to include in a collection
so that the total weight is less than or equal to a given limit S (an integer)
and the total value is maximal.

Bounded knapsack problem (BKP):: Removes the restriction that there is only
one of each item, but restricts the number of copies of each item i to c[i].

Unbounded knapsack problem (UKP):: Places no upper bound on the number of
copies of each kind of item.

Change-making problem:: How can a given amount of money be made with the least
number of coins of given denominations. Similar to UKP, however capacity of
knapsack has to be hit exactly. `weight of item' corresponds to `value of
coin', and `value of item' is always -1.

[[rod_cutting_problem]]
Rod cutting problem:: Same as UKP. rod length -> knapsack capacity, length i
-> item i having a weight of i, value of length i -> value of item i.

Fractional/continuous knapsack problem:: Instead of items we think of
materials.  There is an certain amount (weight) of each material, and we can
pack any amount less than that per material into the knapsack. Solution: sort
materials descendinding by value/weight, then greedely take of each material
as much as possible until the knapsack is full. O(n*lg(n)).


Solution for the 0-1 knapsack problem using <<dynamic_programming>>:

Put the items in some sequence.

1. Suproblems: All possible suffixes of the item sequence (items[i:]) × all possible remaining capacities X≤S.

2. Guessing: In each step, there are two choices: ① shall I include item i (aka current/front item) or ② shall I not?  

3. Recurence:
+
--------------------------------------------------
DP(i,X) =
  if i=n: ③ 0
  else: max(
  ①        DP(i+1, X)       if i+1≤n            ,
  ② v[i] + DP(i+1, X-w[i])  if i+1≤n and w[i]≤X )
------------------------------------------------------------
+
③ is the base case, which is the knapsack problem for an empty set of items
and whatever remaining capacity: the maximal value is obviously 0.

4. Description of the subproblem DAG: Imagine a matrix, each cell represinting
   a vertex in the DAG and thus also represents DP(i,X).  It has n+1 columns
   indexed by i, and S+1 rows, indexed by X.  Thus the top left cell/vertex is
   the original problem (knapsack for all items and capacity S).  The right
   column are the base cases.
+
Example: n=3 items, capacity S=4:
+
----------------------------------------------------------------------
c    item
a    0124     outgoing edges of each matrix-cell / DAG-vertex:
p   4R··③     ☐→① Don't include item i. Edge-weight 0.
a   3···③      ↘② Include item i, which removes w[i] from capacity X.
c   2···③         Edge-weight -v[i].
i   1···③     
t   0···③     R means root of the DAG, i.e. the original problem
y   
----------------------------------------------------------------------
+
Bottom-up approach: Solve subproblems by starting in the bottom right corner
and then going left and/or up: E.g.: ++for i=n⋯0: for X=0⋯S: …++.  Space
complexity can be improved by only using a sliding window of two columns.
Note that the top-down approach doesn't need to calculate all n*S vertices; it
only calculates the ones reachable from DP(0,S).
+
time complexity: Θ(1) for one DP call. Thus the overall running time is +Θ(n*S)+,
i.e. <<pseudo_polynomial>>. It's exponential, since +Θ(n*S)+ is exponential
relative to the input size which is +O(n*lg S)+ (think how many bits you need
to represent the input).

5. The original problem is DP(0,S).

6. The items to be included into the knapsack are (for non-zero weights).
--------------------------------------------------
X = S
for i in [0,n)
  if DP(i, X) = v[i] + DP(i-1, X-w[i]): // i.e. if choice ② was made
    remember item i as included in knapsack
    X -= w[i]
--------------------------------------------------

Applications:

*to-do*

*to-do* process https://en.wikipedia.org/wiki/List_of_knapsack_problems


[[shortest_path_problem]]
=== Shortest path problem
The problem of finding a <<shortest_ path>> (there might be multiple shortest paths) in a graph. Recall that the shortest path is undefined, or it's weight is -INFINIT, if it contains negative-weight cycles, because one can always make an allegedly `shortest path' shorter by walking through the cycle one more time.

optimal substructure: The shortest path problem has optimal substructure. A subpath of a shortest path is itself a shortest path; if it wasn't, we could replace it by the alegedly shorter path and thus make the overall path shorter.

Applications: *to-do*


==== Single source shortest path problem
general graph: <<Bellman_Ford>> +O(V*E)+

non-negative weights: <<Dijkstra>> +O(E+V*lg(V))+

DAG: <<toposort>>, then for each node, for each neighbor, relax. +Θ(E + V)+

unweighted graph: <<BFS>> +O(E+V)+

integer weights: Thorup *to-do*


==== Single destination shortest path problem
Can be reduced to single source shortest path by reversing direction of each edge.


==== Single pair shortest path problem
All known algorithms for this problem have the same worst case asymptotic running time as the best single source algorithms.

non-negative weights: <<A_star>>


==== All pairs shortest path problem
general graph (neg weights, dedects neg. cycles): <<floyd_warshall>> in O(|V|^3^)


=== Longest path problem
*to-do*


=== Graph coloring
*to-do*
Relation to four color problem?


[[toposort]]
=== Topological sort
A _topological sort_ (aka _topsort_, _toposort_, _topological ordering_) of a directed acyclic graph (DAG) is a linear ordering of its vertices such that for every directed edge (u,v) from vertex u to vertex v, u comes before v in the ordering.  Toposort is possible only for DAGs; see <<cycle_dedection>> for how to check.

Algorithm: Augment the <<DFS_all_source,DFS-all-source>> algorithm: when an vertex v is finished, insert it onto the front of the output sequence.  Time complexity +Θ(V+E)+.


[[cycle_detection]]
=== Cycle detection

Algorithms:
- <<DFS>> finds a <<DFS_edge_classification,back edge>>.
- Rocha–Thatte, a distributed algorithm.

Applications:
- Dedect cycles (i.e. problems) in a dependency graph


[[hamiltonian_path_problem]]
[[hamiltonian_cycle_problem]]
=== Hamiltonian path / cycle problem

The __<<hamiltonian_path>> problem__ and the __<<hamiltonian_cycle>> problem__ are problems of determining whether a Hamiltonian path or a Hamiltonian cycle exists in a given graph (whether directed or undirected).  Both problems are NP-complete.


[[LCA_problem]]
=== Lowest common ancestor (LCA) problem
See <<DAG>> for the definition of LCA.

Solutions:

- With O(n^2^) space, O(1) time is trivial: An O(1) time lookup table stores the answer for all input pair vertices.

- Tarjan's off-line LCA: After preprocessing the DAG, solvable in O(1) time and O(n) space.

- If DAG is a cartesian tree: reduce to array, see <<cartesian_tree>>, and solve <<RMQ>> there. *to-do*: how exactly, there we're rediricted to LCA


[[LA_query]]
=== Level ancestor problem / query

Given a rooted tree and a node v, LA(v,d) delivers the ancestor node of v which is at depth d, or equalently, delivers the kth ancestor (where parent is the 1st ancestor), k=height(node)-d.

When n is the number of nodes, after preprocessing which takes O(n) time and O(n) space, solvable in O(1) time.


[[rectangular_range_query]]
=== Orthogonal (rectangular) range (interval) searching (query)

Given a set of points in k-dimensional space, find the ones being in a k-dimensional range.  E.g. when doing a database query, we want to find the persons with age in [20,40] and height in [150, 200] and weight in [50,100].

Given a set of k-dimensional geomteric objects (aka ranges), find the ones which intersect (aka overlap).

Data structes / Algorithms:

- <<kd_tree>>

- <<range_tree>>

References:

- Book "Computational Geometry - Algorithms and Applications", chapter "Orthogonal range searching"

- http://www.cs.utah.edu/~lifeifei/cs6931/kdtree.pdf


[[1d_orthogonal_range_searching]]
==== 1d orthogonal range searching using a BST

Given a set P of points, find the points in range [f, t] (f=from, t=to).

Put the points in a a balanced binary search tree. Often it's one which stores its values in its leaves, see e.g. <<naive_BST>>.   This is especially convenient for higher dimensions, see <<range_tree>>.

Search f and t in the tree. There is one search path until a node, often called the split node, where the search splits in two paths.  Let nf and nt be the two leaf nodes where the two searches end. The leafes of all the subtrees `enclosed' by the two paths (splitnode, nf) and (splitnode, nt) are the result.  In text book examples, typically these subtrees are reported on the fly while descenting from the split node.  On the left search path, the right subtree when descending left, and nothing when descending right.  Analogously for the right search path.

Analysis: O(n log n) for building the binary search tree, and O(k + log n) for reporting, where k is the number of reported points.  Informal proof: Reporting a subtree is O(kʹ), where kʹ is the number of leavs in that subtree.  Searching f and t in the tree is O(log n).


References:

- Book "Computational Geometry - Algorithms and Applications", subchapter "5.1 1-Dimensional Range Searching"

- https://courses.csail.mit.edu/6.851/spring12/scribe/lec3.pdf, first part of section "range trees"

- http://www.cs.utah.edu/~lifeifei/cs6931/kdtree.pdf, chapter "1 - Dimensional Range Searching"


=== Nearest neighbor search (NNN)

*to-do*:


[[closet_pair_of_points]]
=== Closest pair of points

Solutions:

- Deleauny triangulation O(n)

- Voronoi diagram O(n)

- O(n log^2^ n) algorithm


==== Divide and conquer with presorting only in x

References:

- https://www.geeksforgeeks.org/closest-pair-of-points/

- http://cse.unl.edu/~ylu/raik283/notes/ClosestPair.ppt


==== Divide and conquer with presorting both in x and y

Preparation: Sort set of input points by x coordinates → PointsXS. In another container also sort them by y coordinates → PointsYS. The recursive divide and conquer function gets both PointsXS and PointsYS as input parameters.

Base case: n≤3 points. Solve with brute force, i.e. compare all pairs and choose the pair with the smaller distance.

Divide: Partition PointsXS into two equal sized still x-sorted subsets around an imaginary vertical line.  Note that multiple points can lie on that line, and both partitions can contain points on that line.  This is trivial since PointsXS is already sorted in x direction.  Distribute PointsYS into two still y-sorted subsets PointsYSLeft and PointsYSRight.  Points being in PointsXSLeft go to PointsYSLeft, the others to PointsYSRight.

Conquer: Make two recursive calls, one with (PointsXSLeft, PointsYSLeft) as arguments, one with (PointsXSRight, PointsYSRight) as arguments. This delivers min_left and min_right.

Combine: Choose the smaller of min_left, min_right delivering min.  Now it's possible that there's a pair with a distance smaller than min in the vertical strip ±min around the vertical line from the devide step above.  One point of the pair being in the left half of the strip, the other in the right half.  Recall that points on the line can be in either partition.  From PointsYS extract those points being in the strip while keeping them y sorted, resulting in StripPointsYS.  For each point in StripPointsYS, compare it with all points further ahead in StripPointsYS which are at most min away in y direction.  It can be proven that there are at most 7 such points ahead.

--------------------------------------------------
closest_dist(points)
  sort points in x direction
  PointsYS = an y-sorted array of pointers to into points
  return closest_dist_core(points, 0, points.length-1, PointsYS)

closest_dist_core(AllPointsXS: points, from:int, to:int, PointsYS: point_pointers)
  if to-from+1 <= 3
    return closest_dist_brute_force(PointsYS)
  middle_index = (from + to) / 2
  PointsYSLeft, PointsYRLeft = ypartition(AllPointsXS, middle_index, PointsYS)
  min_left  = core(AllPointsXS, from, middle_index, PointsYSLeft)
  min_right = core(AllPointsXS, middle_index+1, to, PointsYSRight)
  min = min(min_left, min_right)
  return closest_dist_strip(min, AllPointsXS, middle_index, PointsYS)  

ypartition(AllPointsXS: points, middle_index:int, PointsYS: point_pointers)
  create two new arrays PointsYSLeft, PointsYSRight
  for each point p in PointsYS
    index = p - AllPointsXS // pointer arithmetic
    if index<=middle_index: PointsYSLeft.append(*p)
    else                  : PointsYSRight.append(*p)
  return PointsYSLeft, PointsYSRight

closest_dist_strip(min, AllPointsXS: points, middle_index:int, PointsYS: point_pointers)
  // Find points being in strip.
  // Note that since PointsYS is sorted, so is StripPointsYS
  xmiddle = AllPointsXS[middle_index].x
  StripPointsYS = new array of point-pointers
  for each point-pointer p in PointsYS
    if abs(p->x - xmiddle) < min
      StripPointsYS.append(*p)  

  for i=0 upto StripPointsYS.length-1
    j=i+1
    // Inner loop proofably makes at most 7 iterations. Note that we
    // also compare points being on the same strip side, which is needless,
    // but its not worth to make a special case.
    while (j<StripPointsYS.length) and (StripPointsYS[j]->y < StripPointsYS[i]->y+min)
      min = min(min, distance(*StripPointsYS[i], *StripPointsYS[j]))
      j++

  return min
--------------------------------------------------

Informal proof that closest_dist_strip's inner loop needs to check at most 7 points ahead: In the worst case, i.e. where the points are packed as dense as possible, the situation looks like in the following diagram. ❍ is the currently checked point as determined by the outer loop, ➊ is the point one ahead, ➌ is three ahead and so on.  Recall that the points are sorted in y.  In this worst case scenario, the 1st and 2nd point coincide; 1st being part of the left partition, 2nd being part of the right partition.  Likewise for 5th and 6th.  We don't need to consider points further ahead, because their y distance to ❍ would be larger than min.  Thus at most 7 points are ahead with a y distance smaller than min.  Personal note:  As far as I understand it, 7 is an upper bound, but maybe not tight.  With more complicated math, we might be able to prove that there are even less considerable points ahead.  But it's really only about proving that it is a constant number of points we have to consider at most.  That results in O(n) for closest_dist_strip.  If we didn't had this proof, it would be O(n²), since we had to assume having to compare every point in the strip with every other in the worst case.

--------------------------------------------------
     vertical
      line
       ‖
  ❍----➊----➌    Each horizontal / vertical
       ‖          neighboring pair is min appart
       ‖
       ‖
       ‖
  ➍----➎----➐
       ‖
--------------------------------------------------


References:

- https://www.geeksforgeeks.org/closest-pair-of-points-onlogn-implementation/

- Book ``Introduction to algorithms'', subchapter ``Finding the closest pair of points''.


[[RMQ]]
=== Range minimim query (RMQ)

Given an array A, find index (here named k) of minimum element in range [i,j].

++k=RMQ(i,j)=(arg)min{A[n]|i≤n≤j}++

(arg)min says that we're interested in the index, not the value of the minimum element.

Solutions:

- Convert A to a <<cartesian_tree>>, see there, and in that tree, solve the <<LCA_problem>>, see there.


[[maximum_flow_problem]]
=== Maximum Flow Problems

_Maximum flow problems_ involve finding the maximum flow in a <<flow_network>>.  It can be seen as a special case of more complex problems, e.g. the circulation problem.

Solutions of max-flow:

- <<ford_fulkerson_algorithm>> in +O(E*f)+ (whereas f is the maximum flow) for the case where capacities are integers.
- <<edmonds_karp>> algorithm in +O(VE²)+ for the case where capacities are integers.
- An algorithm implementing the <<push_relabel>> approach. The generic version is +O(VE²)+, more specific version get better, the `highest label selection' has +O(V²√E)+


=== Consumer producer

Solution using semaphores.  Allows for multiple producers and consumers.

----------------------------------------------------------------------
Semaphore emptyCount
Semaphore fullCount
Semaphore useQueue

produce:
  wait(emptyCount)
  wait(useQueue)
  putItemIntoQueue(item)
  signal(useQueue)
  signal(fullCount)

consume:
  wait(fullCount)
  wait(useQueue)
  item ← getItemFromQueue()
  signal(useQueue)
  signal(emptyCount)
----------------------------------------------------------------------

*to-do*:
- Solution with monitors
- Question: why isn't it in the above solution good enough to only guard the one critical section with a single binary semaphore?


=== Dining philosophers
*to-do*


== Misc. related computer science


=== NFA
*to-do*:


=== DFA
*to-do*:


=== DAFSA
*to-do*:


=== Bit manipulation
A _nibble_ is a four bit aggregation (aka _halb-byte_ or quartet).

--------------------------------------------------
set:    x |=  y
clear:  x &= ~y
toggle: x ^=  y
test:   x &   y
--------------------------------------------------

In C / C&plus;&plus;, +CHAR_BIT+ is the number of bits in a byte.

- http://graphics.stanford.edu/~seander/bithacks.html


== References

- http://ocw.mit.edu/courses/civil-and-environmental-engineering/1-204-computer-algorithms-in-systems-engineering-spring-2010/lecture-notes/

- https://www.quora.com/What-are-the-very-basic-algorithms-that-every-Computer-Science-student-must-be-aware-of

- MIT 6.006: Introduction to Algorithms: https://courses.csail.mit.edu/6.006
  * (fall'11): https://courses.csail.mit.edu/6.006/fall11/notes.shtml, https://www.youtube.com/playlist?list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb
  * Newest lecture notes which are freely available spring'14: the newest vidoes seem to be from fall'11.

- MIT 6.046J / 18.410J: In 2015 called "Design and Analysis of Algorithms", before "Introduction to Algorithms (SMA 5503)". Aparantly in '15 it's an advanced course between 6.006 and 6.851, before it was a course similar to 6.006.
 * Spring '15:
   ** youtube: https://www.youtube.com/playlist?list=PLUl4u3cNGP6317WaSNfmCvGym2ucw3oGp
   ** ocw: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-videos/
 * Fall '05:
  ** youtube: https://www.youtube.com/playlist?list=PL81A705FB7F988E7C
  ** OCW, less videos, but title describing content: http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures/

- MIT 6.851: Advanced Data Structures: https://courses.csail.mit.edu/6.851/
  * spring'12: https://courses.csail.mit.edu/6.851/spring12/lectures/, https://www.youtube.com/playlist?list=PLUl4u3cNGP61hsJNdULdudlRL493b-XZf
  * Newest spring'14: The link to the vidoes are the same as those of spring'12.

- MIT 6.854: Advanced Algorithms
  - Spring 2016: Youtube https://www.youtube.com/playlist?list=PL6ogFv-ieghdoGKGg2Bik3Gl1glBTEu8c
  - Unknown date: youtube https://www.youtube.com/playlist?list=PLXzW9t1q_fb_3k_KzBfBnw0NXjvq45-bf

- https://www.cs.usfca.edu/~galles/visualization/

// Local Variables:
// eval: (visual-line-mode 1)
// eval: (auto-fill-mode -1)
// eval: (filladapt-mode -1)
// End:

//  LocalWords:  pre th ADT Multimap multihash multimap emptyCount fullCount
//  LocalWords:  useQueue putItemIntoQueue getItemFromQueue Treap DAFSA Deque
//  LocalWords:  BST spw spwfs decreaseKey spt dest unicursal eulerian NPC
//  LocalWords:  Königsberg Hierholzer's subgraph supergraph Horner Horner's
//  LocalWords:  adaptors acc umulator Quickselect supremum infimum CLRS DFS
//  LocalWords:  AStarMonotonicH AStar toposort topsort BSF ith preorder args
//  LocalWords:  inorder Sedgewick Karp textlen patternlen patternhash str
//  LocalWords:  texthash issubstring rollinghash len BFS MyStack prev Thorup
//  LocalWords:  Brodal mergeable
