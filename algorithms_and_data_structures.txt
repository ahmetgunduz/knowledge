:encoding: UTF-8
// The markup language of this document is AsciiDoc

= Algorithms & data structures


== Asymptotic notations / Big O notation
In computer science, big O notation is used to classify algorithms by how they respond to changes in input size, typically regarding running time space (memory/disk/...).

In the following +n+ is the _input size_, +f(n)+ is the _number of steps_ needed by an algorithm, and  +T(n)+ is its _running time_.

Types of asymptotic notations:

[cols="3,2,2,6,6"]
|====
| notation | | relation of growth rate | definition | Notes
| +f(n) ∊ ο(g(n))+ | little-oh | f < g | Like O, bug for _all_ positive c | f is dominated by g asymptotically.
| +f(n) ∊ O(g(n))+ | big-oh    | f ≤ g | ++There exist an c>0 and n~0~>0 such that \|f(n)\| ≤ c⋅\|g(n)\| for all n≥n~0~++ | Asymptotic upper bound. No claim on how tight; technically it woudn't be wrong to say that a linear algorigthm is +O(2^n)+
| +f(n) ∊ Θ(g(n))+ | big-theta | f = g | ++There exist an c~1~>0, c~2~>0 and n~0~>0 such that c~1~⋅\|g(n)\| ≤ \|f(n)\| ≤ c~2~⋅\|g(n)\| for all n≥n~0~++ | Asymptotic tight bound. +Θ(g(n)) = O(g(n)) ∩ Ω(g(n))+. 
| +f(n) ∊ Ω(g(n))+ | big-omega | f ≥ g | Like O, but ≥ (instead ≤) | Asymptotic lower bound.
| +f(n) ∊ ω(g(n))+ | little-omega | f > g | Like ο, but ≥ (instead ≤) | f dominates g asymptotically.
|====

Θ is also called _rate/order of growth_.

Note: Because ++O(g(n))++ is really a set, we should actually write ++f(n) ∊ O(g(n))++.  However we often write ++f(n)=O(g(n))++, the equal sign meaning ∊.
Informally, especially in computer science, the big-oh notation often is permitted to be somewhat abused to describe an asymptotic tight bound (it really only describes an asymptotic upper bound) where using big-theta notation might be more factually appropriate in a given context.


_worst case_ / _average case_ / _best case_ refers to the worst / average / best input -- a ``good'' input results in a short running time of the algorithm, a ``bad'' input results in a long running time.  For many algorithms we only care about the worst case, not the average case, because a) the worst case occurs fairly often in practice b) the average case is often as bad as the worst case c) it's difficult to know what an ``average'' input is (often it is assumed that all possible inputs are equally likely).

_Tight bounds_: An upper bound is said to be a tight upper bound (aka _supremum_) if no smaller value is an upper bound.  Likewise for tight lower bounds (aka _infimum_).

_Asymptotic efficiency_: Only look at rate of growth.  Usually, an algorithm that is asymptotically more efficient will be the best choice for all but very small inputs.  An algorithm is said to be _asymptotically optimal_ if, roughly speaking, its big-oh is equal to the big-oh of the best possible algorithm.

_amortized time_: `amortized +O(f)+' for operation o: In a sequence of length L of such o operations, the overall time is +O(L*f)+.  I.e. one of those o operations might use a particular large amount of time compared to the average case, but that time is amortized in the large.  A typical example is appending to an array; if the capacity is full, a new array of larger capacity needs to be allocated, and the data has to be copied.

Common functions ordered after order of growth: c, log~c~(n), n, n·log~c~(n), n^c^, c^n^, n!, n^n^


See also:

- http://bigocheatsheet.com/
- http://stackoverflow.com/questions/1364444/difference-between-big-o-and-little-o-notation
- http://stackoverflow.com/questions/2986074/algorithm-analysis-orders-of-growth-question


== Computational complexity classes

The field of computational complexity categorizes decidable decision problems by how difficult they are to solve. "Difficult", in this sense, is described in terms of the computational resources needed by the most efficient algorithm for a certain problem.

A _decision problem_ is a problem with a binary answer, e.g. yes or no.  A _function problem_ can have answers that are more complex than a simple `yes' or `no'.  Function problems can be transformed into decision problems and vice versa.  Thus computational complexity can focus on decision problems.


P: (Decision) problems solvable in at most polynomial tyme (n^c).

NP (from non-determiniatic polynomial): Decision problems where the a given yes-answer (e.g. yes, this sudoku has a solution), has a proof (can take more than polynomial time) (e.g. this solved sudoku) which can be checked in at most polynomial time (e.g. take the alleged solution / proof and verify it holds up to the sudoku rules).

EXP: (Decision) problems solvable in at most exponential tyme (2\^n^c).

R (R for recursive, in the old days recursive stod for will terminate): (Decision) problems solvable in finite time

NP-hard (or X-hard in general): at least as hard as every element in NP (X in general) (i.e. same hardness or harder, but not less hard than any element in NP (X in general))

N-complete (or X-complete in general): intersection of NP and NP-hard.

Set of problems, orderer on a line after hardness:

foo
--------------------------------------------------
              P-complete  NP-complete  EXP-complete
easier <----------|----------|-------------|----------------> harder
      
 P                   P-hard
 (incl P-complete)   (incl P-complete)
<---------------->|<---------------------------------....

           NP                      NP-hard
           (incl NP-complete)      (incl NP-complete)
<--------------------------->|<---------------------....

           EXP                               EXP
           (incl EXP-complete)               (incl EXP-complete)
<---------------------------------------->|<---------------------....
--------------------------------------------------


P ≠ NP:
 ≈ ``you can't engineer luck''
 ≈ ``solving problems is harder than check solutions''

Most people think P ≠ NP is true, but no one could prove it so far.



The class _P_ consists of those problems that are solvable in polynomial time.  Alternatively, P  is the class of problems that are _tractable_ (``efficiently solvable'').  The class _NP_ (non-deterministic polynomial time) consists of those problems that are `verifiable' in polynomial time, i.e. given a solution it is verifiable in polynomial time that the solution is correct.  _NP-hard_ is a class of problems that are, informally, ``at least as hard as the hardest problem in NP''.  A problem is in the class _NPC_ (aka _NP-complete_) if it is in NP and in NP-hard.

- If any NP-complete problem can be solved in polynomial time, then every problem in NP has a polynomial time algorithm.
- No polynomial-time algorithm has yet been discovered for an NP-complete problem.
- If you can establish a problem as NP-complete, you provide good evidence for its intractability.  You'd better spend your time developing an approximation algorithm or solve a tractable special case.
- Not yet proven that no polynomial-time algorithm can exist for any NP-complete problem.

Examples of NP-complete problems:

- Determining whether a graph contains a simple path with at least a given number of edges

- _Travelling salesman problem (TSP)_: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city.  TSP is a special case of the traveling purchaser problem.
- _Knapsack_: Given a set of items, each with a mass and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.
  * Resource allocation
- _Hamiltonian path/cycle_: a path/cycle in an undirected or directed graph that visits each vertex exactly once
- _Boolean satisfiability_ (_SAT_) problem: *to-do*:
- _Subset sum problem_: Given a set (or multiset) of integers, is there a non-empty subset whose sum is zero?
- _clique problems_
 * Finding the maximum clique (a clique with the largest number of vertices)
 * Finding the maximum weight clique in a weighted graph
 * Listing all maximal cliques (cliques that cannot be enlarged)
- _minimum vertex cover_
- _maximum independent set problem_
- _Graph coloring_ regarding vertices (edges): Coloring the vertices (edges) of a graph such that no two adjacent vertices (edges) share the same color.


== Classifying Algorithms by Implementations

=== Recursion vs iteration

- What is computable by recursive functions is computable by an iterative model and vice versa.

- KISS: Use whichever is more easy to reason about for the given problem.  Since recursion maps easily to proof by induction, for many problems recursion is a straight forward choice.

* Recursion has to pay expense of function calls and function returns, which is typically larger than the (conditional) jump used in the iterative solution.  However in case of tail calls and an compiler featuring tail call optimization becomes pretty much equivalent to iteration since the machine code is iterative.

* Recursion needs memory on the stack for all the locals, the stack frame (the return address, the old stack pointer, ...).  However there are iterative solutions which need an stack or queue, which internally probably uses the heap with all its overhead in space and time.  It depends on the queue/stack implementation which is more efficient in terms of memory usage, locality, ....

- Modern compilers are good at converting some recursions to loops without even asking.


Terms: _base case_ is input for which the solution is directly known.  When the recursion arrives at the base case it is said to _bottom out_.

=== Recipes for convertion recursion -> iteration

==== Tail call
Recipe for translating recursion into iteration for a function ++foo++ for the case where recursive calls are convertible to tail calls:

. Convert all recursive calls into tail calls.  If you're programming language supports tail call optimization, you're already done.

. Enclose the body of the function with a ++while(true) { ... }++ loop.

. Replace each call to ++foo++ according to this scheme: ``++foo(f1(...), f2(...), ...)++'' => ``++x1=f1(...); x2=f2(...); ...; continue;++''

. For languages where identifiers need to be defined: For each +x+ object introduced in the previous step, define the object before the while loop introduced earlier.

. Tidy up.


==== Non tail call
`Recipe' for translating recursion into iteration in case there are n multiple recursive calls which are not tail calls and not convertible to tail calls.  It's more tips than a proper recipe.

- Remember that all local variables (which includes parameters) and the return address are on the stack.  So if one needs to know the return address, i.e. one of multiple possible places, it gets nasty difficult.

- Enclose the whole body in a ++stack<...> s; s.push(args); while (!s.empty()) { current_args = s.pop(); ... }++

- Instead of n times recursively calling foo like ++foo(args1); foo(args2);...++ push the args on the stack in reverse order ++s.push(args2); s.push(args1)++.




Recipe for turning a non-tail call recursive function ++foo++ into one having a tail call:

. Identify what work is being done between the recursive call and the return statement.  That delivers a function +g(x,y)+, so the respective expression could be written as ++return g(foo(...), bar)++.
. Extend the function to do that +g+ work for us.  Extend it with an new accumulator argument, ++foo(..., acc=default_doing_nothing)++, and replace all return statements ++return lorem;++ with ++return g(lorem, acc);++.
. Now you can replace very occurrence of ++return g(foo(...), bar)++ with ++return foo(..., bar)++, since we don't have to do +g+ ourselves any more, we can let +foo+ do +g+ for us.

--------------------------------------------------
// example step 1
def factorial(n):
    if n < 2: return 1
    return factorial(n - 1) * n // thus we have an g: g(x,y)=x*y

// example step 2
def factorial(n, acc=1):
     if n < 2: return 1 * acc
     return (n * factorial(n - 1)) * acc //==factorial(n-1)*(acc*n)

// example step 3
def factorial(n, acc=1):
     if n < 2: return acc * 1
     return factorial(n - 1, acc*n)
--------------------------------------------------
See also: http://blog.moertel.com/posts/2013-05-11-recursive-to-iterative.html


==== Non tail call

--------------------------------------------------
stack localsAndParamsStack;
stack addrStack;
addr = FunEntr;
auto done = false;
do {
  switch (addr) {
  case FunEntry:
    ...
  case X:
    ...
  }
} while (not done);
--------------------------------------------------


!! mind implicit return at end of original function !!

!! how to return values from called function ???

How to translate calls and returns:

--------------------------------------------------
             function call                      | return
machine instr.     pseudo code in loop          | pseudo code in loop
 -----------------------------------------------|-------------------------
                                                | continue
                                                |
(save locals)      localsAndParamsStack.push(   | localsAndParams = 
                       locals and params)       |    localsAndParamsStack.pop()
                                                |
push params        params = new params          |
                                                |
push returnAddr    addrStack.push(addr)         |
                                                |
jmp funAddr        addr = FunEntry              | addr = addrStack.pop()
                   continue                     |
                                                |
                                                | if (addrStack.empty())
--------------------------------------------------


=== Deterministic vs non-deterministic
*to-do*

=== Serial vs parallel vs distributed
*to-do*

=== Exact vs approximate
*to-do*


== Algorithm design techniques/paradigms

=== Brute force (aka exhaustive search)
This is the naive method of trying every possible solution to see which is best.


[[divide_and_conquer]]
=== Divide and Conquer

_Divide_ the problem into two or more subproblems that are smaller instances of the same problem.  _Conquer_ the subproblems by solving them recursively.  If the size of a subproblem is small enough, stop recursion (we say the recursion _bottoms out_) and solve it (we call that small subproblem a _base case_) in a straightforward manner.  _Combine_ the solutions the subproblems into the solution of the original problem.  See also <<relation_between_techniques>>.

Examples: Quick sort


[[decrease_and_conquer]]
=== Decrease and conquer (aka prune and search)
In each step the problem is turned into one single sub problem of smaller size, where as the rest ist pruned.  The algorithm stops when the base case is reached.  My thoughts: The size of a subproblem is typically by a constant factor (on average) smaller than one of the parent problem -- if the size would only decrease by a constant amount, in the worst case 1, it would just be the naive brute force solution.  See also See also <<relation_between_techniques>>.

Examples: binary search, quickselect.


[[dynamic_programming]]
=== Dynamic programming (DP)
Basic idea: `carefull brute force'.  Use brute force, i.e. try (aka guess) all ways (and in case of optimization problems, take best), plus use <<memoization>>, plus divide into subprobems.  Thus DP is often good for optimizations problems.  The memo is typically an associative array with +O(1)+ insert and lookup time.

Example problem referred to below: Consider a steel company cutting steel rods and selling the pieces.  For simplicity lengths are integers.  Given a table of prices which states the price for a rod of length i.  How to cut a rod of length n into multiple smaller rods to maximize revenue.

Dynamic programming needs two hallmarks:

1. _Optimal substructure_. An optimal solution to the problem contains within it optimal solutions to subproblems.  I.e. if you have an optimal solutions to each sub problem, you can combine them to form the optimal solution to the original problem.  Example: in the rod cutting problem, is optimally cutting a rod of length +n+ in two pieces.  That gives us two new subproblems: optimally cutting these two pieces.

2. _Overlapping subproblems_. A given sub-problem has to be solved/computed many times.  If that's not the case, there's no point in doing memoization.  Example: in the rod cutting problem, the problem of cutting a rod of length 2 has to be solved again and again within the problem of cutting a rod of length greater than 2.

Two equivalent ways to implement a dynamic programming approach:

_to-down approach_: by recursion.  In the recursion tree, solve a particular problem only once, when it later is encountered again, prune that whole subtree by looking up the solution in the memo.

_bottom-up approach_: Iteratively solve the subproblems, typically in increasing order of `size' (for general case, see the toposort note at the end of this paragraph), each time using the memo and then memoizating the solution in memo. In general does the same computation as the top-down approach; it's a topological sort of the subproblem dependency DAG.  Sometimes the bottom-up approach can save space, because you might know that you only need the last i solutions, e.g. in the fibonacci example you only need the last two. The topological sorted DAG helps to see if that is the case and how big i is.

--------------------------------------------------
# bottup-up                          # top down
                                     memo = {}
fun fib(n):                          fun fib(n):        
  memo = {}                            if n in memo: return memo[n] 
  for k=1 to(incl) n                   
    if k<=2: f = 1                     <--same
    else: f = memo[k-1]+memo[k-2]      <--" (recursive calls instead lookup)
    memo[k] = f                        <--"
                                       return f
--------------------------------------------------

Misc phrases:
- Etymology / trivia: `Dynamic programming' is a wierd term, just take it for what it is. Still: in british english, `programming' means optimize.  The inventor, bellman, choose it for reasons among `sounds cool to a congress man', `to hide the fact he was doing `math research'.

**to-do:** List examples of problems which can be solved using dynamic programming, e.g. from the problems sections.


[[greedy_technique]]
=== Greedy technique / algorithm

A _greedy algorithm_ repeatedly makes locally best choice/decision, ignoring effect on future, with the hope, but not guarantee, of finding an optimal solution to the overall problem.

Problems for which a greedy algorithm works well generally have these two properties:

- _optimal substructure_: See also <<dynamic_programing>>.  Rational: The choice we just made (an optimal solution to a (mini) sub problem), plus the optimal solution to the subproblem that remains (which we will solve recursively), yiels an optimal solution to the original problem.

- _Greedy choice property_: Locally optimal choices lead to globally optimal solutions.

In many problems, a greedy strategy does not in general produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a global optimal solution in a reasonable time.  A greedy algorithm never reconsiders its choices; it makes locally best choices. This is the main difference from dynamic programming, which is exhaustive and is guaranteed to find the solution.


[[relation_between_techniques]]
=== Relation between techniques

Decrease and conquer is similar to divide and conquer.  However the latter splits the problem into two or more sub problems.  The former doesn't need to combine the results of the sub problems.

In dynamic programming, subproblems overlapp and we need to solve them only once. In divide/decrease and conquer, sub problems do not overlap.

dynamic programming vs greedy algorithm: in dynamic programming and divide/decrease and conquer the choices are made depending on the result of the sub problems. I.e. the sub problems are solved first.  The greedy algorithm makes first a (greedy) choice, thus reduces the problem to a subproblem, and then solves that remaining subproblem.


=== Linear programming
*todo*

=== Heuristic method
Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms.

*to-do*


== Misc algorithm properies & terms

.Properties
Locality::
In-place:: An algorithm using +O(1)+ auxiliary memory space.  Often even +O(log n)+ is considered as in place.

.Terms
Rank:: The rank of an element is its position in the linear order of a set.
__i__th order statistic:: the __i__th-smallest value in the set.
Sentinel:: A sentinel is a dummy object that allows us to simplify boundary conditions.
Online:: An online algorithm is one that can process its input piece-by-piece in a serial fashion.
_Memoization_: The solution to a given (sub)problem is memoized in a `memo pad' (aka table).  E.g. upfront or when first encountering it.  When later seeing the same (sub)problem again, its solution can be looked up in the memo.  See also <<dynamic_programming>>.


== Sorting
Properties of sorting algorithms.  See also properties of algorithms in general.  Comparison-based sorting algorithm are asymptotically optimal when they run in +O(n lg(n))+ time.

Stable:: Stable sorting algorithms maintain the relative order of records with equal keys
Adaptability:: Whether or not the presortedness of the input affects the running time.
external sorting:: When the input data does not fit into main memory, and parts of it must reside on secondary storage.


=== Overview
To sort arrays:

* Bubble sort, Insertion Sort and Selection Sort are bad;
* Shell sort is better but nowhere near the theoretical +O(n log n)+ limit;
* Quick sort is great when it works, but unreliable (+O(n^2^)+ worst case), not stable;
* Merge sort is reliably good, stable, but requires +O(n)+ auxiliary space;
* Heap sort is reliably good, but unstable, and also about a factor of 4 slower than quick sort's best case.

To sort linked lists:

* Copy it to an temporary array, sort that, copy the array back to the linked list.  Main reason: array has much better locality than a linked list where the nodes are scattered within memory.
* Variant of merge sort

*to-do*: Why is in practice quick sort better than merge sort or heap sort, which both have better worst case running time, but all three have Θ(n lg n) average/expected running-time.

References:

- http://en.wikipedia.org/wiki/Sorting_algorithm
- Non comparison sorts with integers: http://en.wikipedia.org/wiki/Integer_sorting


=== Insertion sort

Time: +O(n^2^)+ worst case & average case, +O(n)+ best case.  Auxiliary space: +O(1)+.  Adaptive.  Stable.  In-place.  Online.  The input is logically divided into a sorted part at the left, initially empty, and an unsorted part at the right, initially the complete input.  In each outer iteration, insertion sort removes (see following swap) the leftmost element from the unsorted part.  In an inner iteration it drags the element to the location the elements belongs to within the sorted part by searching to the left and swapping elements on the way.  Often used for small arrays (since time complexity has a small constant factor).

=== Selection sort
Time: +O(n^2^)+ worst case & average case & best case.  Auxiliary space: +O(1)+.  Divide the input logically into a sorted part (initially empty) followed by an unsorted part (initially the whole input).  In each iteration search the smallest element in the unsorted part, swap it with the leftmost element of the unsorted part, then increment the pointer dividing the sorted / unsorted sub lists.

=== Quick sort
Time: +O(n^2^)+ worst case, +O(n lg(n))+ average case, best case: (simple partition: +O(n lg(n))+, 3way partition and equal keys: +O(n)+).  Auxillary space: worst case: (naive: +O(n)+, Sedgewick +O(log n)+).  Not stable in naive implementation.  Hidden factor in time complexity in practice quite small.  The +O(n^2^)+ worst case running time might be a problem when input size is large and used in an real-time system or system concerned with security (because malicious user potentially can trigger worst case behaviour).

=== (natural/external) Merge sort
Time: +O(n lg(n))+ worst case & average case & best case.  Space: +O(n)+ auxiliary memory.  Stable.  Good locality of reference.  Parallelizes well.  External sorting possible.  1) Divide the sequence into two equal length subsequences 2) sort the two sequences using recursion, recursion stops at a sequence of one element 3) merge the two sequences (see below).  Discussion: Good for sequentially accessible data.  Highly parallelizable (+O(log n)+).  Variant: _natural merge sort_:  Time: +O(n)+ best case, the rest remains equal.  Exploits any naturally occurring runs in the input.  Variant: _external merge sort_: Motivation: input data does not fit into memory.  Divide the input data into N blocks, each block fitting into memory.  Sort each block with any sorting algorithm and write the result to disk.  Then with ``externally merge sorted sequences'' merge the blocks.  Variant _merge sort for linked lists_: *to-do*

=== Bubble sort
The input is devided logically into an unsorted part to the left, initially the whole input, followed by an sorted part, initially empty.  In each inner iteration, a sliding window of length two elements traverses the unsorted list from left to right, advancing in one element steps.  At each step, the two elements in the sliding window are swapped if needed to ensure the right element is larger than the left element.  The result of one inner iteration is that the sorted part gets one element added to its left side.  The process is repeated until all is sorted.

=== Heap sort
Time: +O(n lg(n))+ worst case & average case & best case.  Auxillary Space: +O(1)+.  In-place.  Not stable.  In practice often somewhat slower that quick sort, however it has a better worst case run time.  The input is logically divided into an unsorted part, initially the whole input, followed by a sorted part, initially empty.  The unsorted part is heapified into a max heap.  In each iteration, the first (i.e. max) element is swapped with the last,  thus appending a new element to the left side of the sorted part, and thus also shrinking the heap / unsorted part by one.  Using the heap's sift down operation (or just heapify again the unsorted part), the heap property is restablished.

=== Bucket sort
Time: +O(n^2^)+ worst case, +O(n+k)+ average-case and best case.  Auxiliary space: +O(n+k)+.  Stable.  Not comparison-based; it assumes that the elements' values are uniformly distributed, for simplicity, without loss of generality, assume in [0,1).  Make an array of k `buckets', where each bucket is a sequence of elements, initially empty.  For each element in the input, insert it into the bucket having the array index k*elementvalue.  Then sort each of the buckets with another sort.  Then produce the final output by concatenating the buckets.  Note: the time complexity gets worse if the data is not uniformly distributed as assumed, since certain bucket sequences get much longer than other.

=== Counting sort (aka histogram sort)::
+Θ(k+n)+ time complexity, +Θ(k+n)+ or +Θ(k)+ space complexity, depending on whether the _required_ output array (sorting in-place only with the input array is not possible) is taken into account or not.  Not comparison-based; it assumes that each element is an integer in the range +0--k+, where +k=O(n)+.  Stable sort.  Algorithm: With an array a of size k, for each input element x, count the elements less than x, which so gives the index / position of x in the sorted output sequence.  Mental image: internally counting sort computes an histogram of the number of times each key occurs.

=== Radix sort
Not comparison-based.  Stable sort.  Given n d-digit numbers, each digit can take up to k possible values.  Time: +Θ(d(n+k))+ (provided the internally used stable sort has +Θ(n+k)+), however the hidden constant factor is in practice quite large relative to other sort algorithms.  First sort after least significant digit, then after second least, ....

=== Merge sorted sequences
Time: +Θ(n)+.  Space: +O(n)+. Imagine n cards within m sorted piles of cards face up.  Take the smallest card yous see and put it on the sorted pile.

=== Externally merge sorted sequences
Given N sorted sequences on disk which do not fit all together into memory.  For each, make an buffered (in memory) input stream.  Make an (buffered) output stream for the result.  Now there are N input streams and 1 output stream and the algorithm works as described in ``merge sorted sequences''.


== Medians and order statistics

The _ith order statistic_ of a set of n elements is the ith smallest element.  For example, the _minimum_ of a set of elements is the first order statistic (i D 1), and the _maximum_ is the nth order statistic (i D n).  A _median_, informally, is the “halfway point” of the set.  When n is odd, the median is unique, occurring at i D .n C 1/=2.  When n is even, there are two medians, occurring at i D n=2 and i D n=2C1.  Thus, regardless of the parity of n, medians occur at i D b.n C 1/=2c (the _lower median_) and i D d.n C 1/=2e (the _upper median_).  For simplicity in this text, however, we consistently use the phrase “the median” to refer to the lower median.

_Selection problem_: Given a set _A_ of _n_ (distinct) numbers and an integer _i_ with __i ≤ i ≤ n__, find the element _x_ ∊ A that is larger than exactly i-1 other elements of A.

=== Trivial algorithm
Finding the minimum and maximum element can be trivially solved in +O(n)+ time and +O(1)+ auxillary space by iteratively searching through the array for the smallest or largest element respectively.

If the input is sorted first, then we can just access the i-th element in the sorted collection in +O(1)+ for arrays or +O(n)+ for lists respectively.  Certain input can be sorted in +O(n)+ time worst case; for that input also the selection problem can be solved in +O(n)+.


=== Quick select
Time:: +O(n)+ average/best case.  worst case depends on pivot selection method: random: +O(n^2^)+ (very unlikely for unsorted input),  median of medians: +O(n)+ (high constant factor)
Auxillary space:: +O(1)+.

A <<decrease_and_conquer>> algorithm: In each recursion step, choose a pivot and partition the input (of the current recursion step) into a part smaller than the pivot and larger than the pivot respectively.  That delivers the position / index of the pivot.  Then recurse into the left/right part depending on whether i is smaller/larger than the pivot's index.  The base case is either if i is equal the pivot after partitioning, or when the input size is 1.  *to-do* why not continue with trivial way of finding the minimum/maximum when i=0 or input.size-1?, which is +O(n)+ also in the worst case instead quick select's +O(n^2^)+.

Like quicksort, the quickselect has good average performance, but is sensitive to the pivot that is chosen.  Choosing a random pivot yields almost certain +O(n)+ time, the +O(n^2^)+ worst case is still possible however very unlikely.  Note that the best possible pivot is the median (apart from the i-th element, which would directly deliver the solution), since it halves the input.

The ``median of median'' algorithm is a quickselect algorithm which chooses the pivot as follows: it computes an approximate median (being between 30th and 70th percentiles):  Make groups of five elements, sort each with e.g. insertion sort, the 3rd element is the median.  We get n/5 medians.  We use quickselect (i.e. the ``median of medians'' version of it) to find the median of those medians, which is then our pivot.

The book ``Introduction to algorithms'' calls quickselect ``randomized-select'' and ``median of medians'' ``select''.


== Ranking
Given a set (not sequence) of elements, the _rank_ of an given element is its position in the sequence what woul be build when the elements are ordered.

*to-do*: Isn't this the same as ith order statistic?


== String searching
Problem: search a given pattern in a given text.  Let _Σ_ be an alphabet (finite set), _T_ a text of length _n_, _p_ a pattern of length _m_.  Both the pattern and searched text are vectors of elements of Σ.

_naive string search_: iteratively check at each location in the searched-text.  Time: +O((n-m+1)m)+ worst case, +O(n)+ average case (note that m<=n).

_FSA_ / _DFA_: *to-do*


=== Rabin-Karp
Time: +Θ(m)+ pre-processing, +Θ((n-m+1)m)+ worst case running time,  +O(n+m)+ expected running time.  *to-do*: I don't see why the naive approach should have a worse expected running time, or a worse constant factor if equal

Compute a hash of the pattern.  Iteratively move a window over the search text until the left edge of the window hits the end of the search text.  The window has the same length as the pattern.  In each iteration compute a rolling hash of the window.  If the window-hash matches the pattern-hash, do a regular string comparison between the window and the pattern, and if they still match, the pattern is found.

Popular rolling hash functions for Rabin-Karp:

--------------------------------------------------
static const int q = ...; // a prime where q*radix<INT_MAX
static const int h = pow(d, m-1) % q;

int find(const string& text, const string& pattern) {
   int radix = ...; // aka d.  size of alphabet, e.g. 127 or 255
   int textlen = text.length(); // aka n
   int patternlen = pattern.length(); // aka m
   int patternhash = hash(pattern, m); // aka p
   int texthash = hash(text, m); // aka ts
   for ( int s=0; s<=textlen-patternlen; ++s ) {
     if (patternhash==texthash && text.issubstring(s,pattern))
       return s;
     if (s<textlen-patternlen)
       texthash = rollinghash(texthash, text[s+1], text[s+patternlen+1]);
   }
   return -1;
}

int hash(const string& str, int len) {
    int acc = 0;
    for ( int i=0; i<len; ++i ) acc = (radix * acc + str[i]) % q;
    return acc;
}

int rollinghash(int hash, char ch_out, char ch_in) {
    return (radix*(hash - ch_out*h) + ch_in) % q;
}
--------------------------------------------------


== Graph (incl. tree) Algorithms

See also <<graph>>.

=== Breadth-first search (BFS) / traversal

An algorithm for traversing or searching a graph in breadth first order.  Typically used to traverse / search a strongly connected graph starting from a single source.  However traversing / searching a disconnected graph is also possible.

Intuitively the algorithm is: Starting at the root (distance 0), first nodes with distance 1 (to the root) are explored, after that nodes with distance 2, and so on. In pseudo code:

--------------------------------------------------
Breadth-First-Search(Graph, root):
  create empty queue Q
  Q.enqueue(root)
  while Q is not empty:
    current = Q.dequeue()
    for each neighbor node that is adjacent to current:
      if neighbor was never visited so far:
        remember neighbor as visited
        Q.enqueue(neighbor)
--------------------------------------------------

Time complexity +O(V + E)+ (each vertex is enqued/dequed at most once, and each edge is looked at twice (from each of its two sides)). Auxillary space complexity is +O(V)+ (In the worst case the queue contains all vertices. Also, each vertice needs an color attribute).
For graphs which are implicitly defined or very large, time complexity is better given as +(b^d+1^)+, whereas b is the branching factor and d is the distance (weights being 1) up to which we search.

To be able to know whether a given node (aka vertex) was already visited typically each node gets a `color' attribute and/or a distance attribute attached.  If it is known that it's an DAG that is not needed since it's inherently not possible for that algorithm to visit a node twice.

--------------------------------------------------
Breadth-First-Search(Graph, root [, dst]):

  for each node n in Graph:
    n.color = undiscovered |and/or| n.distance = NIL
    [n.parent = NIL]

  root.distance = 0 |and/or| root.color = tentative]
  create empty queue Q
  Q.enqueue(root)

  while Q is not empty:

    current = Q.dequeue()
    
    [do auxillary visit action with current]

    for each node neighbor that is adjacent to current:
      if neighbor.color == unvisited |or| neighbor.distance == NIL:
        neighbor.color = tentative |and/or| neighbor.distance = current.distance + 1
        [neighbor.parent = current]
        [if root==dst: return] // if its only about finding path root->dst
        Q.enqueue(neighbor)

    [current.color = visited] // not needed in the two-colors schemes
--------------------------------------------------

There are multiple common naming schemes for the colors. The three color schemes have no advantage over the two color schemes other than some people find the algorithm easier to understand / visualize.

[options="header"]
|=====
   |scheme 1     | scheme 2     | scheme 3     | relation to distance
   |undiscovered | unvisited    | white        | NIL / infinite
.2+|discovered   | tentative    | gray      .2+| not NIL / not infinite
                 | visited      | black
|=====


.Applications
- shortest-path where all edge weights are equal / absent, i.e. where for all paths the path weight equals the path distance.


==== Tree level order traversal: Recursive approach


==== Tree level order traversal: Iterative approach

Is basically a BFS, simplified by the fact that a tree is an acyclic graph, and that typically the distance is not something that we want to know, and thus neither the color nor the distance attribute is needed.

--------------------------------------------------
Tree-Level-Order-Traversal(root):
  create empty queue Q
  Q.enqueue(root)

  while Q is not empty:
    current = Q.dequeue()
    for each child of current:
      Q.enqueue(n)
--------------------------------------------------


[[DFS]]
=== Depth-first search (DFS) / traversal

An algorithm for traversing or searching a graph in depth first order.  Typically used to traverse the complete, possibly disconnected graph.  As opposed to search only a connected graph starting from a single source; however that is also possible. 

==== Recursive algorithm

Intuitively the algorithm is:  Imagine a maze.  Start following any path and leave bread crumbs at each node (crossing) you passed. Once you reach a dead end node (i.e. a node which has no edges wich lead to a node without bread crumbs), you backtrack to the previous node and try the next edge, until that node becomes a dead end node too, and then you backtrack again, and so on.

[[DFS_all_source]]
--------------------------------------------------
DFS-all-source(graph):
  for each vertex in graph, set color=unvisited and parent=NIL
  for each vertex v in graph:
    if v.color == univisted:
      DFS-visit(graph, v)

DFS-single-source(graph, source:vertex):
  for each vertex in graph, set color=unvisited and parent=NIL
  DFS-visit(graph, source)

DFS-visit(graph, current:vertex):
  [do auxillary visit action with current]
  current.color = tentative
  for each neighbor adjacent to current: // aka explore edges (current,neighbor)
    if neighbor.color is unvisited:
      [neighbor.parent = current]
      DFS-visit(graph, neighbor)         // tail call if the following can be ommited
  [current.color = visited]              // not needed in the two color variants
  [push to front of linked list]         // only for topsort
--------------------------------------------------

Analysis DFS-all-source: Time complexity: +Θ(V+E)+. Rational: Each vertex is visited (is current) once -> +O(V)+.  Each outgoing edge of each current (i.e. each node) is looked at -> +O(E)+. Space complexity: +O(V)+. Rational: Each vertex has attributes attached. Also in the worst case, when the graph is a list, there are as many recursion calls (stack pushes) as vertices.

==== Iterative algorithm

--------------------------------------------------
DFS-visit(graph, current:vertex):
  create empty stack S
  S.push(source)
  while S is not empty:
    current = S.pop()
    if current.color is discovered
      continue
    current.color = discovered
    for each neighbor of current:
      [neighbor.parent = current]
      S.push(neighbor)
--------------------------------------------------


[[DFS_edge_classification]]
==== Edge classification

These edge properties are edge properties of the forest that results from a DFS, not of the graph per se.  Here, exploring an edge (current, neighbor) means the loop body within DFS-visit.

_Tree edges_ are edges in the depth-first forest.  Edge (current, neighbor) is a tree edge if neighbor is unvisited when first exploring that edge.

_Back edges_ are those edges (u,v) connecting u to an ancestor v of u in a depth-first tree.  

_Forward edges_ are those edges (u,v) connecting u to an descendant v of u in a depth-first tree.

_Cross edges_ are all the other edges: between two non-ancestor-related subtrees (i.e. between trees in the depth-first forest or between sub-trees within a given tree).



==== Tree (pre-/in-/post-) order traversal: Recursive approach

Trivial

==== Tree (pre-/in-/post-) order traversal: Iterative approach

Is a DFS, but since a tree has no cycles, the color attribute is not needed.

--------------------------------------------------
void traverse(Node* n) {
  MyStack<Node*> parents; // pop returns top element and removes it
  Node* prev = NULL; // prev is always non-NULL except at the beginning
  parents.push(prev);
  for ( Node* next = NULL; n ; prev = n, n = next) {
    // came from top
    if ( prev==parents.top() ) {
      preorder_visit(n);
      if (n->left) { // go down left
        parents.push(n);
        next = n->left;
      else if (n->right) { // skip left, go down right
        inorder_visit(n);
        parents.push(n);
        next = n->left;
      } else { // skip left, skip right, go up
        inorder_visit(n);
        postorder_visit(n);
        next = parents.pop();
      }
    }

    // came from left
    else if (prev == n->left) {
      inorder_visit(n);
      if (n->right) { // go right
        next = n->right;
      } else { // skip right, go up
        postorder_visit(n);
        next = parents.pop();
      }
    }

    // came from right
    else {
      postorder_visit(n);
      next = parents.pop(); // go up
    }
  }
}
--------------------------------------------------


=== (Greedy) best-first search
*to-do*:

=== A*
A* is an algorithm which solves the single-pair shortest path problem.  A* uses an heuristic, involving the function h, to find the next node to be added to the shortest path tree.  It can also be used to solve the single-source shortest-path problem, but is not intended for it, since h is typically optimized for a specific destination node.  A* is a generalized version of the Dijkstra's algorithm: if h returns always 0 (i.e. is independent of a destination node) then A* equals Dijkstra.

How the algorithm works intuitively (use case `monotonic h'):  Construct the shortest path tree (*spt*) from a given source vertex to a given destination vertex by starting with adding source to the shortest path tree, and then iteratively adding a next vertex to the shortest path tree.  The vertex v to be added to the spt is the one of the yet unadded ones with minimal `estimated shortest path length from source to destination via v'. That is correct due to the monotonic h and the fact that any sub path of a shortest path is itself a shortest path.

Each vertex gets attached two additional attributes: shortest path tree parent (*+spt_parent+*) and shortest path weight from source to this node (*+spwfs+*).  While a node is in the queue, these are tentative values. Once a node is dequed and thus really added to the shortest path tree, they remain at that final value.  In case of +spwfs+ it's value is often called (upper bound) estimate; it will only decrease, never increase during the algorithm, it is thus never smaller than the true value.

As parameter the algorithm takes an heuristic function *h*, or more verbosely, +*estimated_spw_to_dest*(from:vertex)+, which shall return an estimated shortest path weight from the given vertex to the implicit destination vertex. More on h later.

From h another function is derived: ++function vertex::**k**() = .spwfs + estimated_spw_to_dest(this);++.  It returns the estimated shortest path weight from the implicit source vertex to the implicit destination vertex via vertex v.  k() is used as the key for the min priority queue.

Algorithm which assumes h is monotonic:

When dest==NIL the algorithm solves the single-source shortest path problem.  The solution is the shortest path tree given by the .spt_parent attribute of each vertex, along with the .spfs attribute of each vertex.

When dest!=NIL the algorithm solves the single-pair shortest path problem.  The solution is the shortest path from source to dest with a weight of dest.spwfs and given by a linked list from dest to source, starting at dest.spt_parent.

--------------------------------------------------------------------------------
function A*(graph, source:&vertex, dest:&vertex,
        estimated_spw_to_dest:function(:vertex)->double )
    create min priority queue unvisitedQ  where key is vertex::k() <1>
    for all vertices v in graph:
        [v.spt_parent = NIL] // spt = shortest path tree
        v.spwfs       = (v==source ? 0 : INFINITE) // spwf == shortest path
                                                   // weight from source
        unvisitedQ.enqueue(v)

    while unvisitedQ not empty:
        current = unvisitedQ.deque() <2>
        [if current == dest return] // if dest is not NIL
        for each neighbor of current:
            relax(current, neighbor, unvisitedQ) <3>

function relax(u:vertex, v:&vertex, Q)
    alternate_spwfs_for_v = u.spwfs + weight(u,v)
    if alternate_spwfs_for_v < v.spwfs:
        Q.decreaseKey(v, alternate_spwfs_for_v)
        [v.spt_parent = u]
--------------------------------------------------------------------------------

Optimizations and notes to the above algorithm:

<1> Optionally adapt the min priority queue implementation slightly: If +dequeue+ returns NIL if all remaining keys are infinite, we can sooner terminate.  Also the implementation might take advantage of the knowledge that many keys are infinite, especially at the beginning.
<2> Extracting vertex +current+ from +unvisitedQ+ means that  +current.spwfs+ and +current.spt_parent+ are no longer temporary approximations but now have their final value.  Intellectually that means that now the node is really added to the spt, although technically the values have been already modified at some earlier time.
<3> Note that +neighbor+ could already be part of shortest path tree and thus could be skipped.  However it's not worth explicitly checking that (in the presented implementation there would also not be a trivial way to do so), since the if statement within +relax+ implicitely ensures that not much is made with +v+.

Properties of the A* algorithm:

- complete (will always find a path if one exists)
- optimal (*to-do*: i think this explains optimal wrongly: finds the shortest path), but only if the provided heuristic function is admissible.  However see <<bounded_relaxation>> later in the A* sub chapter.
- *to-do:* I don't trust the time complexity, space complexity noted in Wikipedia,  I guess they forget to account for the costs of the min priority queue and of h.  If h is const, then there two options: call it once for every vertex at the beginning and store the result, or call it every time when needed.  Depending on the problem one or the other will be more optimal.
- *to-do:* Does it work for the case when the single-source shortest path problem is tried to be solved?
- Is an best-first search, but not greedy
- *to-do*: Are negative weights allowed? Put it into the beginning of the A* chapter

About the heuristic function h aka +estimated_spw_to_dest(from:vertex)+:

- Responsibility: Returns an estimated shortest path weight from the given vertex v to the implicit destination vertex.
- Should be an admissible (it must not overestimate) heuristic.  If it fails to be admissible, A* is no longer optimal.  However see also <<bounded_relaxation>>.
- Should be monotone (aka consistent: For every edge (x,y): h(x) ≤ edge_weight(x,y)+h(y)).  A* can be implemented more efficiently by not making a node the current more then once (remember the above algorithm is for monotonic h).  Running such an A* algorithm on graph G is the same as running Dijkstra's algorithm on an alternate graph G' where edge_weight'(x,y) = edge_weight(x,y) + (h(y)-h(x)).
- *to-do*: must be constant (during the run-time of the algorithm)?
- Examples how to implement h: euclidean distance.

[[bounded_relaxation]]
_Bounded relaxation_: While the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path.  It is possible to speed up the search at the expense of optimally by relaxing the admissibility criterion.

Relation between diffrent naming schemes. Note that in scheme 1 unvisited means ``it still is in Q'', whereas in scheme 2 unvisited means ``it has never been `seen' in any way (was never the current or the neighbor of the current)''.

[options="header"]
|=====
   |scheme 1           | scheme 2       | scheme 3 | relation to spwfs
.2+|unvisited / in Q   | unvisited      | white    | INFINIT, tentative value
                       | tentative/open | gray     | <INFINIT, tentative value
   |visited / not in Q | closed         | black    | final value
|=====

Note that here `distance' is misleading since in regular graph terminology distance (number of edges) and path weight (sum of edge weights) are not the same thing.

[options="header"]
|=====
|scheme 1           | scheme 2
|spt_parent         | π
|spwfs              | d or distance
|=====

=== Dijkstra's algorithm
Solves the single-source shortest path problem for a weighted graph with non-negative edge weights, able to produce a shortest path tree.  That notably includes directed or undirected graphs and graphs with cycles.

Basic algorithm: Each node has an attribute `shortest path weight from source' which is initially tentative and infinite and then gradually is relaxed until it reaches its final, true smallest value.  A min priority queue, with key being this attribute, contains initially all nodes.  The top node in the priority queue is guaranteed to have the smalles spwfs among all the nodes still in the queue.  Dequeing it semantically adds it to the spt.  At that point its spwfs is at its true value, i.e. is no longer a tentative value.
----------------------------------------------------------------------
// Terminology: spwfs=shortest path weight from source, parent = parent in the
// shortest path tree

Dijkstra(graph, source:vertex [,dest:vertex])
  create min priority queue unvisitedQ where key is the vertice's spfs attribute
  for all vertices v in graph:
    [v.parent = NIL]
    v.spwfs   = (v==source ? 0 : INFINITE)
    unvisitedQ.enqueue(v)
  while unvisitedQ not empty:
    current = unvisitedQ.deque()
    [if current==dest return]
    for each neighbor of current:
      relax(current, neighbor, unvisitedQ)

relax(u:vertex, v:vertex, Q)
  alternate_spwfs_for_v = u.spwfs + weight(u,v)
  if alternate_spwfs_for_v < v.spwfs:
    Q.decreaseKey(v, alternate_spwfs_for_v)
    [v.parent = u]
----------------------------------------------------------------------

Dijkstra a special case of the A* in that that A*'s h function is constant 0, but is a generalization of A* in that that it not only can solve the single-pair shortest path problem but also the single-source shortest path problem. Algorithm when implemented in terms of A*:

----------------------------------------------------------------------
function Dijkstra(graph, source:vertex)
    return Base(graph, source, NIL, lambda(v:vertex){0})

function Dijkstra(graph, source:vertex, dest:vertex)
    return Base(graph, source, dest, lambda(v:vertex){0})
----------------------------------------------------------------------

Time +O(E+V*lg(V))+ if a Fibonacci heap is used to implement the min priority queue.  Using a binary heap it's +O((E+V)*lg(V))+, with an array it's  +O(E+V^2^)+.  Auxillary space: +O(V)+ -- however, the same space amount is also used for an answer which includes all shortest path weights and the shortest path tree.  If the answer must only include the weight of one target node the time and auxiliary space complexity remains the same.

Rational why negative weights are not allowed: The Dijkstraw algorithm relies upon that adding a node to the shortest path does not decrease the shortest path weight.


=== Bidirectional search
It runs two simultaneous searches: one forward from the initial state, and one backward from the goal, stopping when the two meet in the middle.

*to-do*

=== Bellman-Ford
Solves the shortest-path single source problem. Allows for negative weights (which Dijkstra doesn't) and can report negative cycles and abort shortes path computation, that is the path weight for such nodes is undefined.

--------------------------------------------------
fun bellman-ford(G, src:vertex)
    set distance/spt-parent of each node to inf/null
    source.distance = 0
    |V| times
        for each edge e
            relax(e)
    for each edge e
        if relax(e) would relax
            abort, negative cycle found
--------------------------------------------------


=== Floyd–Warshall algorithm
*to-do*


[[topsort]]
=== Topological sort
A _topological sort_ (aka _topsort_, _toposort_, _topological ordering_) of a directed acyclic graph (DAG) is a linear ordering of its vertices such that for every directed edge (u,v) from vertex u to vertex v, u comes before v in the ordering.  Toposort is possible only for DAGs; see <<cycle_dedection>> for how to check.

Algorithm: Augment the <<DFS_all_source,DFS-all-source>> algorithm: when an vertex v is finished, insert it onto the front of the output sequence.  Time complexity +Θ(V+E)+.


[[cycle_detection]]
=== Cycle detection

Algorithms:
- <<DFS>> finds a <<DFS_edge_classification,back edge>>.
- Rocha–Thatte, a distributed algorithm.

Applications:
- Dedect cycles (i.e. problems) in a dependency graph


=== Eulerian trail / Eulerian cycle
An Euler tour in an undirected graph is a walk that traverses each edge exactly once.  If such as walk exists, the graph is called _traversable_ or _semi-eulerian_.  An Eulerian cycle (aka Eulerian tour) in an undirected graph is a cycle that uses each edge exactly once.  If such a cycle exists, the graph is called Eulerian or unicursal.

Hierholzer's algorithm solves the Eulerian cycle problem in linear time: +O(E)+.

Trivia: Was first discussed by Leonhard Euler while solving the famous _Seven Bridges of Königsberg_ problem in 1736.


=== Hamiltonian path / cycle
A Hamiltonian path is a path in an undirected or directed graph that visits each vertex exactly once.  A Hamiltonian cycle (or Hamiltonian circuit) is a Hamiltonian path that is a cycle.  A Hamiltonian cycle is a special case of the traveling salesman problem: adjacent cities have distance one, the others distance two, and verifying that the total distance traveled is equal to n.

Determining whether such a path or cycle exists is NP-complete.


== Concurrency related algorithms

=== Consumer producer

Solution using semaphores.  Allows for multiple producers and consumers.

----------------------------------------------------------------------
Semaphore emptyCount
Semaphore fullCount
Semaphore useQueue

produce:
  wait(emptyCount)
  wait(useQueue)
  putItemIntoQueue(item)
  signal(useQueue)
  signal(fullCount)

consume:
  wait(fullCount)
  wait(useQueue)
  item ← getItemFromQueue()
  signal(useQueue)
  signal(emptyCount)
----------------------------------------------------------------------

*to-do*:
- Solution with monitors
- Question: why isn't it in the above solution good enough to only guard the one critical section with a single binary semaphore?


=== Dining philosophers
*to-do*


== Misc algorithms

=== Horner's method / Horner's scheme

Task: Evaluate a polynomial P(x)=a~0~ + a~1~x + ... + a~n~x^n^ at x=x~0~.  Solution: Since the polynomial can be rewritten as a~0~ + x (a~1~ + x(a~2~+...+x(a~n~)...)) we can solve it beginning at the deepest level and iteratively go outward: b~n~=a~n~, b~n-1~=a~n-1~+x~0~b~n~, ..., b~0~=a~0~+x~0~b~1~ with b~0~ being the solution.  In code, with b~i~ stored in ++acc++umulator:

--------------------------------------------------
double polynomial(double x, const vector<double>& coefficients) {
    double acc = 0;
    for (int i=coefficients.size()-1; i>=0; --i) {
        acc = coefficients[i] + x * acc;
    }
    return acc;
}
--------------------------------------------------


=== Hashes
For terms and variables (+h(k,m)+, +m+, +k+, ...) see <<hash_table>>.  Typically k is an integer of the size of a CPU word. Thus prehashing is used to map any originial key of any size to k.


==== division method
+h(k,m)=k mod m+. In practice not so bad if m is prime and not close to a power of two. Still pretty `hackish'.  Rational for m being prime: When m has common factors with k, the effectively used table size gets divided by the product of those factors: ++k=k%m=(a*f1*f2)%(b*f1*f2)=a%b)++


==== multiplication method
++h(k,m,a,w) =((ak) % 2^w^) >> (w-r)++, where as m=2^r^, i.e. +r=lg(m)+, and the machine stores integers in words of size w bits. a should be odd and not close to a power of two, between 2^r-1^ and 2^r^.

Intuitively: The multiplication mixes up bits, especially in the area from bit (w-r) to w, so we take that area: +% 2^w^+ cuts away the part left of bit w, the shift right cuts way the bits right of bit (w-r).


==== universal hashing



[[ADT]]
== Abstract data types (ADT)
An abstract data type is defined only by the operations that may be performed on it and by mathematical pre-conditions and constraints on the effects (and possibly cost) of those operations.  In OO lingo, it is an interface.  See also <<data structures>>,  which in OO are (non-abstract) classes.

[[collection]]
[[container]]
=== Collection (aka Container)
Grouping of data items.  Generally, the data tiems will be of the same type.

Common operations: Create empty container, report number of objects it stores (size), delete all its objects (clear), insert new objects, remove objects, provide access to stored objects.

[[linear_collection]]
.Linear collections
The elements form a sequence. Example ADTs: <<list_adt>>, <<stack>>, <<queue>> (<<priority_queue>>, <<deque>>, <<depq>>)

[[associative_collection]]
.Associative collections
Given a key, the collection yiels a value. Example ADTs: <<associative_array>> (<<set>> (<<multiset>>))

<.Graphs
Data items have associations with one or data items in the collection. Example ADTs: <<tree_adt>>.


Notably usually not considered a collection: fixed-sized arrays


[[array_data_type]]
=== Array data type

Random access

Implementation:


[[list_adt]]
=== List (aka sequence)

Sequencial access (no random access)

Implementations: linked list, array


[[map]]
[[associative_array]]
[[dictionary]]
=== Associative array (aka map, symbol table, dictionary)
<<collection>> of (key, value) _pairs_ (aka _items_), such that each key appears at most once in the collection.  Specialization of <<multimap>>.

Operations: _insert_ (aka add) a pair, _delete_ (aka remove) a pair, _look-up_ (aka search, find) value associated to a given key.  Optionally also _iterate_ over all pairs, _modify_ (aka reassign), the value of an already existing pair.

Implementations: association list, hash table, binary search tree, radix trees, tries, Judy arrays, ....


[[multimap]]
==== Multimap (aka multihash)
Is a generalization of a <<map>> (aka associative array) in which more than one value may be associated with a given key.  My words: As with <<multiset>>s, this is used in two distinct senses: either equal values are considered identical, and are simply counted, or equal values are considered equivalent, and are stored as distinct items.


[[bag]]
[[multiset]]
==== Multiset (aka bag)
A specialization of an <<associative_array>> in that the value part of the associative array's (key, value) pairs is absent or a sentinel value (like 1).

A generalization of a <<set>> in that it allows duplicates.  This is used in two distinct senses: either equal values are considered identical, and are simply counted, or equal values are considered equivalent, and are stored as distinct items.


[[Set]]
==== Set
A specialication of a <<multiset>> (which in turn is a specialization of an <<associative_array>>), in that no duplicates are allowed.


[[deck]]
[[dequeue]]
[[deque]]
=== Double-ended queue (aka dequeue, deque, deck)
<<linear_collection>> where elements can only be inserted to and removed from either side of the sequence.  Is a generalization of a <<queue>> and a <<stack>> in that elements can be inserted and removed to/from both sides.

Implementations: <<circular_buffer>> which resizes when it's full. <<dynamic_array>>, placing the current elements in its middle, and resize when either side becomes full.

Implemenyed more specialized ADTs: <<collection>>.

Terminology: Deque is the abbrevation of double-ended queue.  Deque (pronounced deck) is the abbbreviation thereof.  Deck is as in an deck of cars, which also provides a good mental image.

See also: - http://www.codeproject.com/Articles/5425/An-In-Depth-Study-of-the-STL-Deque-Container
- C&plus;&plus;'s deque allows random access/insertion, is thus pretty similar to vector. vector vs deque discussions: http://stackoverflow.com/questions/5345152/why-would-i-prefer-using-vector-to-deque, http://www.gotw.ca/gotw/054.htm


[[depq]]
==== Double-ended priority queue (aka depq or double-ended heap)
*to-do*


[[queue]]
=== Queue
<<linear_collection>> where the element removed is prespecified by a first-in-first-out (FIFO) policy.  Is a specialization of a <<deque>> in that insertion is only allowed on one side and removal only on the other side.

Common operations: Elememts can only be added to its _tail_ side (_enqueue_), and only be removed from the other side called _head_ (_dequeue_).  The only element that can be accessed is the one on the head side (_front_ or _peek_).

Common implementations offer +O(1)+ time and +O(1)+ auxiliary space for these operation and +O(n)+ space for the collection aspect.

Common implementations: circular buffer, doubly linked list, singly linked list with an additional pointer to the last node

Implemented more general ADTs: <<collection>>, <<deque>>


[[priority_queue]]
==== Priority Queue
A min (max) priority queue is similar to a queue, however dequeue extracts the element with the max (min) key.  I.e. each element has a key.  Principal operations for a max-(min-)priority queue: _insert_ (aka _enqueue_), _dequeue_ (aka _extract-max_(__-min__)), _peek_ (aka _max_(_min_)), _increase-key_(_decrease-key_).

Sorting and priority queues: If it is possible to perform integer sorting in time T(n) per key, then the same time bound applies to the time per insertion or deletion operation in a priority queue data structure (Thorup 2007.  It's however a complicated reduction).  *to-do*: elaborate more on relation sorting to priority queues

Common implementations: <<heap>>, self-balancing binary tree


[[stack]]
=== Stack
<<linear_collection>> where the element removed is prespecified by a last-in-first-out (LIFO) policy.  Is a specialization of a <<deque>> in that insertion and removal are only allowed on one single side.

Main operations:  Insertion is often called _push_ and can be only to one side called _top_.
Removal is often called _pop_ and can only be the element at the top end.  The only element that can be accessed is the one on the top end of the stack (_top_ or _peek_).

Implementations: <<array>>, <<linked_list>>.



[[tree_ADT]]
=== Tree
Note that there is a distinction between a tree as 1) an abstract data type,  2) a data structure and 3) a topic in graph theory.

Terms (see also those of <<graph>>):

- _siblings_: nodes with the same parent.
- _cousins_: nodes with the same grand parent.
- _internal node_: A node with at least one child.
- _external node_ (aka _leaf_): A node with no children.
- _degree_: Number of sub trees of a node
- _level_: *to-do*: i don't understand that
- _height of tree / node_: Largest distance (see <<graph>>) between root / that node and any leaf.
- _depth_ of node: Distance from root to that node.
- _forest_: A set of zero or more disjoint trees.
- _ancestor_: Whether or not a node is it's own ancestor is not defined across literature. CLRS say yes.p
- _descendant_: Whether or not a node is it's own descendant is not defined across literature. CLRS say yes.


Implementations: See those of <<graph>>,  and the methods for storing a <<binary_tree>>


[[graph]]
=== Graph
In the following, no distinction is made between the term graph referring to an specific abstract data type and the term graph referring to a topic in mathematics.

See also http://en.wikipedia.org/wiki/Glossary_of_graph_theory

A graph +G=(V,E)+ is given by a set of _vertices_ +V+ (aka _nodes_) and a set of _edges_ +E+, each edge being an pair of elements from +V+. In an undirected graph the two vertices of an edge are said to be _adjacent_ to each other; in an directed graph only the dst vertex of an edge is adjacent to the src vertex.  An _undirected graph_ is one in which edges are an unordered pairs; the edge ++(a,b)++ is identical to the edge ++(b,a)++. An _directed graph_ (aka _digraph_) is one in which edges are ordered pairs and are also called _arcs_, _directed edges_ or _arrows_.  A _loop_ is an edge which starts and ends on the same vertex.  _Link_ is an edge with two different ends.  A _simple_ graph is an undirected graph that has no loops and at most one edge between any two different vertices.  A _path_ (aka _walk_, however Wikipedia says that path commonly refers to an open walk) in an undirected graph is an ordered sequence of vertices {v~1~,...,v~n~} such that v~i~ is adjacent to v~i+1~.  A _closed walk_ is one where the first and last vertices are the same, an _open walk_ is one where they are different.  A _trail_ is a path in which all edges are distinct.  A _simple path_ does not have any repeated vertices.  The _path weight_ is the sum of the weights of its constituent edges.  The _shortest path_ from vertex u to v is any path with minimal path weight.  The _shortest path weight_ is the path weight of the shortest path; defined to be infinite if there is no path.  Any sub path of a shortest path is itself a shortest path.  The _distance_ between two vertices in a graph is the number of edges in a shortest path.  A _sparse_ graph is one for which +|E|+ is much less than ++|V|^2^++.  A _dense_ graph is one for which +|E|+ is close to ++|V|^2^++.  A _(pre/in/post)order tree walk_ does the key action with the current node's payload before/between/after recursively calling the children.  A _clique_ in an undirected graph is a subset of its vertices such that every two vertices in the subset are connected by an edge.  A _vertex cover_ is a set of vertices such that each edge of the graph is incident to at least one vertex of the set.  An _independent set_ (aka _stable set_) is a set of vertices, no two of which are adjacent.

In an undirected graph, two vertices are called _connected_ if there is a path between the two, otherwise they are called _disconnected_. A graph is said to be connected if every pair of vertices is connected.  A _connected component_ (or just _component_) of an undirected graph is a subgraph which is connected and is not connected to any vertices of the supergraph.  A directed graph is called _strongly connected_ (or _strong_) if it contains a path from u to v and from v to u for every pair of vertices u and v.  A _strongly connected component_ of a directed graph G is a subgraph that is strongly connected, and is maximal with this property: no additional edges or vertices from G can be included in the subgraph without breaking its property of being strongly connected.  If each strongly connected component is contracted to a single vertex, the resulting graph is a directed acyclic graph, the _condensation_ of G.  A _cut_ (aka _vertex cut_, or _separating set_) of a connected graph G is a set of vertices whose removal renders G disconnected.  A _complete graph_ with n vertices, denoted Kn, has no vertex cuts at all, but by convention κ(Kn) = n − 1.  The _connectivity_ (or _vertex connectivity_) κ(G) (where G is not a complete graph) is the size of a minimal vertex cut.  A graph is called _k-connected_ (or _k-vertex-connected_) if its vertex connectivity is k or greater.

A _tree_ is a conncted acyclic graph.  A _spanning tree_ of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G.  A _minimum spanning tree_ (aka _MST_) of is a spanning tree of G with the minimal total weighting for its edges.

A _flow network_ (aka _transportation network_) is a directed graph where each edge has a _capacity_ and a _flow_ which can no exceed the capacity.  The amount of flow into a node must equal the amount of flow out of it, unless the node is a _source_ or a _sink_. _maximum flow problems_ involve finding a feasible flow through a single-source, single-sink flow network that is maximum.  It can be seen as a special case of the more complex problems, e.g. the circulation problem.   The _max-flow min-cut theorem_ states that the maximum flow is equal to the minimum capacity over all possible s-t edge cuts.  An s-t edge cut is an edge cut such that one of the resulting component contains the source and the other component contains the sink.  The capacity of an edge-cut is the sum of the capacities on the cut edges.  Solutions of max-flow: _Ford–Fulkerson algorithm_ +O(E*f)+ for the case where capacities are integers, whereas f is the maximum flow.

Common problems: _single-pair shortest path_ problem: from single source to a single destination, _single-source shortest path_ problem: from a single source to all others, _single-destination shortest path_ problem: form all others to a destination, _all-pairs shortest path_ problem: from all to all.  The chapter <<NP_complete>> also lists some graph problems.

Common implementations:

- _Adjacency list_. Typically for sparse graphs.  Collection of unordered lists, one for each vertex.  There sub-forms how to implement an adjacency list:
 * Objects representing nodes, each having a list of pointers to its adjacent nodes.  Optionally these pointers can also point to an object representing an edge, which allows storing data on edges.
 * An associative array associates each vertex (being the key) to an unordered list of its adjacent vertices (being the value).  For the associative array, often a hash table is used.  If the key can be an integer, e.g. when the vertices are enumerated, then a simple array can be used.
- _Adjacency matrix_.  Rows represent source vertices and columns represent destination vertices and cells the associated edge.  Data on vertices typically stored externally.  Typically for dense graphs, or when a quick way is needed to tell if two vertices are adjacent.
- _Incidence matrix_.


[[data_structure]]
== Data structures
A concrete particular iway of organizing data in memory.  In OO lingo, its is a (non-abstract) class.  See also <<ADT>>, which is in OO lingo an interface.


[[table]]
[[array]]
=== Array data structure (aka table)
Fixed size, +Θ(1)+ time for indexing, with a very low constant factor.  ++O(0)++ wasted space.  Due to the fixed size, elements cannot be added / removed.


[[dynamic_table]]
[[dynamic_array]]
=== Dynamic array (aka array list, dynamic table, resizeable array)
In contrast to <<array>> the size is variable, thus allows elements to be added / removed.  _Capacity_ is the number of elements the container could currently hold, and the _size_ is the number of elements it actually currently contains.

[[table_doubling]]
==== Table doubling
When size=capacity upon an insertion, create a new table with double the capacity and copy all elements over -> insertions are +Θ(1)+ amortized.  Upon deletions, when you don't mind slack, never resize the table (as the STL does), or half the capacity when size drops below capacity/4. In that case both insertions and deletions are +Θ(1)+ amortized. (You can't half the capacity when the size reaches half the capacity because in a sequence like inserting/deleting/inserting/deleting, each operation could encompass a table resize which would mean +O(n)+ per operation.)  Of course, other constants than 2 can be used, as long as the factor which is to do shrink is greater than the factor to enlarge.

One can get +Θ(1)+ by roughly this idea: When you remark that you start to get full, start a new table with a larger capacity, initially empty.  On each insertions operation, copy a constant amount of items from the old table to the new one.  Once the old table is really full, just switch over to the new table.  All in all it's quite complicated, so it's not that often used.


[[linked_list]]
=== Linked list

Implementation of the ADT <<list>>.

Orthogonal properties:

- Singly, Doubly or Multiply linked
- Circular linked yes/no
- Sentinel nodes yes/no


[[circular_buffer]]
=== Circular buffer (aka cyclic buffer, ring buffer, circular queue)
Uses a single, fixed-size buffer as if it were connected end-to-end.

Internally uses 1) an array which's size equals circular's buffer capacity, 2) an pointer (or index) to the first element and 3) one to the last element.  Pointers in a circular buffer wrap around at the underlying array border (array.first and array.last (according array.size=circular_buffer.capacity)).

Implements the ADT <<queue>>

Difficulties:

- Depending on the exact implementation, distinguish the case that the buffer is empty and that it is full is not possible, because in both cases start and end point to the same element.


=== Direct-address tables
Be U the universe / set of possible keys.  Time: +O(1)+ worst average best case.  Space: +O(|U|)+.

A _direct address table_ is an array of size |U|.  A key's value is the index into the array where the data corresponding to the key is stored.


[[hash_table]]
=== Hash tables
Terms/Variables: +m+ is the table size. +k+ is the key of the item to be put into the hash table, i.e. the input to the hash function. Is an integer, theoretically of any size, e.g. tousands of bits, but see below. +h(k,m)+ is the hash function mapping keys to table slots. It's codomain is thus [0,m).  +n+ is the number of elements stored in the table. +α=n/m+ is the _load factor_ of the table.

Basic idea: Implements <<associative_array>> ADT; more terms there.  Initial idea is to store all values in a <<dynamic_table,(dynamic) table>>  with the keys (being (large) integers) as index.  However that obviously uses way too much space.  So make the table smaller, that is of size +m+.  Use the hashfunction +h(k,m)+ to map from a key +k+ to an index into the table.

Access time is +O(k)+ best/average case, ++O(k+n)++ worst case.  Part of that time is the time needed to generate the hash, which is +O(k)+.

When two keys hash to the same slot that is called a _collision_.  If no collision occurs, that is the best case, and the overall access time is +O(k)+ as noted above.  Depending on the collision resolution algorithm, in the worst case all slots are tried, and overall access time is ++O(k+n)++ as noted above.

See also <<table_doubling>>.


==== Chaining
Each table slot has associated an sequence of items, typically a singly linked list. The expected chain length is the table's load factor.

Insert/delete/find: +Θ(1)+ (+Θ(1+loadfactor)+, but when the the loadfactor is a constant (relative to n), it becomes +Θ(1)+).

m should be +Θ(1)+. If +m+ is too small, the loadfactor is too high, in the worst case not constant (relative to n) anymore.  That would lead to hash table operations not being +Θ(1)+ anymore.  If +m+ is too small, we waste space.


=== Association list
Is an implementation of the ADT <<associative_array>>.

*to-do*: more details


[[binary_tree]]
=== Binary tree
A <<tree>> data structure in which each node has at most two children.  Note that a <<binary_search_tree>> is something else with more restrictions.

Properties:

- _full_(aka _proper_): Every node other than the leaves has two children.
- _perfect_: (aka ambiguously (see next) complete): A full binary tree in which all leaves have the same depth
- _complete_: Every level, except possibly the last, is completely filled, and all nodes are as far left as possible.
- _balanced_: *to-do*:
- _degenerate_ (aka _pathological_): Each node has at most one child.  The tree is thus effectively a linked list.

Methods of storing:

- See <<graph>>
- As an implicit data structure in an array.  Be i the current node's index, 0 the first index, then its parent is at index floor((i-1)/2), its right child at 2i+1 and its left child at 2i+2.  In the case of a complete binary tree, no space is wasted.  See also <<binary_heap>> which commonly uses this scheme.


[[binary_search_tree]]
=== Binary Search Tree (aka BST, ordered/sorted binary tree)
Is a specialized <<binary_tree>> where 1) each node has a comparable key 2) for each node: the key of the left child, if child present, is smaller than the node's key, and the key of the right child, if present, is larger than the node's key.  Be +n+ the number of elements.  +h≥lg n+ the height of the tree.  The expected height is +h=lg n+ for a randomly built binary tree.

_binary search tree property:_ If node +y+ is in the left subtree of node +x+, then +y.key<=x.key+, if +y+ is in +x+'s right subtree, then +y.key>=x.key+.

_Search_ key +k+: +O(h)+. Recursively or iteratively, for current node +x+, if +k<x.key+ continue with left subtree, else right subtree.

_Min_/_Max_: +O(h)+. Follow left/right subtree until the leaf is reached.

_Successor_/_Predecessor_: +O(h)+. **To-do**

Is an implementation of the ADT <<associative_array>>.


=== Red-Black / AVL Tree
For each node, we maintain an extra attribute, it's color, which is either black or red.  NIL is treated as leaf and as external node.  The tree is approximately balanced, no path is more than twice as long as any other.  A red-black tree with n internal nodes has height +h≤lg(n+1)+.

_Properties_: 1) Every node is either red or black 2) the root is black 3) NIL is black 4) if a node is red, then both children are black 5) For each node, all simple paths from the node to descendant leaves contain the same number of black nodes.


_Left/right rotation_: +O(1)+. preserves the binary-search-tree property.

_Insertion/deletion_: +O(lg n)+. Implementation not trivial.

An _AVL_ tree is height balanced.  For each node, the height of the left and right subtree differ at most 1.  For each node, we maintain an extra attribute, it's height +h+.


=== B-tree

*to-do*:

=== Skip list
**to-do**


[[heap]]
=== Heap
A _heap_ is a specialized tree-based data structure that satisfies the _heap property_: If node A is a parent node of B, then the key of node A is ordered with respect to the key of node B with the same ordering applying across the heap.  In a _max heap_ the parent node key is greater than or equal to those of the children, in a _min heap_ it's smaller than or equal.  Thus the element with the largest (max heap) / lowest (min heap) key is always stored at the root.  Note that there is no implied ordering between siblings or cousins.

Time complexities for binary, binomial, Fibonacci, pairing, Brodal, rank pairing, strict Fibonacci:

Creation::
- create-heap: create an empty heap
- make-heap (aka build-heap aka heapify): create a heap out of given elements. +O(n)+ binary, others *to-do*.
- union (aka merge): +Θ(m lg(n+m))+ binary, +O(lg(n))+ binomial, +Θ(1)+ others

Inspection::
- min (max) (aka peek or find-min/max): +Θ(1)+
- size()

Modification::
- extract-min(-max) (aka pop): +O(lg(n))+
- insert: +Θ(lg(n))+ binary, +Θ(1)+ others
- decrease-(increase-)key: +Θ(lg(n))+ binary & binomial & pairing,  +O(1)+ others

Applications of heaps:

- The heap data structure is one maximally efficient implementation of the <<priority_queue>> ADT.
- Merge sort
- Dijkstra's shortest-path algorithm
- Order statistics

A heap data structure should not be confused with `the heap' which is a common name for the pool of memory from which dynamically allocated memory is allocated.


[[binary_heap]]
==== Binary heap
In a _binary heap_ the tree is a complete <<binary_tree>>. *to-do*: study implementation of the basic operations.

--------------------------------------------------
        0
    1       2
  3   4    5  6      0123456789 array index
 7 8 9               01-2---3-- tree level
--------------------------------------------------

parent(i) = floor((i-1)/2)
right-child(i) = (i+1)*2 - 1
left-child(i) = (i+1)*2


- +heapify(i)+.  Assumes that children of node +i+ are max heaps, but +i+ might violate the heap property.  Time: +O(lg(nst))+, where nst are the number of nodes in the sub tree rooted at i.
- +build_heap()+:  Converts an array into a heap.  Common implementation: in a bottom-up manner, for each node, starting at one-before-leaf-height height, call +heapify+.  Time: +O(n)+.


==== Fibonacci heap

*to-do*:

=== Treap

*to-do:*

=== Trie (aka digital tree, radix tree, prefix tree)
Let k be the length of the search key.  Let n be the number of elements in the trie.  Time complexity is +O(k)+.

A trie can be seen as a DFA (Deterministic finite automaton) without loops.  A trie can be compressed into an DAFSA (deterministic acyclic finite state automaton).  A trie eliminates prefix redundancy.  A DAFSA additionally also removes suffix redundancy.

Is an implementation of the ADT <<associative_array>>.

Compared to a hash table:

- Trie has predictable look-up time +O(k)+.  A hash table has +O(k+n)+ time complexity worst case:  O(k) is used to generate the key, looking up the key is O(1) average but O(n) worst case.
- A trie does not need a hash function
- A trie can provide an alphabetic ordering of the entries by key.  I.e. a trie supports ordered traversal.
- Locality is worse for a key, since it randomly accesses the nodes.
- A trie typically uses more space than a hash table, since the graph uses quite a lot pointers, and typically one pointer equals 4 characters.


=== DAFSA as data structure
Represents a finite (since it has no cycles) set of strings aka keys.  Single source vertex.  Each edge is labeled by a letter / symbol.  Each vertex has at most one vertex which is labeled with a given letter.  The accepted strings are formed by the letters on paths from the source to any sink / NIL vertex.

Can be seen as an compact form of a trie.  Uses less space than a trie.  A trie eliminates prefix redundancy.  A DAFSA additionally also removes suffix redundancy.  A trie can store attributes for each string aka key, whereas a DAFSA cannot.

Is an implementation of the ADT <<associative_array>>.


=== Radix tree (aka radix trie, compact prefix tree)
A radix tree is a space-optimized trie, where each node with only one child is merged with its parents.  That each child is no longer labeled with a single character potentially with a string.

Be k key length, n the number of members in the data structure.  Lookup, insertion, deletion have time complexity +Θ(k)+.

Is an implementation of the ADT <<associative_array>>.

Compared to a binary tree:

- Binary tree has +O(k * (lg n))+ time complexity for look-up, insertion, deletion.  Mind that comparing a key requires +O(k)+; in many times the worst-case occurs, due to long prefixes towards the leaves.


== Augmenting data structures

1. Choose an underlying data structure DS.
2. Determine additional information AI to maintain in DS.
3. Verify that we can maintain AI for the basic operations on DS.
4. Develop new operations.

Let +f+ be an attribute that augments a red-black tree +T+, and suppose that the value +x.f+ for each node +x+ only depends on only the information in the nodes +x+, +x.left+ and +x.right+. Then we can maintain +f+ in all nodes of +T+ during insertion and deletion without affecting the +O(lg n)+ performance of these operations.


== Misc. related computer science


=== NFA
*to-do*:


=== DFA
*to-do*:


=== DAFSA
*to-do*:


=== Bit manipulation
A _nibble_ is a four bit aggregation (aka _halb-byte_ or quartet).

--------------------------------------------------
set:    x |=  y
clear:  x &= ~y
toggle: x ^=  y
test:   x &   y
--------------------------------------------------

In C / C&plus;&plus;, +CHAR_BIT+ is the number of bits in a byte.

- http://graphics.stanford.edu/~seander/bithacks.html

=== Misc terms
Sentinel:: A sentinel is an object to represent the end of a data structure.


== References
- http://ocw.mit.edu/courses/civil-and-environmental-engineering/1-204-computer-algorithms-in-systems-engineering-spring-2010/lecture-notes/
- https://www.quora.com/What-are-the-very-basic-algorithms-that-every-Computer-Science-student-must-be-aware-of



// Local Variables:
// eval: (visual-line-mode 1)
// eval: (auto-fill-mode -1)
// eval: (filladapt-mode -1)
// compile-command: "asciidoc -a toc -a icons algorithms_and_data_structures.txt"
// End:

//  LocalWords:  pre th ADT Multimap multihash multimap emptyCount fullCount
//  LocalWords:  useQueue putItemIntoQueue getItemFromQueue Treap DAFSA Deque
//  LocalWords:  BST spw spwfs decreaseKey spt dest unicursal eulerian NPC
//  LocalWords:  Königsberg Hierholzer's subgraph supergraph Horner Horner's
//  LocalWords:  adaptors acc umulator Quickselect supremum infimum CLRS DFS
//  LocalWords:  AStarMonotonicH AStar toposort topsort BSF ith preorder args
//  LocalWords:  inorder Sedgewick Karp textlen patternlen patternhash str
//  LocalWords:  texthash issubstring rollinghash len BFS MyStack prev Thorup
//  LocalWords:  Brodal mergeable
