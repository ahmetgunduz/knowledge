// The markup language of this document is AsciiDoc
:encoding: UTF-8
:toc:
:toclevels: 4


= Big Data

== Intro

[[data_technology_stack]]
Data technology stack:

[cols="1,3"]
|=====
| user interfaces |
      Excel, Access, Tableau, Qlikeview, BI tools
| querying |
      SQL, XQuery, MDX, SPARQL, REST APIs
| data stores |
      RDBMS, MongoDB, CouchBase, ElasticSearch, Hive, HBase, MarkLogic, Cassandra
| indexing |
      Key-value stores, hash indices, b-trees, geographical indicies, spatial indicies
| processing |
      two-hase processing: MapReduce / dag-driven proc: tez, spark / elastic computing: EC2
| validation |
      XML schema, JSON schema, Relational schemas, XBRL taxonomies
| data models |
      Tables: Relational model, column store, wide column store / trees: XML Infoset, XDM / graphs: RDF / Cubes: OLAP / Unstructured text / key-value model
| syntax |
      Tables: CSV / trees: XML, JSON / graphs: RDF, graph / Cubes: XBRL / Unstructured text
| encoding |
      ASCII, ISO-8859-1, UTF-8, BSON
| storage |
      local FS, NFS, GFS, HDFS, S3, Azure blob storage, DynamoDB
|=====

|=====
|           | master              | slave
| Spark     | cluster manager     | worker node
| Hadoop v1 | job tracker         | task tracker
| YARN      | resource manager    | node manager
| YARN      | application manager | application master
| YARN      | application master  | task
| HBase     | HMaster             | region server
| HDFS      | name node           | data nodes
|=====

_througphut_: amount of data per time

_transfer time_: time to transmitt a given amount of data, excluding latency.

_latency_: Time between asking for something and receving the first bit of that something. Some authors use the term latency to mean what is defined here as total response time.

_total response time_ (or, conflictlingly, latency): Latency + transfer time

The history of storage: progress made 1956-2010: capacity: 150'000'000 times more, throughput 10'000'000 times more, latency 8 times more. To increase throughput, we can parallize. To improve latency, we can do batch processing.

Very rough typical measures:

|=====
| Instruction    | 1 ns
| Fetch L1 cache | 1 ns
| Fetch memory   | 100 ns
| Disk           | 100-300 Mbit/s
| Ethernet       | 1-10 Gbit/s
| Roundtrip packet US Europe | 150ms
|=====

How can we get more work done:

1) Make SW efficient. ``You can have a second computer once you've shown you know how to use the first one'' (Paul Barham). We can gain factors of speed, and we have to pay once the development costs, and can apply it to all machines we ever will have.

2) _horizontal scaling_ (or _scale out_): Add more nodes, typically commodity HW. Price grows about linearly with overall computing power.  Typically useful when the bottleneck is disk throughput, as opposed to CPU.

3) _vertical scaling_ (or _scale up_): Replace a node with a more powerfull node. Either by completely replacing, or by adding more RAM and/or CPUs. Price grows about exponentially with overall computing power.  Nowadays scaling up the CPU typically means more cores, as opposed to not long ago, where it typically meant faster.

_Amdahl's law_: speedup = 1 / ((1-p) + p/s). Say in the current configuration, p percent of the execution time is directly affected by the following parallelization. The raw speedup (newtime/olddtime) in isolation is s. Reflects the viewpoint of constant problem size (but in reality, the parallization makes the problem harder).

_Gustafson's law_: speedup = (1-p) + sp. For p and s, see Amdahl's law. Reflects the viewpoint of constant computing power. A higher Gustafson's speedup means you can fit more people on a single cluster.

_Scalability! But at What COST?_: The paper reminds that scalability is only a mean to achieve the real goal, which is performance. So in a way it re-states Paul Barham's ``You can have a second computer once you've shown you know how to use the first one''.  They showed multiple different real-world scalabale programs solving a given problem running on 128 cores. They were however beaten by a single thread program. Even more so if the single threaded program did use an efficient non-parallel algorithm. They proposed the measurement `COST': Configuration that Outperforms a Single Thread. Or more colloquially, how many cores does the scalable system require until it gets faster than an single threaded system solving the same problem. Note that clusters may have other benefits besides (hopefully) better performance through scalability. For example fault tollerance. But even then you still have to ask yourselve, wether the increased complexity of the cluster really helps you.

*to-do* https://www.youtube.com/watch?v=6bWBEJBMNG0. I still don't get why he can be so much faster. Both in examples with single threaded, and when he also parallelizes with a few cores on his laptop. I think it's neither the case that he's a genius, nor that the others are dumb. Or were the others _really_ that careless?

_Design principles_ of big data (by Fourny Ghislain Gilles)

- Learn from the past / don't reinvent the wheel (e.g. we need schemas, query languages encapsulating the used data model)

- Simplicity

- Modularize the architecture / make good abstractions (see <<data_technology_stack>>)

- Homogeneity in the large (e.g. blocks in HDFS, regions in HBASE, virtual nodes in chords). So at the large, things are easy to handle, which allows us to scale out.

- Heterogeneity in the small (e.g. be able to add columns in HBase, a document might be missing a field or have an additional field). Heterogenity gives flexibility to the client.  But the cost of increased complexity does not affect the system in the whole.

- Separate metadata from data (i.e. make schema optional, aka schema on read)

- Abstract (separate) logical model from its physical implementation

- Shard the data

- Replicate the data

- Use lots of commodity HW

_choosing optimal chunk size_. One extreme is to do the splitting of the data shuch that one split / chunks fills the capacity of one `executor', the other is really small splits. When the splits are too small, we run into latency issues. We have to pay latency for each access to the many chunks.  If the splits are too large, theres not enough flexibility; in reality the splits don't have exact sizes; it is difficult to completely make use of the ressources of an executor if the chunks are too big (think a bar which is filled horizontally with different chunks, if the chunks are around half the size of the bar, 50% of the bar is unused). As a rule of thumb, make the split size one tenth of the executors capacity.

_optimize network usage_: Try to push down prejection and selection as close as possible to the source. Then less data has to be transmitted. That's e.g. what Spark's DataFrame do, as opposed to Spark's RDDs.

A database _transaction_, by definition, must be _ACID_: All the following must be guaranteed even in the event of errors, power failures etc. _Atomicity_ (each transaction succeeds completely or fails completely), _Consistency_ (each transaction brings DB from one valid state to another valid state, maintaining DB's invariants), _Isolation_ (result is as if transactions were executed in strict sequence), _Durability_ (once a transaction has been committed, will remain committed).

_Consistency models_: _Strict consistency_: Changes are atomic and appear to take effect instantaneously. _Sequential consistency_: Every client sees all changes in the same order they were applied. _Causial consistency_: All changes that are causally related are observed in the same order by all clients.  _Eventual consistency_: If no updates are made, then eventually all accesses will return the last updated value. However in practice there's a continous stream of updates, so consistency will never happen.  In other words: Every update will eventually be propagated. _Weak consistency_: Clients may see updates out of order, or may not see an update at all.

_Availability_: Measure of the percantage of time the service / equipment is in an operable state. A common measure is "99.99%" (with x many nines).

_Reliability_: Measure of how long the service / equipment performs its intended function. Usually measured by _mean time between failure_ (_MTBF_) which is defined as total time in service / number of failures, or by _failure rate_, which is defined as the inverse of MTBF.

_Durability_: A common measure is "loss of 1 in x objects".

_Response time_: One possible measure is "<10ms 99.9% of cases"

_3Vs of big data_: volume, variety and velocity *to-do*

_Load balancing_: *to-do*, Partition schemes

_batch processing_: *to-do*

_data independence_: *to-do*

_sharding_ (or _horizontal partitioning_): Partition the data, and give each shard (aka partition) to a different machine. Thus the load per machine is reduced, we can operate in parallel, and it allows for scaling out. Having one shard per customer (or one shard for european customers, one for north american customers etc)  is also considered a typical form of sharding.

_vertical partitioning_: *to-do*

_Replication_: Rational: Fault tolerance. Local: node failure. With a lot of nodes, you are almost guaranteed that a node will fail. Regional: natural catastrophe. Thus spreading datacenters gives proximity to client (gives smaller latency) and protects against regional failures.

_Storage classes_: High availability at high costs on one end and low availability (hours to access data) at low cost on the other end. The low end is typically for backups.

_structred_ vs _unstructred_: As so often, there are shades of gray. Text is unstructured. A `messy' XML / JSON document is semi-structured, close to unstructured. A `nice'  XML / JSON document is semi-structured, close to structured. ProtocolBuffer / XML-with-DTD is structured.

_schema_ / _validation_: One validates a document against a schema.

_scema on write_ means that you first need to define a schema before you can insert new data. When inserting it is ensured that the data is valid w.r.t. the schema. E.g. a relational DB. _schema on read_ means that data is verified against a schema only upon request, e.g. after reading the data. I.e. data and schema are decoupled. E.g. XML.

_value space_: What the set of valid values of a type logically are. _lexical space_: how these values are represented on disk.

Random notes:

- Random access to pages is generally expensive, or the other way round, sequencial access is much faster
 * binary search is a bad option

- Dealing with (multi)sets, i.e. unordered collections, as most SQL queries do, has the advantage that it is more parallelizable as when it had to be ordered.

- Typical disk block sizes are 0.5kB to 4kB. Virtual memory page size is typically 4kB. Typicall a DB does I/O in 64kB blocks.

- _data center_: ~1k - 100k machines, 1-100 cores / server, 1-12TB local storage / server, 16GB - 4TB RAM / server. 1GB/s network bandwith for a server. A rack consists of nodes.


=== CAP Thoerem

The _CAP theorem_ is about the following impossibility triangle (you can have at most 2 of 3): you only can have two, but never three.

- _consistency_: every read receives most recent write or an error; if not consistent, we have to deal with conflicts somehow

- _availability_:  every request (except under network partition) receives a non-error response (conversly, not having A at all means always getting an error) with low latency (low being subjective, making availability subjective).

- _partition tolerance_: system continuous to operate despite an abitrary number of messages being dropped/delayed by network between nodes.

*to-do* point out that consistency and availability in the context of the CAP theorem mean different things than the same terms in the context of ACID.

CP examples: HBase, MongoDB, Redis, MemcacheDB, Big-table like systems

CA examples: Traditional relational data bases (PostgreSQL, MySQL, etc.)

AP examples: Dynamo-like systems, Voldemort, Riak, Cassandra, CouchDB

A: always a error response

CA: always (except network partition case) non-error repsonse, read always returns most recent write. E.g. one maschine is web server which handles client requests, behind is a node having a traditional DB server providing ACID. As long as there's no network partition, we have consistency and availability. If we have network partition (link between server and DB goes down), then the client's requests are answered by errors (CA says that we don't have the P).

AP: always non-error response, even in case of network partition, but maybe a read doesnt return most recent write.

CP: Something like DynamoDB, where the coordinator writes synchronously to the replicator nodes. During the write, which might take a long time because we might have to wait until the network partition is over, the coordinator can't serve further request, thus availability goes away.

Doing updates (i.e. propagation to other nodes) asynchronous gives you availability, because you still can update. If you are synchronous, you can be consistent, but you are no longer available.

*to-do* `consistency' in CAP and in ACID are not the same? In a distributed data base, where each node replicates the full data base, does consistency refer to a single data base or to the global database?


=== REST

_REST_ API (Representational State Transfer): REST is the way HTTP should be used. It's always a method (GET, PUT, DELETE, POST, ...) plus a resource (URI). PUT must be idempotent (when issued multiple times, the 2nd plus requests have no effect). GET must be side-effect free.  POST is the most generic, it can have side effects.


=== OLAP, OLTP, Data Warehouse

_On-Line Transaction Processing_ (_OLTP_): `Traditional' database operations. Typically there are _lots_ of operations (aka transactions), each only touching a small portion of the database. E.g. there are millions of customers, or products, but you query only one.  Tables are typically normalized to avoid anomalies. Typically there are a lot of writes (but as any operation in OLTP, touches only a small portion of the database, e.g. insert a new record or modify one existing record). Typically very fast, i.e. usable in an interactive environment.

_On-Line Analytic Processing_ (_OLAP_) is examination of data for patterns or trends in order to make decisions. This generally involves few highly complex queries that typicially touch large portions of the database. Typically tables are denormalized (introducing risk of anomalies) in order to make queries fast. Typically there are a lot of reads and (almost) no writes. Typically quite slow, e.g. grab a coffe or overnight. The term OLAP can be viewed in a wide sense where it includes also things such as Spark and MapReduce, but historically some people view it in a more narrow sense where OLAP means data cubes.

OLAP applications commonly take place in a readonly copy of the master database (or multiple source data bases), called a _data warehouse_. OLAP queries and OLTP querries are generally different, so a dataware house has a different architecture than a traditional database. The data warehouse is readonly; however there might be incremental updates from the source data bases. Typically implemented via data cubes.

The process of creating the data warehouse from the source databases is also called _extract transform and load_ (_ETL_). The subject of the query (is it about customers, or sales, or ...) might influence the resulting data warehouse. Often the copy is made overnight, and thus is up to one day out of date. The transform step includes things such as normalizing data (not in the RDBMS sense) (e.g. turn all of {mister, monsieur, Herr, ...} into one single form, say mister), otherwise cleanup the data for easier OLAP queries, filter, join etc. The load step means actually creating the dataware house data (often a cube) from the data comming out of the transformation step. The load step includes checking integrity constraints, store in sorted order, build indicies, partition the data.

Implementations of OLAP:

[[ROLAP]]
_ROLAP_ (_relational OLAP_): OLAP is implemented on top of a RDBMS by using a so called _star schema_. There is one table which stores the facts table. The facts table is visually the center of the star. The columns representing the dimensions are the key of the facts table, the columns representing the measures are the dependend columns. Each dimension (i.e. column in the facts table), can optionally have satelite data, which is stored as another table, called _dimension table_. E.g. the customer dimension / column in the facts table stores customer ids, and there is a "customer" dimension table having customer id as key, and further dependend columns such as name, address etc. So a dimension table describes all possible values of a given dimension. The facts table column this dimension table is associated with is a foreign key. As an extension to the star schema, if you normalize the dimension tables, you get the so called _snow-flake schema_.

_MOLAP_ (_multi-dimensional OLAP_): OLAP which stores its data in an optimized multi-dimensional array storage. The physical layer is typically directly memory and/or disk.

_HOLAP_ (_hybrid OLAP_): Mix of ROLAP and MOLAP.

OLAP query Langagues (recall that in a wide sense, OLAP is not restricted to cubes):

- In case of ROLAP: SQL as a low level query language, on top of which higher level OLAP query languages such as MDX can sit.

- In case of cubes: MDX.


== Relational model

A _data model_ is a collection of high-level data description constructs that hide many low-level storage details. Most DBMS today are based on the _relational data model_, in which there's a single way to represent data: A _relation_ (or _table_) represents data as a two-dimensional table. The _schema_ of a relation relation describes the relation by specifyinig its name and the name and _domain_ (aka _type_) of its _fields_ (aka _attribute_ or _column_). Think of a relation as a type; concrete instances thereof are called, well _(relation) instances_. An relation instance is a set (not list) of _(data) records_ (or _row_ or _tuple_).  A record has one _component_ for each attribute the relation. _Integrity constraints_ are conditions that each record must satisfy.  A _block_ (or _page_) is the unit of transfer for disk I/O.

Levels of abstraction:

- Views describe how users see the data
- Conceptual schema defines logical structure
- Physical schema describes the files and indexes used

--------------------------------------------------
                 Query Optimization
                 and Execution
                       |
                       V
                 Relational Operators
                       |
                       V
            +--> Files and Access Methods <--+
            |          |                     |
            |          V                     |
Concurrency-+--> Buffer Manager           <--+- Recovery
Controll    |          |                     |  Manager
            |          V                     |
            +--> Disk Space Manager       <--+
--------------------------------------------------

Notation:

- +[T]+: The number of pages needed to store all records of table T.
- +p~T~+: The number of records of table T fitting into a single page.
- +|T|+: Cardinality: the number of records in table T.

_Query optimzer_ translates SQL to _Query Plans_ , an internal language. The
_query executor_ is an interpreter for query plans. Think of query plans and
(dataflow) directed graphs, where nodes are relational operators and directed
edges represent data tuples (columns as specified).

Relational operators may be implemented using the iterator design pattern.

When measuring costs, often asymptotic notations in terms of number of I/O accesses are used, since I/O is much more expensive than CPU, even with flash. Sometimes, as improvement, a distinction is made between random access and sequential access, since also their costs differ substantially.


=== Integrity Constraints

Part of the DDL (data definition language).

A _superkey_ for a relation is a set of columns such that no two distinct tuples can have same values in all these columns. In other words, a superkey is a set of attributes within a table whose values can be used to uniquely identify a tuple.  A _(candidate) key_ (or _unique key_) for a relation is a minimal superkey, i.e. no column can be removed from the superkey such that the new column set is still a superkey.  The attributes / columns constituting the candidate key are called _prime attributes_.   Attributes that doe not occur in _any_ candidate key are called _non-prime attributes_.  A table can have multiple candiate keys, one of which can be choosen to be the _primary key_, all others are then _alternate keys_.  A _foreign key_ is a set of columns in one relation that uniquely identifies a tuple of another, possibly the same, table.  The relation containing the foreign key is called the _child relation_, the relation containing the respective candidate key is called the _parent table_ (or _referenced table_).

primary key vs unique key: It seems that technically the only difference is that a table can have at most one primary key, but zero or more unique keys. Further differences are among typicall defaults associated with these constraints, and the semantic meaning. Primary key is meant to identify a row, unique key is meant to ensure a constraint. Most DBMS will by default create a clustered index for primary key and an unclustered index for each unique key, and by default primary key has a non-null constrained while unique key doesn't. At least in Oracle, when all columns of a key are null, and there is no not-null constraint, then the key constraint is satisfied.

_Domain constraint_: Kind of a type, but with additional conditions attached. (Chapter 5.7.2).

_Primary key constraint_: Key must be unique within table

_Foreign key constraint_ (aka _referential integrity constraint_): A key that establishes a relationship between its table or view and a primary key or unique key, called the _referenced key_, of onther table or view. The table or view containing the foreign key is called the _child_ object, the table or view containing the referenced key is called the _parent_ object. Child and parent can be the same table or view.

_General contstraint_: View CHECK constraint on a table or an ASSERTION which is global / not associated with any table.

Note that being able to write down constraints in the DDL helps to remove redundancy. If we coudn't do that, these constraints would appear at multiple places / multiple programs working with the DB.


=== Normal forms

_Anomalies_ are problems, e.g. problems arising from having redundancy, which in turn arises when to many fields are cramed into a single relation such that it contains many tuples which are nearly identical. The typical way of solving the problem is to _decompose_ such an ill-designed relation into multiple relations.

A _normal form_ is a property of a relation with the intention of avoiding anomalies. A relation is in _1st normal form_ iff the domain of each attribute is an atomic type.  A relation is in _2nd normal form_ iff additionally all functional dependencies are on the whole candidate key, for all candidate keys. A relation is in _3rd normal form_ iff additionally every non-prime attribute is non-transitively dependent on every key of R. Bill Kent: "[Every] non-key [attribute] must provide a fact about the key, the whole key, and nothing but the key.". Requiring existence of "the key" ensures that the table is in 1NF; requiring that non-key attributes be dependent on "the whole key" ensures 2NF; further requiring that non-key attributes be dependent on "nothing but the key" ensures 3NF.


=== Relational algebra

_Relational algebra_ (aka just _algebgra_): Operational (thus procedural), i.e. we can build arbitrary expressions on the basis of operators, each taking one or more operands. The domain and image of each operator are relations. Relations have set semantics (in contrast to multiset), i.e. no relation can have duplicate rows (SQL has multiset semantics, i.e. tables can have duplicate rows. I.e. in pure relational algebra often there's a `remove duplicates' sub step. However in practice that is rather expensive since it involves sorting or hashing). Relation algebra is typically not directly used, but via SQL, which uses it internally.

Useful for representing execution plan semantics. Close to query plans.

_Relational calculus_ (aka just _calculus_): A declarative language -- Describe what you want, rather than how to calculate it. A variant is the _tuple relational calculus_ (aka _TRC_), which heavily influenced SQL.

Exprecity of relational algebra and relational calculus is equivalent.


==== Basic operators

There are only five operators: selection, projection, and 3 set operators: set difference, set union, crossproduct. There are convenience operators being based on these basic operators.

_Selection_ (or _Restriction_) (filter query): σ~_condition_~(_relation_) (s as in sigma/select): Keep matching tuples, cut away the rest.  The (selection) condition is a boolean expression, where primaries are literals and fields of the given relation. The output are the tuples of the input instance which satisfy the condition. The output has the same schema as the input.

_Projection_ (filter query): π~_fieldlist_~(_relation_) (p as in pi/project): Keep given columns, cut away the rest.  Returns new relation, having only the given fields of the input relation. Has to remove duplicates.

_(set) union_ (set query): A ∪ B (row-wise): Row-wise concatenate relations.  A and B must be _union compatible_ (sequence of field domains must be equal). Has to remove duplicates.

_(set) difference_ (set query): A - B (row-wise). Cut away rows which appear in B. A and B must be union compatible. Note that unlike the other basic operators, it cannot be implemented with an online algorithm, because each next tuple from B can remove a tuple from the tentative output.

_(set) intersection_ (set query): A ∩ B. Keep only rows appearing in both.  Defined as A-(A-B). A and B must be union compatible.

_crossproduct_ (aka _cartesian product_) (binary query): A ⨯ B. The output relation instance has each tuple of A, each of which followed by each tuple of B.  The output relation's schema is the concatenation of A's schema plus B's schema. By convention field names are overtaken; in case of name conflicts, corresponding fields are unnamed and must be referred to by position.


==== Some important compound operators

_(conditional) join_ (binary query): A ⨝~condition~ B: Defined as σ~_condition_~(A ⨯ B).

_equi join_ (binary query): A conditional join where the condition solely consists of one or more equalities, combinded by logical and. They can be implemented efficiently; In effect, there is only one equiality, where the rhs and lhs are the concatenation of the individual original lhs/rhs. E.g. (r1.f1=r2.f1 and r1.f2=r2.f2) is equivalent to (concat(r1.f1,r1.f2)=concat(r2.f1,r2.f2)).

_natural join_ (binary query): A ⨝ B: Condition demands equivality (A.fieldx=B.fieldx) for all fields having the same name. I.e. it's an implicit equi join. However, in contrast, also a projection follows which cuts away the duplicate fields. If there are no common field names, the result is the crossproduct.

_Inner joins_ don’t include non-matching rows; whereas, outer joins do include them. _Left outer join_ always has at least one tuple for each tuple of the lhs input relation, and if there are no tuples of the rhs relation matching the condition, fills the components with NULLs. _Right outer join_ is analogous. _Full outer join_

_division_: A / B: Defined as π~x~(A)-π~x~((π~x~(A)⨯B)-A). More informally: Say A tells which supplier supplies which part, and B lists parts. A/B deliviers suppliers which supply all the parts in B.


==== Extended operators

_duplicate-elimination_ δ (d as in duplicate/delta): Eliminates duplicate rows, i.e. turns a multiset into a proper set.

_aggregation_: Apply some operation (e.g. sum, average) to all components of a column.

_grouping_ γ (g as in grouping/gamma): Put tuples matching a condition in the same group, and then perform some aggregation to columns within each group.

_extended projection_: In addition to projecting out some columns, we now can produce new columns.

_sorting_ τ: Turn a relation instance into a list of tuples. Note that not all relational operators accept lists as arguments.

_outer join_: *to-do*


=== SQL

See sql.txt


== Local implementation of relational model


=== Implementation of relational operators

==== select

FP: number of pages in file. As always, time analysis is in terms of page I/Os, not considering writing the result.

OMP: in case of ordered input, number of pages containing the matching tuples

MT: number of matching tuples

no index on column, unsorted data:: Scan all tuples. O(FP)

no index on column, sorted data:: Binary search to find first matching tuple, then sequential scan as long as tuples match. O(log FP + OMP)

B+ tree index on column:: Walk B+ tree to find first matching tuple, then scan as long as tuples match. O(log~fanout~

==== join

_Theta join_: Given sets R and S, the theta join R ⨝~Θ~ S delivers all pairs {r,s} where the predicate Θ(r,s) is true, r and s being members of the set R and S respectively. In an _equi-join_ Θ is an equality test; it can be optimed. As a special case of that, even more optimizeable, is when one operand is a key.


===== simple nested loop join algorithm

--------------------------------------------------
foreach record r in R:
  foreach record s in S:
    if theta(r,s): result.add({r,s})
--------------------------------------------------

page I/O cost, assuming arbitrary large [T] and [R], ignoring writing result: |R|*[S]+[R], i.e. _very_ bad.

===== chunk (oriented) nested loop join algorithm


Improvement: Make number of iterations in outer loop as small as possible, so we have to go pages of S as few times as possible. So outer loop reads from R in `chunks', one chunk being B-2 pages large. It's -2 because we need one page for the input streaming buffer for S, and one page for the output streaming buffer of the result.

--------------------------------------------------
foreach chunk in R:
  read in chunk from R
  for each record r in current Rchunk:
    foreach record s in S:
      if theta(r,s): result.add({r,s})
--------------------------------------------------

page I/O cost: [R]/(B-2)*[S]\+[R], becomming [S]+[R] if outer table, i.e. the Rchunk, fits completely into memory, i.e. if [R]<=B-2.


===== indexed nested loop join

For the special case of equi-joins.

--------------------------------------------------
foreach record r in R:
  foreach record s in R where r==s:
    result.add({r, s})
--------------------------------------------------

page I/O cost: [R]+|R|*costOfFindingAKey


===== sort-merge join

For the special case of equi-joins, here R.r_attrib=S.s_attrib

------------------------------------------------------------
sort R on r_attrib -> sortedR
sort S on s_attrib -> sortedS
scan sortedR and sortedS in tandem to find matches
------------------------------------------------------------

page I/O cost: cost(sort R) + cost(sort S) + [R]+[S].

As an optimization, the sorts, each having internally a set of sorted chunks, ommit writing an output. Instead, the `scan sortedR and sortedS in tandem' step operatoes on all these chunks; each chunk is connected to an input buffer. Thus instead of the normal B-1 chunks a sort creates, now it can only create (B-1)/2 chunks. So we saved 2*([R]+[S]), since we saved writing/reading the sortedR and sortedS.

Naturally a good variant if R and S need to be sorted on r_attrib and s_attrib respectively anyway in the query plan.


===== hash join

For the special case of equi-joins, here R.r_attrib=S.s_attrib

----------------------------------------------------------------------
using coarse hash function, partitionate R,
  restriction: no partition might be larger than B-2 pages,
                  so it might be as usual a recursive process
using coarse hash function, partitionate S, partitions can be of any size
for each partition pr of R
  read in partition pr, building an inmemory hashtable (using upto B-2 pages of memory)
  for each record s in partition of S being associated to pr: (nomal streaming using one input buffer)
    if hash table contains key s.s_attrib:
      result.add({r, s}) (normal streaming using one output buffer)
----------------------------------------------------------------------

Often R is called the building table, and S the probing table.

Note that the probing table's partitions can have an arbitrary size (in pages), since they are streamed. Thus you want to make the smaller table the building table, and the larger table the probing table.


=== Files and Access Methods

A _(DB) file_ is a collection of pages. A _page_ is a collection of records. Each _record_ has an _(physical) record id_ (rid), which is a pair (page_id, slot_id). Records can be fixed width or variable width. The file API supports insert/delete/modify/find(via recordid) a record, scan all records.

_System catalogs_ store properties of each table, index, view and other stuff such as statistics, authorization etc.

A DB file is typically implemented as one or more OS files, or as raw disk space, e.g. in POSIX directly a device. Note that a DB file might spawn multiple disks.

[[index]]
==== Index

An _index_ (aka _access path_) is a disk based data structure that organizes data records of a given table, or references to them, on disk to optimize certain kinds of retrieval operations. A table can have multiple indexes on it. A _search key_ is over any subset of columns of that table. In contrast to the key of the table, multiple records can match a search key. An index is implemented as a collection of _data entries_. A data entry with search key value k, denoted as k*, contains enough information to locate the matching records. There are three main alternatives of how to store a data entry: Alternative 1) (k,record). I.e. the index directly stores the records of a table. To avoid redundancy, this alternative is used at most once per table. Alternative 2) (k, rid). Alternative. 3) (k, rid-list). Alternative 2 and 3 obviously introduce a level of indirection. A _clustered index_ is one where the ordering of data records defined by its data entries is roughly the same as the ordering of the data records of the file of the underlying table. By definition alternative (1) is clustered. For alternatives (2) and (3), the file must be roughly (see <<clustered_file>>) or strictly sorted (see <<sorted_file>>). Regarding range search queries, clustered indexes are in general much faster than unclustered, due to the usual contigous access advantages and since more of read in page is actually used, i.e. less pages have to be read. The costs for a clustered index is maintainenance cost to (roughly) maintain the ordering of the data records. Often that means that the pages containing data records are not fully packed (2/3 is a common figure) to accomodate future inserts, which degrates performance since more pages nead to be read/written for a given amount of records.

Common kinds of selections (aka lookups) that indexes support:

- key operator constant, and specifically equality selections, where the operator is =.
- Range selections, where op is a relational operator <, >, ....
- N-dimensional ranges: e.g. points within a given rectangle.
- N-dimensional radii: e.g. points within a given sphere.
- Regular expressions

[[bplus_tree]]
==== B+ tree

_B+ tree_ is an high-balanced n-ary tree. It's the most widely used data structure to implement an index. They have fast lookups and fast range querries. Is typically the most optimized part of an DBMS.

Each node is stored in a page. Unlike with a B tree, internal nodes only
contain pointers to further nodes, never data; only leaf nodes contain data or
pointers to data. Also leaf nodes form a linked list. Together this allows for
more efficient scans over a range of data.

Regarding high-balancedness: Each node contains m entries with the soft restriction d<=m<=2d, i.e. it's always at least 50% full, where d is called the _order_ of the tree. The high balanced property guarantees O(log N) access time, i.e. guarantees that even after insertions/deletions performance can't degenerate to linear time. Then again, since keys can be of variable width (e.g. strings), and the data entries in the leaf nodes can be variable width (e.g. see alternative 3 in <<index>>), in practice this is seen sloppy. sometimes a physical criterion is used (`at least half full' in terms of bytes).

Key compression increases fanout, which reduces height, which reduces access time.

Algorithm to _insert_ into an already full node: split node, which obviously includes allocating a new node, and which makes space for new item. Introducing a new node obviously also means that we need to insert a new item into the parent node which points to the new node. Now this can be a recursive process, where in the worst case it ripples up all the way up and we have to split the root. If data entries are directly data records (see alternative 1 in <<index>>, advantages see there), splits can change record ids, which means having to update referees, which is considerable disadvantage.

Similarly for _deletion_. We should maintain the d<=m<=2d invariant. However in practice m<d is allowed, since in practice it's a rare case that given a big table there are so many deletions which would shrink it to a small table. Note that all leafs have the same depth, and there are no rotations upon insertion/deletion has with other kinds of balanced trees.

Creation of a B+ tree given a collection of keys should no be done via individual inserts, since the resulting page access pattern is very random and thus slow. Instead, we do _bulk loading_: Sort the index's data entries. Then iteratively soak them up and create leaf nodes. A fill-factor parameter determines how full the leaves shall be. Create/update parent nodes as in the insertion algorithm. Looking at the usual tree drawing, we see that always the right-most internal nodes are touched whereas the other nodes aren't at all, an access pattern which works very well together with an LRU page buffer.


=== Buffer management

A cache storing in memory a collection of pages from the disk space management below. Consists of a collection of frames, a frame having the same size as a page. Allocated at startup time.

Each frame has associated: pageid/NIL, pin_count (aka reference_count), dirty_flag.

A request for a page increments pin count. A requestor must eventually unpin it and indicate whether page was modified (-> dirty flag).

pin_count==0 means unpinned means `free to be exchanged by another page from disk'. When pin_count goes to 0, that is the event of `page is now no longer used'.

There different replacement policies for replacing a frame: least-recently-used (LRU), most-recently-used (MRU), clock, ....

As an optimization, pre-fetch is often employed.

Buffer leak: when a page request can't comply because all pages in buffer are pinned. That is considered a bug in the DB; pages should only be pinned for a very brief time.


=== Disk space management

Disk space manager provides about this API: allocate/free a page, read/write a page. Higher levels expect that sequencial access to pages has an especially good performance.


== Data Store


=== MongoDB

A document store. Each document stores a tree via JSON.

Features:

- Indexing. generic secondary indicies, unique index, compound index, geospatial index, full-text indexing.

- Aggregation pipeline (framework for data aggregation)

- Special collection types: time-to-live collections, fixed-size collections

- Can be used as file storage *to-do* inclusive metadata?

Notable absent features: joins and complex multirow transactions. An architectural decision to allow for greater scalability.

A _document_ is an ordered map (i.e. ordered set of keys with associated values). However you should not rely on ordering; apparantly even some tools ignore / mess-up the order. Values have types. A value can be again an document (called _embedded document_), resulting in a nested structure, i.e. a tree. Every document must have an ___id key__, the value of which must be unique within a collection. The type is unrestricted, but defaults to ObjectId. _id is always indexed.

A _collection_ is a set of documents. Indicies are defined per collection. Collections are hierarchical, giving raise to _subcollections_. However, a subcollection does not have any special properties. A fully qualified collection name consists of dot (.) separated parts.

A _database_ is a set of collections. As a rule of thumb, store all data for a single application the the same database.

A _MongoDB instance_ is a set of databases.

_Names_: The keys are cassensitive strings. Any Unicode character is allowed except for the null character. Also, the characters "." and "$" have special meanings. The prefix "system." is reserved.

Writing is atomic with granularity of one document. _Write concern_ is a client setting which specifies whether writes should be _acknowledged_ or _unacknowledged_. Unacknowledged writes return immediately and don't return any error.

_document order_ / _padding_: The storage format is BSON. The data is stored on the local drive, see also <<mongodb_shard_data>>. The documents of a collection are stored one after another, each with some padding (given by the _padding factor_), which allows a document to grow. However, if the padding doesn't suffice, the document is relocated, usually to the end of the collection. The padding factor is initially 1 (no padding), and is dynamically adjusted depending on how many documents outgrow their padding and how many updates are within the existing padding.

_cursor snapshots_ / _snapshot query_: The problem: "cursor = db.mycollection.find(myquery); while (cursor.hasNext()) { var doc = cursor.next(); doc = process(doc); db.mycollection.save(doc); }". When a document outgrows its padding, it is relocated, usually to the end of the collection.  In that case, the loop visits the document a 2nd time.  The solution is to snapshot the query: "db.mycollection.find(myquery).snapshot();" Snapshotting makes the query slower.

[[mongodb_shard_data]]
Shard data (group documents). Each shard is replicated. Each shard is stored via an master (called primary) - slave (called secondary) architecture. Writes are made a bit faster by trading durability for availability. Say there are 5 secondaries. A client writes to the primary, the primary writes to all 5 secondaries. Once the primary gets acknowlodgments from say two secondaries, it acknowledges to the client. The primary/secondary use their local drive, i.e. they are not on top of say MapReduce / Spark / HDFS etc.

Replication brings:

- Fault tolerance / prevent data loss

- More availability since if one source is busy, we use another source.

- Lower latency if a data center geographically closer to the client

- Faster due to parallel processing *to-do* does this apply to MongoDB?


==== Tutorial

Run +mongod+ to start the MongoDB network server. Run +mongo+ to start the MongoDB JavaScript shell. That shell is also a standalone MongoDB client. By default it connects to the `test' database and assigns this database connection to the global variable +db+.

+use <databasename>+ switches to the specified database, i.e. db will afterwards refer to the specified database.

------------------------------------------------------------
mydoc = { "mykey1" : "myvalue1", "mykey2" : 42 }
db.mycollection.insert(mydoc)
db.mycollection.find({"mykey2": 42})
------------------------------------------------------------


==== Operations

The query language is very primitive / simple which looks like JSON. Is intended to be used via high level programming langauge.

Provides CRUD operations (on documents): create (+insert(newdoc)+), read, update (+update(matchexpr, newdoc)+), delete (+remove(matchexpr)+, matching documents will be removed)

===== update

db.collection.update(query:document, update:document, options:document) : WriteResult

Modifies the document(s) matching the query according to the specified update. By default, only a single document is updated; set the `multi' parameter to true to update all documents that match the query.

update(query, newdoc): replaces the existing document with the new document.

update(query, {"<modifier>" : {"keypath" : newvalue}})

- $set: sets the value of given key to the specified value, creating it if doesn't yet exist
- $unset: removes the key (use 1 as newvalue)
- $inc: increments the value by newvalue
- $push: appends newvalue to the array of given key. To append multiple values, use suboperator $each, i.e. newvalue = {"$each" : [newvalue_1, newvalue_2, ...]}
- ...


===== find

db.collection.find(query:document, projection:document) : cursor

Returns a set (via a cursor) of documents matching the query.

find({"foo.bar": myvalue}): Selects all documents where myvalue matches value of the element specified by the path. Interprets foo.bar as path, i.e. bar is the child of foo. The path may appear anywhere in the tree.  If the foo.bar element has an array as value, it is a match if any array element matches myvalue. When the query is the empty document ({}), all documents are returned.

_query conditionals_: find({"foo.bar": {"<op>" : myvalue}}): op can be {$eq, $ne, $gt, $lt, $lte, $gte, $in, $nin, $or, $not}. $in ($nin): the value of the element specified by path is (not) in the array specified by myvalue. $or: myvalue is an array of query:document. $not: myvalue is an query:document.

_projection_: find(query, {"foo": 1, "bar": 1}). Selection (query) and projection at the same time. Returns only the specified keys, here foo and bar. Note that the _id key is still returned by default. Use value 0 instead 1 to return all keys but the state one.

_cursor_ / _cursor modifiers_: The order in which cursor modifiers are specified has no effect *to-do* so in which order are they then executed/applied?

- pattern: cursor = db.mycollection.find(...); while (cursor.hasNext()) { doc = cursor.next(); ... }.

- _sort_: cursor.sort(sort:document). Specifies the order in which the query returns matching documents. You must apply sort to the cursor before retreiving any documents from the database.

- _limit_: cursor.limit(value). Limits the the number of returned documents to the specified value. You must apply limit() to the cursor before retrieving any documents from the database.

_query modifiers_ (or _wrapped queries_): db.mycollection.find(myquery).sort(mysort) is wrapped in a larger document, resulting in {"$query" : myquery, "$orderby" : mysort}. The more general form is { "$query" : myquery, <option>:optionvalue }, where <option> can be $max, $min, $orderby etc.

How to identify nodes in the tree, i.e. different schemes for assigning ID s to tree nodes:

- integers: enumerate nodes in a preorder traversal

- floating point: as integers, but allows for insertion

- dewey IDs: hierarchical as enumrating book chapters

- ORDPATH IDs: *to-do* as dewey ids, but initially only with odd numbers. Unlike dewey, allows for insertion by using even numbers in some way.


==== Indexes

As in relational data bases to make selection faster. MongoDB's indexes work almost identically to typical relational database indexes. B-trees or Hash tables. At the large scales MongoDB is intended for, indexes are essential. A query that does not use an index is called a _table scan_.

The _id key is always indexed.

A _compound index_ is an index on more than one field / key.

_Cardinality_ refers to how many distinct values a field can have. "gender" traditionally only two, "email" is likely to be unique for each document in the collection, "age" is somewhere inbetween.

_Creating an index_: db.mycollection.ensureIndex({"mykey" : 1}). For compound indexes, it's analogous {"mykey1" : 1, "mykey2" : 1, ...}. The value 1 is for ascending, the value -1 for descending, "hashed" for an hash index, "text" for a text index. _Indexing embedded documents_: Keys in embedded documents can also be indexed the natural way: ensureIndex({"mykey1.mykey2": 1}).

Rational for specifying order of an index: Makes sense when sorting on multiple criteria. Say you want to sort first by key1 ascending, then key2 descending, but the index is ascending for both key1 and key2. Then for each distinct key1, the associated key2s are in the wrong order in the index and we have to jump around in the index.

_unique index_: ensureIndex({...}, {"unique": true}): Establishses a contract that each value will only appear once in the index. Inserting/updating a document which violates that contract will cause an error. Creating an unique index on a collection which violates that will cause an error (in which case you might want to use dropDups to drop duplicates).  If a document doesn't contain the respective field, it appears as null in the index. Followingly there can be at most one document in the collection omitting the respective field (see sparse index as a possible remedy).  Note that the implicit index on \_id is a unique index. _Compound unique index_: Only the concatenation of the index's fields must be unique.

_sparse index_: ensureIndex({...}, {"sparse": true}): Sparse indexes only contain entries for documents that have the indexed field, even if the index field contains a null value. The index skips over any document that is missing the indexed field. Note that MongoDB's sparse indexes are a completely different concept from RDBMS' sparse indexes.

index {"key1" : 1, "key2" : 1}, query find({"key1" : ...}).sort({"key2"}): Called _point query_. The index can be used very efficiently.

index {"key1" : 1, "key2" : 1}, query find({"key1" : {"$gt": ..., "$lt": ...})): Called _multi value query_. The index can be used very efficiently; however here the 2nd field (key2) of the index is not used.

index {"key1" : 1, "key2" : 1}, query find({"key1" : {"$gt": ..., "$lt": ...})).sort({"key2" : 1}): The index only helps for the find part. The documents found by find have to be sorted in memory explicitely; the index on key2 only helps directly if all the document currently in question have the same key1.

As before, but now with a index which switches the keys, i.e. {"key2" : 1, "key1" : 1}: Now MongoDB can iterate over each key2 in the index, and for each key2 efficiently find the key1 matching the find query.

_Only one index is used_: (Currently) MongoDB can only use one index per query (that is actually per $or clause). Thus if the query asks for two keys (find({"key1":..., "key2":...})), only one index can be used.

_hint_: We can use hint to force MongoDB to use a certain index, as for example in db.mycollection.find(...).sort(...).hint({"key1":1, "key2":1}). See also patterns below. If you want a _table scan_ (i.e. don't use any index), say hint({"$natural":1}).

_explain_: db.mycollection.find(myquery).explain(): Describes what the query is doing.

_covered indexes_: When an index contains all the values requested by a query, it is said to be a _covering query_. In that case the document an index entry refers to must not even be looked at.

_implicit indexes_: A compound index can be used for any query which uses prefix of the index's keys. I.e. if the index is (key1, key2, key3, key4), then it can be used for queries on (key1) or (key1, key2) or (key1, key2, key3) and so on.


patterns / tips:

- index {"sortkey":1, "query_criteria":1} often works well, because most apps don't want all possible results for a query but only the first few. *to-do* how does that relate to _right-balanced_ index?

- Create index on fields with high cardinality, to narrow down the set of matching documents quickly.

- Creating on existing documents is slightly faster than creating the index first and then inserting all the documents.


=== Neo4j

A graph database, the labeled property graph variant. Uses Cypher as query language.


==== Neo4j Internals

Does replication, but not sharding. Uses a master-slave architecture for replication. That also increases availability.

Transactions (by definition ACID) are semantically identical to traditional RDBMS transactions.

Data structure: Labels of a node are stored as a singly linked list attached to a node. Same goes for properties. The edges of a node are stored as a doubly linked list (called _relationship chain_), where the pointers are stored in the edge data structure. Each edge has each a pointer to the source node and the target node. Properties for a node / responsibility are stored in a singly linked list.

The graph is stored in multiple store files. Each _store file_ contains a certain aspect: nodes, relationships, properties etc. Ids use 4 bytes. Note that the fixed size records allow for O(1) lookup of a specific record.

The _node store_ stores all the nodes. Each node is stored in a fixed sized record of 9 bytes. 1 byte as `isInUse' flag, id of the node's first relation in the relationship chain, id of the node's first property.

The _relationship store_ stores all the relationships. Each relationship is stored in a fixed sized record of 33 bytes.  1 byte as `isInUse' flag, ids of to source and destination node, id of relationship type, ids of the next / prev relationship for the source / destination node, id of edge's first property.

The _property store_ stores all the properties (both of relations and nodes). Fixed size records. Four property blocks, the id of the next property in the chain. One to four property blocks are required to store a single property. *to-do* better understand the details how a single property is stored

The _cache architecture_ consists of the operating system's file cache and a high-level cache.  The file system cache is intended for modifications.  The _high-level cache_ (or _object cache_) stores nodes, relationships and properties and is optimized for arbitrary read patterns.  Here the data structure is different from the one on disk. A node also stores its properties and pointers to its relations, sorted first by relation type, then by relationship direction (in/out). A relationship stores its properties; but no longer pointers to next/prev relationship in the relationship chain.

_Indexes_: Indexes are used to quickly find the starting node of a query, see also START clause in cypher.
p
_Latency_: Most queries flollow a pattern wherby an index is used to quickly find starting nodes; the remainder of the graph traversal then uses a combination of pointer chasing and pattern matching. That is, performance does not depend on the graph size, but only on the size of the graph bequing queried. Contrast this with RDBMS where joins depend on the size of the two involved tables.

_API hierarchy_: From low-level to high-level: Kernel, Core API, Traverser API, Cypher.

*to-do* I don't understand how to scale out. So with neo4j, we have the same limitations in size as a traditional RDMS, right? It must fit on a single machine.

*to-do* Despite the caches (which as all caches are tiny compared to the data on disk), I don't see how can all this pointer chasing be efficient. Isn't the query time totally dominated by disc seeks?


== Processing

=== MapReduce

_MapReduce_ is a programming model for parallel data processing.  Works on top of a `key-value' model; quotes because keys need not to be unique.  Aims to scale linearly in the number of nodes added to the cluster.

A MapReduce _job_ is a unit of work that the client wants to be performed: it consists of input data, the MapReduce program, and configuration information.  The job is divided into _tasks_, of which there are two types: _map tasks_ and _reduce tasks_.  In Hadoop, the tasks are scheduled using YARN.

The input is divided into pieces called _input splits_ (or just _splits_), each split containing a set of key-value pairs, each split being approximately the same size in bytes.

One _map_ tasks is created for each split.  Typically one mapper node will have multiple splits / map tasks under its responsibility.

_combiner_: As an optional optimization, to reduce the amount of data that shuffle needs to process and that needs to be send across the network, a mapper node also does a _combine_ step.  Very often the combine function is the same as the reduce function.  Required conditions: associativity, commutative, same input and output types.  In Hadoop, the combiner is regarded strictly as an optimization, and there are no guarantees on how many times it is called for a given map output, if at all.

_Shuffle_: Each mapper node _sorts_ its output by key, _partitions_ by key, and sends each partition to the reducer node responsible for the respective key.  So each reducer node receives multiple partitions for a given key and merges them.  The details of shuffling are explained further below.

The _reducer_, for a given key, receives _all_ key-value pairs having that key.  A reducer can be responsible for multiple key, but a key can only be assigned to exactly one reducer.  As a consequence, a reducer might start bevore mapping and shuffling is finished, but a reducer can't start producing output bevore all mappers and the shuffling is finished.  Note that certain jobs don't need a reducer at all, in which case we also can omit the shuffling.

_sort_ and _copy_ part of the shuffling after mapping: The mapper writes its output to a circular buffer, 100 MB by default.  When the percentage of of used space is about a certain threshold, by default 80%, a background thread starts to sort and spill the data to disk, as described in more detail in the following.  The data is sorted by output/intermediate key in memory.  If there's a combiner, it is run on parts with the same key (now being in sequence due to the previous sorting).  Recall that typically the combiner reduces the amount of data.  Then the data is written to the local file system.  Each spill creates a new spill file.  When the maper is finished producing output, the spill files are merged into one file, keeping the sorting.  If there are at least three spill files, the combiner is run again.  A sequence in the output file with the same key is called a partition. That processing of the mappers output is also called copy phase. Recall that this is similar to what HBase does when flushing the memstore to a store file.  I.e. at the end there are zero or more spill files plus what's left in memory.  As in HBase, an LSM-tree can be used to merge them into one file.  As an optimization, one can try to do all that in a more stream like fashion, e.g. merge spill files and send them to reducers while the maper is still producing output.

_merge_ part of shuffling:  It's the reducer that asks via HTTP each mapper `send me the partition for the following set of keys'. The reduce tasks uses multiple copier threads to fetch partitions in parallel from mappers. As data is accumulated, at one place (memory or disk) per copier thread, a background thread merges it, to disk or memory.  Merging is hierarchically, i.e. tree like, as opposed to all inputs directly into a single file.  The merging is also called _sort phase_, despite it is _not_ about sorting.

*to-do* why this hierarchical merging? Assistent asks for reference, I send an email.

A common split size is one HDFS block.  If the splits are too small, then there is too much overhead of managing the splits.  On the other hand small splits are nice because the parallel processsing is better load balanced; a faster machine can process proportionally more splits than a slower machine.  Also, if the split size was larger than one HDFS bock, it could not be guaranteed that both HDFS blocks are on the same machine, which would be bad for data locality optimization.  However note that the last key-value pair of a split might spawn two HDFS blocks.  This is a drawback we have to live with.  Recall that HDFS allows to read parts of a HDFS block, so the problem is mitigated somewhat.

In general one should try to give as much memory to the copy phase and sort phase as possible, relative to the actual map and reduce.  E.g. the map and reduce functions should not use unbounded collections.

Common formats:

- text file: Each line has a special seperator character separating key and value.

- text file: Each line is a value. The keys are implicitely generated, i.e. not stored in the file, and are the positions where the respective line starts.  Often used when the mapper is not really interested in a key.

- _sequence file_: Unsorted sequence of generic binary key-value pairs.  More formally, the actual tuple is (keylength, key, valuelength, value).

- _map file_: As sequence file, but sorted and additionally has an index for faster lookup.

--------------------------------------------------
                         input
split
                         input kv type
Map
                         [intermediate kv type]
[Combine]
                         intermediate kv type
Shuffle (sort & partition)
                         intermediate kv type
Reduce
                         output kv type
--------------------------------------------------

_data locality optimization_: As an optimization, let the map run on the data nodes.  This paradigm is also called _bring the query to the data_.  Thus no network transfer needed for the map step.  If the data node hosting the HDFS block is already completely busy with other tasks, the job scheduler will look for a free map slot on a node in the same rack hosting a replica.  Also recall that the last key-value pair of a split might spawn an HDFS block, thus that other HDFS block might also not be local to the mapper node.

Even if the data to precess were `only' hundreds of gigabites, i.e. would fit on a single machine, it can still make sense to let run MapReduce on a cluster.  The bottleneck with one single machine is often the throughput of the disk.  The CPU and/or RAM  might also be a bottleneck, but can be dealt with also by other means than using a cluster, e.g. by more efficient code.

If the overall problem gets more complicated, in general you should try to divide it into multiple simple jobs, instead of making the map and reduce of a single job more complex.  If the dependencies between the jobs are non-linear, i.e. a DAG, there are libaries helping to run the DAG of jobs.

Some figures:  A typical job in a 1k node cluster (a large cluster) would run in a couple of hours.  The processed data is in the TBs.


=== Hadoop 1.0 resource management

Master-slave architecture. The master is called _JobTracker_, the slaves are called _TaskTrackers_.  The JobTracker does scheduling (i.e. distributes the tasks), i.e. manages the ressources.  It also does task monitoring.  If some task or TaskTracker has a problem, the JobTracker has to care about it, e.g. by rescheduling the task.

Issues with version 1: The JobTracker has to many responsibilities. As a consequence, scalability is limited, <4000 nodes and <40'000 tasks.  Also the task slots are allocated statically before the job starts -- as a consequence, it may turn out that the mappers of a job are working at maximum capacity, and the reducers are idle.

**to-do**  I can't properly put this chapter and the next in the stack.  Neither can I properly name the field it is about (resource management?) and distinguish it from MapReduce.  Apparently YARN is for more than just MapReduce.  Can we say the same for the equivalent thing of the version 1.0?


=== Hadoop 2.0 resource management / YARN

YARN (yet another resource negotiator).  The master is called YARN ResourceManager, the slaves are called YARN NodeManager.  The main improvement is separation of scheduling and monitoring, which in version 1.0 were both done by the master.  In version 2.0 monitoring is pushed down to so called ApplicationMasters running on the slaves.  Scalability is improved (v2 relative to v1), 10'000 nodes and 100'000 tasks, which is about the size of a data center.  Since scalability is improved, also availability is improved.  Fully backwards compatible.

*to-do* list multi-tenancy as an improvement

The _ResourceManager_ must take care of cluster utilization, give capacity guarantees (e.g. hold the promise that a container has 16GB RAM), guarantee fairness (if 10 jobs are using the cluster, each shall get its fair share, see also schedulers), and must fulfill SLAs.  The ResourceManager provides a client service API to the clients so they can start/end jobs, get informations about jobs.  The ResourceManager's responsibilities include to know about the resources available in the cluster.   I.e. a list of the live NodeManagers and what their resources are.   New / rebooted NodeManagers have to register at the ResourceManager.  The NodeManagers repeatedly send liveliness (aka heartbeats) to the ResourceManager.  Note that the ApplicationManager has a similar but different responsibility with respect to ApplicationMasters.  The ResourceManager's responsiblities include the role of the ApplicationManager.  The RessourceMassager can contact the NodeManager to ask it to kill containiers in order to free resources.  Summary:

- Top level master. First contact person for a client.

- Manage resources of cluster.  List of live NodeManagers and their resources.

- Give capacity guarantees, fulfill SLAs etc. via role of scheduler (_excluding_ monitoring responsibilities)

- Role of ApplicationMaster

The _ApplicationManager_, which is a part of the ResourceManager, tracks the jobs / applications currently running on the cluster.  There is at most one ApplicationManager per node, else we would have again (as Hadoop v1) have a bottleneck. Similarly it also keeps track of the current ApplicationMasters.  The ApplicationManager also maintains a list of jobs / applications waiting to be scheduled in case the cluster is full.  When a ApplicationManager is started in a container of a NodeManager, it has to register at the ApplicationManager ("Hy, all went well, I'am started").  ApplicationMasters repeatedly send liveliness (aka heartbeats) to the ApplicationManager.  ApplicationMaster can allocate/deallocate containers during the application.  Think also when one container, i.e. its parent node, dies.  Summary:

- Manages the running and waiting applications of the cluster.

- Manages the ApplicationManagers, including tracking that they are alive

Each _NodeManager_ provides a set of containers. It must track its resources, including the amount of free resources.  Resources such as memory, CPU (number of cores), disk, network.  A node manager repeatedly reports to the RessourceManager its free resources.

A _container_ is a non-static assignment of resources.  Currently CPU and memory; a generalized model can also support things like network bandwidth or GPUs. Say a node has 8 cores and 64KB RAM, then each container might get assigned one core and 8GB RAM.  Each container can run a map task or reduce task or ApplicationMaster.

*to-do* Is the resource assignment to a cluster fixed? Does a cluster try to provide different types of containers (e.g. some high cpu low ram, some low cpu high ram, some averaged).

An _ApplicationMaster_ runs in an container. The ApplicationMaster's primary responsibility is one application / job.  Create it, allocate containers for it via the ResourceManager, run tasks in the allocated containers, monitor the application (i.e. the tasks that make up the application).  Monitoring includes relaunching died tasks, making HBase fault tolerant.

_Basic sequence of running a job / application_: A YARN application starts with a client resource request to the ResourceManager; a notification that the client wants to submit an application.  The ResourceManager responds with an ApplicationId and information about the capabilities of the cluster that will aid the client in requesting resources. The client sends an application submission context and an container launch context to the ResourceManager.  The _application submission context_ contains the ApplicationID, user, queue and other information needed to start a respective ApplicationMaster.  The _container launch context_ (_CLC_) contains resource requirements, job files, security tokens and other information needed to set up an container and within it an environment to launch the ApplicationMaster.  The ResourceManager picks an available container for the ApplicationMaster, then often called _container0_.  The ResourceManager contacts the respective NodeManager and lets it start the ApplicationMaster. The just started ApplicationManager sends a _registration request_ to the ResourceManager, which responds with information about minimum and maximum capabilities of the cluster.  Based on that information, the ApplicationMaster will request containers from the ResourceManager, which will respond as best possible based on scheduling policies and send information about the now assigned containers. The ApplicationManager contacts the respective NodeManagers, and starts containers by sending the NodeManager CLCs.  The ApplicationMaster continously sends heartbeats to the ResourceManager.  A heartbeat contains progress information.  It can be used to request more ressource or to release resources.  The ResourceManager can direct NodeManagers to kill containers.  When a job is finished, the ApplicationMaster informs the ResourceManager that the job completed successfully.  The ResourceManager then asks the NodeManager to aggregate logs, and asks all involved NodeManager to kill the respective containers, including the one for the ApplicationMaster.

A client sends a job to the ResourceManager.  The ResourceManager chooses one of the free containers to be the ApplicationMaster for that job.  The ApplicationMaster decides how many containers it needs to assign tasks to and then asks the ResourceManager for the locations of that many free containers.  The request can contain hints like how much RAM the container should have, on which rack or node it ideally should be (so e.g. a mapper can run on the node hosting the required HDFS block).  The ApplicationMaster then directly contacts containers received from the ResourceManager.  Note that now, in contrast to version 1, the master is only involved at the beginning of the job.  It is no longer involved after the job starts.  As a consequence we get better scaling.

**to-do** Chapter 7. I'm not sure I understand who exactly does the splits, and wether or not the splits move around in the network. I assume the client does the split logically, i.e. only by getting to know which HDFS slave hosts which HDFS block of the data. From then on, only that location information is transfered / used by involved nodes / task.  The HDFS data of the HDFS blocks is in general not transfered over the network (only if the associated mapper can't be on the same node)


==== Yarn Schedulers

_steady fair share_: Share (aka percentage) of total resources taking also empty qeues into account.  _Instantaneous fair share_: Share of total resources when not taking the empty queues into account and redistributing their share to the non-empty queues.  _Current allocations_: Percentage of resources each queue currently actually is using.  Steady fair share and instantenous fair share are goals, current allocations is the reality.  _Dominant resource fairness_ is one of multiple possible ways of computing current allocation as one single value if there are multiple resources. For each application, take the maximum resource percentage among all resources the application currently uses. Then value for a given application is its max divided by the sum of all max.

_Schedulers_ (of jobs) the ResourceManager might use:

- _FIFO scheduler_

- _Capacity scheduler_: There's a set of hierarchical FIFO queues, taking user applications, each queue guarantees a certain capacity (resources, e.g. cores).  The clusters resources are distributed among the queues.  E.g. say we have two queues, one gets 70% of the cluster's resources, the other gets 30%.  In the most naive variant, if one queue is empty, it's assigned resources are wasted / idle.  In an improved variant, we use instantenous fair share instead of steady fair share as target.  As a consequence, non-empty queues get the resources of empty queues.  If a new job enters a previously empty queue, it has to wait until resources get free.  However, if that new job has top priority, is possible to preempt running jobs.  Each queue has strict ACLs that control which users can use the queue.

- _Fair scheduler_: Highly simplified: The aim is that all applications get, on average, an equal share of resources over time.  If there is an single app, it gets all the resources of the cluster.  If a new app comes in, when ressources get free, they are assigned to the apps such that over time, each app roughly gets the same amount of resources.  Apps can have priorities, which serve as weights to determine the fraction of total resources that each app gets.  The scheduler further organizes apps into queues, and shares resources fairly between these queues.  By default all users share a single queue, named `default'.


==== Authentication

ApplicationMaster's are not trusted, since they run user code.

When the ResourceManager creates an ApplicationMaster, it gives it an _ApplicationToken_.  The ApplicationMaster uses that token to autorize a resources request.  When an ApplicationMaster receives a list of containers it is entitled to use from the ResourceManager, it gets also a _ContainerToken_ for each container.  It uses that ContainerToken when it requests a NodeManager for a container to use, to authorize that request.

*to-do* How to prevent a malicious ApplicationMaster from allocating many ressources from the RessourceManager without actually using them? Or using them in a meaningless sense, e.g. do silly computations.  I assume ACLs of the scheduler's queues?


=== Spark

A processing model based on a DAG. Is primarly intended for immutable data. For immutable data, see streaming.

A node in the DAG (also called _lineage graph_) is a RDD (see below), an edge is a _transformation_.  To _create_ RDD s corresponding to root nodes in the DAG, we can create them from local or distributed fileystem, or from a process genereting it.  To make use of the RDD s, typically the ones corresponding to leaf nodes, there are so called _actions_. An action `materializes' an RDD.  We can e.g. dump it to local or distributed filesystem, or display it on the screen. Each action createas one _job_.  Multiple jobs can share the intermediate RDD s.

_Lazy evaluation_. Only when an action is invoked, the respective subgraph of the DAG is executed.  Lazy evaluation helps to reduce the number of passes over intermediate data by grouping operations together.  In Hadoop MapReduce, developers often have to spend a lot of time considering how to group together operations to minimize the number of MapReduce passes. In Spark users are free to organize their program into smaller, more managable operations.

_Resilient distributed dataset_ (_RDD_): An immutable collection of _values_ (or _objects_). Each value can be anything.  Is partitioned, each _partition_ can be on another machine.  The partitionboundary can be at any byte boundary. Thus when an RDD is for example stored on HDFS, we can take the HDFS blocks as partitioning.  _RDD types_: In a vanilla RDD, the values are of any type. There are _pair RDDs_ where each value is a key/value pair and _numeric RDDs_ where each value is of numeric type.  For these specialized RDDs there are additional transformations and actions.  Note that all RDDs can be viewed as vanilla RDDs and thus support all vanilla transformations and vanilla actions. Note that potentially an RDD is distributed across multiple input machines.

A transformation with _narrow dependency_ is one that can produce one output value of the output RDD by only seeing one or a few values of the input RDDs. Thus it can be easily parallelized, maybe even on the same machine.  The complementary concept is _wide dependecy_.  There one value in the output RDD depends on a lot of values in the input RDDs. hose transformations require shuffling as in MapReduce, which is an expensive operation. The wide dependency transformations having multiple input RDDs and a single output result in a _join_, the ones having a single input single output in a _simple shuffle_.

These paths in the DAG (of RDDs & transformations/creations/actions) that consist only of narrow dependency transformations can be combined into one node, called a _stage_, which procudes a DAG of stages transformations/creations/actions. Each stage can be parallelized on multiple machines, without the need for network communication.  The transformations between stages require shuffling and thus typically network communication.

*to-do* per RDD, must all values have the same type? If not, it would really be an type-value pair, where the type is implicit?

_Persisting RDDs for optimization_ (more accurately: caching): Recall that RDD s are by default always recomputed.  However a RDD can be requested to persist itself, i.e. each RDD has a persist attribute.  After computingthe RDD for the first time, its content is persisted (more accurately: cached).  As a consequence, if multiple actions depend on that RDD, the sub DAG consisting of that RDD and all its anchestors needs only be computed once.  There are options to specify how to persists, e.g. when memory/disk shall/shallnot be involved.  If memory is used up, Spark will evict some partitions to make room to persists new partitions.  Note that persisting is not an action, thus calling persist does not trigger evaluation.

[[prepartition_rdd]]
_partitioning RDDs_: Explicitely partitioning (and persisting) an pair RDD may improve performance if that RDD (more precisely, the stage containing it) is used multiple times as input for an transformation or action which internally shuffles.  The partitioning is such that same keys are in the same partition, i.e. the partitioning internally does shuffling.  Consider e.g. a join. If one of the two stages being input the the join is pre-partitioned, then the join can overtake that input without shuffling and only needs to shuffle the other input stage.  Partitioning is a transformation.  Since it only makes sense to prepartition if a following transformation internally shuffles, it in general doesn't make sense to not also persist the RDD resulting from the partitioning transformation. Each RDD stores as a property its _partitioning information_, such that transformation can make use of it. Each transformation knows wheter it retains / creates / destoys the partitioning and sets the partitioning information property of the output accordingly.  Some generic transformation which destroy partitioning (that is, Spark cannot guarantee that it is retained, given a user specified function) have counterparts which retaing partitioning. E.g. map and mapValues. Example of prepartitioning: "myrdd2 = myrdd1.partitionBy(new HashPartitioner(100)).persist()".

The DAG is executed on top of YARN.

Application interface: Write an application (e.g. in Java, Scala, Python) using the Spark library, and send the byte compiled program to the cluster.

Shell interface: Nice for interactive prototyping.

Data model summary: The entities are the RDD s. The things that can be done with the entites are creation, transformation and action.

Spark's and MapReduce's design goal was to address disk throughput bottlenecks (since disk throughput did not increase as much as capacity), rather than CPU / Memory / Network bottlenecks. That we now also can use multiple CPUs / memories in parallel is merely a nice side effect.

One goal was also that the sytem uses as much of the available ressources of the cluster as possible.  So companies owning the cluster can actually use what they invested money in.  So Spark works well when many people use the same cluster. This in contrast to when only one person uses the cluster -- in that case, maybe another system than Spark is appropriate.  So it was _not_ so much desisgned to having a particular high response time.


==== DataFrames & SparkSQL

A logical layer providing the relational model. Provides transformations and SQL on relations, called DataFrames. Sits on top of Spark's RDDs.

_DataFrame_: An RDD where a value represents a row. Thus the RDD as a whole represents a table. One can convert back and forth between a DataFrame and a RDD. When converting from RDD to DataFrame, a schema must be given, unless it can be infered automatically.

_SchemaInference_: Infere the relational schema from the source of a table; that source can be in various formats which may not explicitely and/or formally describe a schema.

_Logicial transformations_. One can write a program by using DataFrames, transformations & SQL on them as building blocks.

_Catalyst_ compiles and optimizes a program into an RDD DAG: The program is converted into a logical plan. The logical plan is optimized. It is converted to multiple physical plans. Based on a cost model, the best physical plan is selected. It is converted into an RDD DAG.

Think of RDD transformations as byte code.  DataFrame transformations are compiled into that byte code.  The prof said that on the RDD layer Spark will exactly do the user provided DAG, while on the logical DataFrame level spark will heavily optimize.  I suspect that also on the RDD level there can be internal optimizations, just usually not with a same big impact.

_ColumnarStorage_: A DataFrame is stored in memory by column, i.e. one column in memory is a sequence of cells. An advantage is that a query often only looks at some rows of a table. The name of the column needs only to be stored once, wheras in a naive RDD each table field redundantly has to store the row & column name.


==== Some creations

_SparkContext.parallelize_(collection): Depending on collection type, a vanilla RDD or a pair RDD is created.


==== Some general transformations

_filter_ (selection in relational algebra): A predicate function determines for each value in the RDD wether or not it passes.

_map_: Applies a function to every value in the input RDD, producing a new output RDD of same size and possibly with different value type.  As in MapReduce, only that it doesn't have to be about key-values.

_map values_: As map, but only for pair RDDs; the user supplied function only operates on the values.

_flatMap_: Applies a function to every value in the input RDD. The output of the function is a collection. Thus logically from a initial point of view is a list of collections. The actual output is the flatening, i.e. just a list of elements.

_flat map values_: analogous to map values

_distinct_: Removes duplicates. May require shuffling.

_sample_: Similar to filter. Lets through a random sample / subset of the input RDD.

_union_: Concatenate input RDDs

_intersection_: Produce one RDD per value that exists in any of the input RDD s. *to-do* understand better the details

_subtract_: Produce on RDD per value that exists in the lhs RDD, except for those values that exist also in the rhs RDD.

_cartesian product_: Think lhs RDD as column vector and rhs RDD as row vector. The resulting RDD corresponds to the product matrix, each value being a tuple.

_group by_: Applies a function to every value. The function returns a key, and thus logically yields a key/value pair. Then it continuous like groupByKey.


==== Some key-value pair transformations

RDDs are not required to be lists of key-value pairs, but they can be, and some transformations make use of them.

_group by key_: Groups by key. The result is a list of (key,value-collection) pairs.

_cogroup_: Like a group by key, but for multiple input RDDs. Each resulting value has the logical form (key, (Iterable, Iterable, ...)), where each iterable is w.r.t. its corresponding input RDD.

_join_: Joins the two input RDD s by key. The output RDD is a list of (key,value-collection) pairs, where each value-collection contains either one or two values.

_reduce by key_:  Groups by key, and then reduces each group to a single key-value pair by an user given function.  The function must be commutative and associative and the type of the returned pair must match the type of each input pair.  Expensive since it requires shuffling.

_map values_: Each value is transformed by a user given function.

_keys_: Drop the values, resulting in a list of keys. I.e. the output is no longer a list of key-value pairs, but technically a list of values.

_values_: Analogous to the `keys' transformation.

_subtract by key_: Given two input RDD, keep only those key-value pairs of the lhs RDS where the key does _not_ appear in the rhs RDD.

_reduce by key_: As reduce action, but for the values of each key seperately.

_fold by key_: As fold action, but for the values of each key seperately.

_combine by key_: Most general of reduceByKey and foldByKey. In each partition individually, the first time a key is encountered that hasn't been seen before, a user supplied createCombiner function is called which creates an initial value out of the value associated with that key.  When it's a key that has been seen before, the user supplied mergeValue function is called. When merging partitions, the user supplied mergeCombiners function is called to merge the collection values of each key.

_partitionBy_: *to-do* Book p62. I don't understand why partitioning should help.


==== Some actions

*to-do*

_collect_: Access the RDD in memory. I.e. the RDD must fit into memory, and thus collect can't be used on large RDDs.

_saveAs(Text|Sequence|...)File_: As the name sais.

_count_: Returns the number of values.

_count by value_: Returns a list of tuples (value, valuecount).

_reduce_ (as in MapReduce): User provides a binary function returning a value of the same type as the two arguments. The function is applied to (1st, 2nd), then (result, 3rd) and so on. E.g. to sum all elements, the function would would e.g. be "lambda x,y: x+y".

_fold_: As reduce, but gets passed a "zero value" to be used for the initial call on each partition.

_take_: Return the first n values.


==== Some key-value pair actions

_collect as map_: As collect, but using the language's dictionary data type.

_count by key_: For each key, return the number of occurences.

_lookup_: Return value for a given key.


=== Tail latency

When computation is parallelized among many nodes, it is almost guaranteed that one or more node needs substantially more time than all the other nodes.  When a single node has a probability of p of requiring less time than 1s (SLA), then when having n machines, the probability that at least one needs more than 1s is (1-p)^n^, which goes to 1 quickly.

Some reasons why some taks take substantially longer than the average:

- When resources are shared, some tasks might be unlucky an have to wait longer to get access than on average.

- A background deamon doesn't use much time on average, but every now and then it might use quit a bit of resources.

- Periodic maintenance activities, e.g. log compaction, (heap) garbage collection, garbage collection in SSDs, data reconstruction in a DFS.

- Multiple layers of queueing in intermediate servers and network switches.

- Computers might throttle under high CPU load to avoid overheadting. There's a delay when waking up a computer or some piece of HW from a power-saving mode.

_(naive) hedge request_: Execute each task / request twice, the one first done wins. Trades time for ressources.

_(defered) hedge request_: Duplicate a task / request only after the execution time of the original tasks exceeds the x percentile (say 95%) of the empirical response time, where the empirical response time was measured by some past benchmark.  Possibly one benchmark for each class of tasks.

_tied request_: Put the task into two queues. The task which starts first wins, the other is removed from the queue. That helps to mitigate the problem that a few tasks wait long in their queue.

_micro-partitions_: 100 or 1000 partitions per machine. When benchmarks show that a node is slower, assign less partitions to it, or in an extreme case shut it down completely.

_good enough_: If the true / optimal result is not mandatory, we can just ignore tasks that take too long.


== Data Stores

=== HBase

HBase is the open source version of Google's Bigtable. Based on the wide column store model.

Each table has a row ID column being by definition the primary key. Columns are grouped in a column families.  The idea is to group together whats frequently accessed together.  The column families must be known in advance, but not the columns.  The number of columns can be very high (compared to relational DB).

Rows have an order.

Operations: put/get/delete (row), scan (rows)

Can store billions of rows; a traditional RDBMS (single machine) can store millions.

Scanninig, i.e. iterating over all rows of a table, is an relative expensive operation since its not trivial.

Has low latency (relative to HDFS) because of the memstore and the block cache; latency due to access to underlying DFS falls aften away.

Best practice: Keep row ids and column names short. Rational: Every KeyValue stores them.  I.e. a given row id or column name appears a lot of times.  Keeping them short lets you save space, both on disk and in memory. I.e. you can pack more KeyValues into your memory.

Offers row level atomicity. Can offer it because one region is handled by exactly one region server.

Good (relative to RDBMS) for sparse data (sparse meaning not every column, given a row, contains a value).

*to-do* Replication, what kind of consistency is offered


==== Implementation of HBase / Physical layer of HBase

Partition table first horizontally (i.e. group rows), then vertically (as already done by column families).  We need horizontal partitioning because we can have billions of rows not even fitting on a single machine.  A horizontal partition is given by the range (min-incl, max-excl).  Such a range of rows is called a _region_.  Obviously the max-excl equals the min-incl of the next partition.  The intersection of horizontal and vertical partitioning is what is stored together and is called a _store_. I.e. a region is composed of multiple stores.

Master slave architecture.  The master is called HMaster, a slave is called region server.  The _HMaster_'s responsibility is the meta data.  A _region server_ is responsible for a set of regions, and thus implicitely also for the stores of that regions.  A region is assigned to one region server.

A store is stored as one or more files, called _store file_ (or _HFiles_), on a DFS.  One store file is actually an _SSTable_, a flat sorted list of key-value pairs, one pair also called  _KeyValue_, plus an index for faster key lookup.  The _index_ contains the first key of every block in the HFile.  A store file is immutable.  The index is loaded into memory.  The key is logically a (rowid, column-number) pair refering to a cell of the original table, and the value storing the content of that cell.  KeyValues are stored sequentially, forming a bytestream, making it efficient for transfer.  Each KeyValue is stored as tuple (keylength, valuelength, key, value).  The length of the keylength and valuelength elements are fixed width, e.g. 32bit.  Practically the key is a tuple

(rowidlength, rowid, columnfamiliylength, columnfamily, columnqualifier, timestamp, keytype)

Again rowidlength, columnfamiliylength are fixed width, and their value defines the length of the respective tuple element.  Timestamp and keytype are fixed width.  So columnqualifier length can be computed, taking the outer keylength into account.  Technically, the columnfamily is not required, since we already know in which column family we are.  The timestamp is the version.  The keytype is actually a deletion mark.

The key-value pairs of the store file are read in blocks of about 64kB; no pair is ever split.  Note that these are not the same blocks as the ones the underlying DFS might have.

_put_: First write to HLog file, then to the memstore.  The _HLog_ (or _write ahead log_ or _WAL_), a journal, is a security measure in case we loose what's in the memory before the memory could be flushed to disk.  It is stored on the underlying DFS.  There is one HLog per region server.  The _memstore_ is an in memory cache of modified KeyValues.  There is one memstore per store.  When certain criterions are met, the memstore is flushed to disk, creating a new storefile (as always with sorted rows).  After flushing, the log file can be discarded.  Thus we keep generetaing partially redundant store files (but remember that each KeyValue as an version, and we have a total order).  Every now and then, we do _compaction_:  Replace the existing HFiles by one new HFile by merging them.

*to-do* what exactly is the benefit of the WAL file? Now I also have to synchronously write the data. I could directly append to a speical unsorted HFile instead? Or is it to have the simple design choice that hfiles are always sorted.

To reduce latency and increase throughput, besides the memstore,  there's also a read cache called _block cache_ containing the last read HBase blocks.  Thanks to the block cache and the memstore, we don't always have to access the underlying DFS.

_delete_: Similar to put, where the modification is to check the `is deleted' flag.

*to-do* how does the in memory index look like about? KeyValue can be in many places: cache, memstore, multiple store files.

*to-do* really understand lsm-tree and compaction

Guarantees ACID on the row level via per-row locks. That gives us total order of row versions.

Overview:

Table +
Region +
Store  +
StoreFile(n) + Memstore(1) + HLog(1) +
Block | - +
KeyValue | KeyValue


==== Optimizations

Besides the memstore, there's also an in-memory _cache_ of KeyValue s.  A unit of the cache is a block.  The MemStore is for KeyValue s not yet flushed to disk, the cache is for  faster access to already persisted KeyValue s.  The cache is composed of two hierarchy levels, the _LRU BlockCache_ and the _bucket cache_.  LRU BlockCache caches the last recently used blocks.

_short circuiting_ / _colocation_ (process data where it is stored):  Is when the requested block of the underlying DFS is stored on the same physical node as the region server requesting that block runs on.  Thus effectively the region server reads the block from its own local drive, without paying network overhead.  This is a situation that occures most of the time as a result of the design of HDFS and HBase, in particular from the <<hdfs_replica_placement>> strategy of prefering to store a block on the client itself.  One could think that due to HDFS having a life, over time the HDFS data node (runing on the same physical node as the HBase region server) will no longer itself store the HDFS block.  But due to the compaction of HFiles and the HDFS replica placement strategy, we will restore colocation over time.

An in-memory _bloom filter_ is used to reduce access to HFiles when searching keys.  Size of bloom filter and number of hash functions used is subject to research.

The LSM-tree structure's purpose is to minimize the number of required compactions.


=== Cassandra

Similar to the one of HBase.


=== Google Bigtable

Successor and proprietary version of HBase.


=== Spanner

Distributed NewSQL database, similar to HBase.  Claims to bring back ACID / externally-consistent distributed transactions.

Data Model: Multi-column primary key. A _timestamp_ column.  Partition table horizontally into _directories_ (region in HBase).  A _tablet_ is a set of directories.

Two level Master-slave architecture.  The one top level master is called the _universemaster_,  the masters are called _zonemasters_, the slaves are called _spanserver_. There's one zonemaster-spanservers subtree per data center.

Can store trillions of rows; a traditional RDBMS (single machine) can store millions, HBase billions. Can have hundrets of data centers, millions of machines.

Sacrifice high availability to get low latency.


== Data models

_Key-value model_:  A data model. Some mapping from a key to a value.

_Column store_ (or _column-oriented DB_): A data model. Store data by columns (as opposed to by rows). One advantage is that subsequent cells in the same row tend to be similar, thus compression algorithms tend to work well.

_Wide column store_: A data model. Store data by rows, keys identify rows, `group' columns in families. However each row can have its own individual columns.  Thus a wide column store can be interpreted as a two level nested key-value store.  The key of the outer level is the row id, the key of the inner level is the column id, and the column family id is also given.  In the tabular model, joins are very expensive.  In the tabular model we love to have data in normal form, and as a consequence there are many joins.  Paradigm of BigTable: store together what is accessed together (i.e. quite the opposite of normal forms). That makes batch processing better, since we only have to pay latency once (recall we want to avoid latency as much as possible), and after that it's just throughput. To fulfill the paradigm, we denormalize. That can also be seen as precomputing the joins we expect to occur often.  Thus reads become faster.  The price is that we introduced anomalies, so writes are now more expensive. Examples: Google's BigTable, HBase, Cassandra.

_relational database_: Homogenous collection of flat items.

_document store_ (or _document-oriented database_): Heterogenous collection of arborescent items. Replaces the concept of a `row' in a relational database with a more flexible model, the `document'. Scaling out is much easier than with a relational database. Scales to billions of documents and PBs of total storage.  We have project and select, but not join.  Join is less important, since we already denormalized (i.e. pre-joined).  Can also be interpreted as a key-value model if the documents have ids (as MongoDB does).  Implementations: mongoDB, CosmosDB (Azure) CouchDB, elasticsearch, existdb, Cloudant, ArangoDB, basex, MarkLogic. There are ways to store trees in an relational database, but its not really nice. It works better if the trees are (almost) heterogneous. It works even better if they are (almost) flat.

_graph databases_: see <<graph_databases>>

_unstructured document_: text?

_semi-structered document_  (or _document-oriented information_): Database model of document store.

_structured documents_: table

*to-do* diagram which plots (# entities, # storage per entity (or total storage)) where document store, relational data base, object store etc.


=== Cube

A _data cube_ (or _OLAP cube_ or just _cube_ or _multidimensional dataset_) is a data model meant for OLAP. You can also view it as a multidimensional spread sheet (spread sheet as in Excel). Each cell has one or more value in it, also called _measure_. The number of measures and type of each measure is uniform for each cell of the cube. Each dimension might be a property such as name, country etc. A _facts table_ is a data cube flatened to a table. Each dimension gets a column, and there's an additional column for each measure. Each row in the facts table is called a _fact_.

*to-do* compare and contrast _raw-data cube_ and _formal data cube_; make the above paragraph correct regarding this distinction.

Operations on a data cube:

_Aggregation_ (loosely equivalent to SQL group-by on the facts table): `Remove' one dimension by aggregating all values along that dimension, resulting in a new data cube with one dimension less. E.g. if one dimension is date, but we don't care about the date, we can ignore the date by aggregating over it.

_Slicing_ (equivalent to SQL selection (WHERE clause) on the facts table): Regarding a given dimension, only care about values fulfilling a given predicate, and drop the other values. E.g. if one dimension is customer_firstname, the predicate might be (customer_firstname = "Bob" or customer_firstname = "Alice"). The dimensions of the cube remain the same.

_Dicing_ (equivalent to SQL group-by): Partition each dimension. Possibly turn one dimension into one single partition. (Only relevant if you try to imagine this: Each partition / `slice' must not be continuous along its dimension. So you might mentally want first to reorder such that each partition is continous along its dimension and then truly a slice). So the cube is diced into dices. Each die, consisting of multiple cells along each dimension, becomes a cell of the new cube by aggregating all cells of the die. Say one of the many dimensions is product, which has satelite data which has a color column, then `dicing by product.color' means that we partition the product dimension by product.color, and each other dimension is turned into one single partion.

*to-do* so dicing by dim1 is the same as aggergating over all other dimensions?

_Drill-down_ is the process of making the dicing partitions more finely (which includes dicing more dimensions) and / or make the slicing predicates more specific (which includes making the predicate look at more dimensions). _Role-up_ is the opposite: make the dicing partitions more coarse (which includes dicing less dimensions) and / or make the slicing predicates less specific (which includes making the predicate look at less dimensions).

Assuming OLAP is implemented as <<ROLAP>>, and assuming a star schema (see ROLAP) is used. The above data cube operations can be implemented in SQL as follows:

Aggregating over dim1:

--------------------------------------------------
SELECT /* all dimension cols except dim1,
          SUM(measi), i for all measure cols */
FROM factstable
GROUP BY /* all dimension cols except dim1 */
--------------------------------------------------

Slice on (dim1 = 42 or dim = 77). The core part is the WHERE clause. So if you add that core part to another query, you added "slice on (dim1 = 42 or dim = 77)" to that query.

--------------------------------------------------
SELECT *
FROM factstable
WHERE /* filter on dim1, e.g. "dim1 = 42 or dim = 77" */
--------------------------------------------------

Dice by dim1Table.color:

------------------------------------------------------------
SELECT dim1Table.color,
       /* SUM(measi), i for all measure cols */
FROM factstable JOIN dim1Table on dim1 = dim1Table.id
GROUP BY dim1Table.color
------------------------------------------------------------

Dice by dim1Table.color and dim2 (i.e. every value of dim2 gets its own partition):

------------------------------------------------------------
SELECT dim1Table.color, dim2,
       /* SUM(measi), i for all measure cols */
FROM factstable JOIN dim1Table on dim1 = dim1Table.id
GROUP BY dim1Table.color, dim2
------------------------------------------------------------

As syntactic sugar, to make the union of multiple dicers less verbose: The following is the union of the the tables produced by (dicing by dim1 and dim2, dicing by dim1, dicing by no dimension (i.e. aggregate all the cells of the cube)). The produced data can be used to create a cross tabulation table (only that the totals for dim2 are missing)

------------------------------------------------------------
SELECT dim1, dim2,
       /* SUM(measi), i for all measure cols */
GROUP BY GROUPING SETS ((dim1, dim2),(dim1),())
------------------------------------------------------------

As syntactic sugar, the previous can even be made more concise:

------------------------------------------------------------
SELECT dim1, dim2,
       /* SUM(measi), i for all measure cols */
GROUP BY ROLLUP (dim1, dim2)
------------------------------------------------------------

The previous produced the data required for a cross tabulation table, however the totals for dim2 were missing. To get the data required for a complete cross tabulation table:

------------------------------------------------------------
SELECT dim1, dim2,
       /* SUM(measi), i for all measure cols */
GROUP BY CUBE (dim1, dim2)
------------------------------------------------------------


[[MDX]]
==== Query Language: MDX

_MDX_ (Multi-Dimensional eXpressions) is a query language for cubes. A dimension can be hierarchical. E.g. in the geography dimension, the top level values are Europe, Asia, America etc., Europa in turn can be zoomed in to yield Switzerland, Germany etc., Switzerland can be zoomed in to yield Zürich, Schwyz, etc.


==== Syntax: XBRL

_XBRL_ (eXtensible Business Reporting Language) is a XML based syntax for storing cubes. Each fact (i.e. row of the facts table) is stored as an XML element. The attributes of which define the dimension values, the content the measurements.


[[graph_databases]]
=== Graph Database

Motivation:

- Tables have exensive joins. In the worst case, there are multiple joins, e.g. when we traverse tables. Trees (document store) try to solve the problem by pre-joining (aka denormalization). However the downside is that we need to know in advance the joins we want to pre-join for. In graph databases, relationships are first-class citzens. No joins required, everything is pre-joined. In RDBMS, performance of a join degrates with table size. In a graph database, query cost is proportional to only the size of the part of the graph traversed to satisfy that query.

- In a RDBMS, one has to decide on a schema up front. In a graph database, the schema can evolve in tandem with the actual data.

Relationships are first-class citizens of the graph data model. In RDBMS, they are implemented indirectly, e.g. via foreign-keys.

There are two kinds of graph databases, (labeled) property graph and triple stores (RDF):

A _graph compute engine_ does global graph computations / queries.

_labeled property graph_: A kind of graph database. A _node_ can have properties and labels. An _edges_ (aka _relationships_) and can have properties and one label. Relationships are directional. A _property_ (or _attribute_) is a key-value pair. A _label_ can also be thought of as a _type_ or role, but in this view note that a node can be of multiple types. Technically a label is the same as an additional property with key "type" and as value a set of strings, however labels are more explicit and allow for more direct / convenient querying. Examples: Neo4j (native graph database, i.e. has its own physical way of storing, opposed to be on top of say an RDBMS). Note that unlike in many other areas, currently in graph data bases we don't do sharding, because the whole point of a graph is to go from any place to any place quickly, and with sharding it would mean that we may go from one machine to another machine while traversing the graph. However in near future we might find ways how to do sharding efficiently.

_Triple stores (RDF)_: A kind of graph database, see <<triple_store>>. Examples: Semantic Web.

_index-free adjacency_ (or _native graph processing_): Nodes point to each other via pointers. The key message of index-free adjacency is, that the complexity to traverse the whole graph is O(n), where n is the number of nodes.


==== Cypher

A declarative query language  for querying and updating a labeled property graph, specifically Neo4j. Based on the concept of _specification by example_.

_Transactions_: Any updating query will run in a transaction and thus will always either fully succeed, or not succeed at all. If there is already a transiction in the running context, that one will be used. That can be used to run multiple updating queries by opening a transaction, running all the queries, finish the transaction.  If there is no transaction in the running context, the updating query will automatically be wrapped into a transaction.

A _pattern_ consists of nodes and relationships connecting the nodes. A _relationship_ is one of: $$--$$, $$-->$$, $$<--$$. A _relationship restriction_ is enclosed in brackets, and the whole thing is inserted into the relationship ASCII arrows like so: $$-[mylabel]->$$. The previous example states that the relationship must have a label mylabel. If you later want to refer to that relationship, [myvar:mylabel] will bind the found relation to the variable myvar. To match either of multiple relationship labels, seperate them by pipes like so [:mylabel1|mylabel2]. To match properties, put them in curly braces (possibly preceded by label stuff) like so [{myproperty:somevalue}]. A _node_ is put in parenthesis like so: (). You can also say (myvar): if myvar is already defined, that defines a node, otherwise the found node is bound to myvar.

Cypher is composed of _clauses_. Each clause starts with a keyword.

_START_: To specify nodes and / or relationships and bind them to variables. For example ``START myvarname=node:myindexname1(mypropkey1="somevalue")'' uses the index myindexname1 to find all nodes with matching property, and assigns the result to myvarname.

_MATCH_: A comma separted list of patterns. Patterns are described above.

_WHERE_: To further conastrain matches

_RETURN_: To specify which nodes, relationships, and properties to be returned.

_CREATE_ / _CREATE UNIQUE_: Creates nodes and relationships. ``CREATE (mynode), (mynode2:label1:label2 {key1:'value1', key2:'value2'})'' creates a new node and binds it to the variable mynode, and it creates another node with the given labels and properties and binds it to mynode2. ``Create (node1)->[myrelation:label1]->(node2)'' creates a relation from node1 to node2, attaches label label1 to it, and assigns the new relation to the variable myrelation.

_DELETE_: DELETES nodes, relationships, and properties.


[[triple_store]]
=== Triple store / Resource description framework (RDF)

A way of defining a graph database. A triple (subject, property, object) defines a directed edge in a graph. The object is the source node, the property is the property of the edge, the subject is the destination node.

*to-do* slides 121 122 123 etc (RDF schema, classes, types, ontology, simple entailment, OWL (Web Ontology Language) )

*to-do* what's the difference between triple store and rdf? Is triple store more generic than rdf? Are there things which are an triple store but not rdf?

Formats: RDF/XML, Turtle, JSON-LD, RDFa, N-Triples


==== RDF/XML

A syntax for RDF.  A prefix:localname is semantically concatened to build an IRI. The IRI the prefix is bound to typically ends with a sharp sign (#).

The +rdf:about+ attribute of an +rdf:Description+ element specifies the subject. Property / object can be defined either by an simple element where the element name is the property and the text content the object (a value). Alternatively it's an empty element where the element name is the property and the +rdf:resource+ attribute specifies the object and optionally the +rdf:datatype+ attribute defines the XML type of the text content. The child element +rdf:type+ is as any another property (the propert being rdf:type and the object being the value of the rdf:resource attribute) and is not required.

--------------------------------------------------
<rdf:RDF
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
    xmlns:foo="myIRI#">
    <rdf:Description rdf:about="Subject1IRI#self">
        <rdf:type rdf:resource="myIRI#mytype"/> <!-- also a property -->
        <foo:Property1 rdf:resource="Object1_IRI"/>
        <foo:Property2>Object2 literal value</foo:Property2>
        <foo:Property3 rdf:datatype="...">1990-07-04</foo:Property3>
    </rdf:Description>
</rdf:RDF>
--------------------------------------------------

The above rdf:Description with an rdf:type child element can be abbreviated to, i.e. rdf:Description and it's child element rdf:type are merged into one element:

--------------------------------------------------
<foo:mytype rdf:about="Subject1IRI#self">
   ... the three properties foo:Property(1|2|3) ...
</foo:mytype>
--------------------------------------------------


==== JSON-LD

A syntax for RDF.

--------------------------------------------------
{
  "@context" : {
    "rdf" : "specialIRI",
    "myprefix1" : "namespaceIRI1",
  }
  "@id" : "myIRI",
  "rdf:type" : "someIRI",
  "myprefix1:myproperty1" : "someotherIRI",
  "myprefix1:myproperty2" : somevalue
}
--------------------------------------------------

==== RDF Schema

A schema for RDF.

indentation = rdf:subClassOf:

--------------------------------------------------
rdfs:Resource
  rdfs:Class
--------------------------------------------------


==== SPARQL

A query language for triple stores (aka RDF).

--------------------------------------------------
PREFIX myprefix1: namespaceURI1

SELECT ?s
WHERE {
  ?s myprefix:myproperty1 myprefix:someValue1 .
  ?s myprefix:myproperty2 myprefix:someValue2 .
}
--------------------------------------------------


==== Turtle

--------------------------------------------------
@prefix myprefix1: namespaceURI1 .
@prefix myprefix2: namespaceURI2 .
eth:self myprefix1:myproperty1 someURI1 , someURI2, someURI3 ;
         myprefix2:myproperty2 somevalue .
--------------------------------------------------


=== JSON

https://www.json.org/

Technically, an object can have multiple occurences of the same key, but one should not do that.


== Storages

=== Key-value store

Same data model as object storage, but implemented differently. Intend to have low latency. Smaller objects (kB sized). No metadata. Note the key-value store is not the same as key-value model.

Much simpler than a relational database. We drop consistency (we only have eventual consistency) and gain availability and partition tolerance and scalability.

Simple things are much easier to scale out than monolithic things (such as a table in the relational model).

In contrast to object storage, no metadata.

Examples: DynamoDb


[[block_storage]]
=== Block storage

Object is divided into blocks.  Large amount of huge files: millions of PB files.  I.e. limited in number files.  An object (aka file) is a sequence of blocks (or chunks).

Block size on a local file system is \~4kB; in a relational database \~32kB. In a distributed file system such like HDFS it's ~128MB -- good compromise between latency and throughput.  Too small blocks would mean too many blocks to wait for, and since its over the network latency would be bad (relative to the time it takes to transmitt the complete block). Too big means we can't even put it on a single machine.  Also if the number of blocks of a file is smaller than the number of tasks of a mapreduce, we can't parallelize as much.

Examples: GFS, HDFS


=== Object storage / document database

huge amount of large files: billions of TB files.  I.e. limited in file size.  As a consequence, a file fits on a single machine. An object is a black box.

Object storage lets you scale. Make model of local filesystem simpler. 1) throw away hierachy (file system tree). 2) Metadata is no longer fixed but flexible: assign values to keys. 3) Flat and global key-value model (associate IDs to files). 4) use commodity HW.

on scalability issues with a local drive: A data base on a local machine might work for that machine.  Maybe, if you're lucky, it even works when accessed by multiple people on a (small) LAN.  But it doesn't work on a WAN.  The disk just can't cope with the amount of requests.  Also, on a typical file system you can't have billions of files.

latency is low relative to a database: s3 ~ few 100ms, typical database 1-9ms, both where client is in same region.

Examples: S3


=== Chord / distributed hash table

A protocol for a peer-to-peer distributed hash table. Used by DynamoDB.

Assigning keys to nodes:  Say the key size is 128bit. Imagine the 128bit numbers on a ring.  Each node uniformily at random chooses a 128bit number.  Then each node stores the keys between itself and the previous N ≥ 1 nodes. If N > 1, we get replication.  Note that this assignment of keys to nodes is very simple and predetermined.  Also note it's only about assigning keys to nodes; there's no relation to how nodes are physically conencted.

Query, i.e. finding a node responsible for key k: The trivial solution would be that the nodes on the ring form a linked list, which would result in linear time query.  Here each node keeps a _finger table_, where the i-th entry stores a `pointer' to the node being 2^i^ nodes away.

Pros:

- highly scalable

  * incremental stability (easy to add/remove nodes)

- robust against failure

- self organizing

Cons:

- being a hashtable there's only lookup by key (e.g. no text search)

- nothing said about data integrity (here replication is about loss, not corruption)

- security issues (you need to have full control over the nodes themselves and the set of existing nodes)

- bad luck when nodes choose randomly their position on the ring and there are large gaps giving big burden on the node at the end of the gap

- not considering that nodes are heterogenous (i.e. have different power)

The last two can be solved by the following extension: Each node gets a number of _tokens_ (or _virtual nodes_), the number proportional to the node's power. Now instead of nodes, we place place the tokens on the ring. Since there are now many tokens, and due to the central limit theorem, it's virtually impossible to have large gaps.  Also, we now adapt to the heterogenous network.  When adding a node, it takes over tokens from existing nodes.  When deleting a node, its tokens are redistributed among remaining nodes.

*to-do* make this `extension' an part of the initial thing

_vector clock_: Each object as associated a set, called _context_, of nodeid-number pairs, where nodeid is unique in the set. The number denotes how many times the given number wrote (put) the object. Multiple contexts for a given object form a partial order (i.e. a DAG).


questions:

- Slides 197+: I don't see how this works in the distributed system with no masters. Where are the preference lists stored? What does partition-aware client mean?

- why not return (C,[(n1,3)]) , (D,[(n1,2), (n2,1)]). Answer: The protocol is such that it's a black box for the client


=== Google File System (GFS)

Requirements:

- Throughput has top priority.

- A capacity of millions of PB files.

- Fault tolerance and robustness (a local disk might fail, in a clustser with 10 tousands nodes, nodes _will_ fail). That means we need monitoring of the disks status, error detection, automatic recovery, so at the top layer we get fault tolerant.

- Latency has secondary priority.

File update model: Only append and upsert, i.e. no random access.  Appending should work for hundreds of clients in parallel.  This is a suitable model e.g. for sensors, logs, intermediate data.

Master slave architecture.


=== Hadoop distributed file system (HDFS)

Open source distributed file system. Open source version of GFS. MapReduce. Wide column store (HBase). Block storage (by default 128MB blocks (configurable on a file-by-file basis), 64 bit block id, see also <<block_storage>> for pro/cons of block sizes). File hierarchy model.

Designed for:

- Peta byte files. I.e. a single file doesn't fit on a single drive, for that alone we need block storage.  A file is divided into blocks. Each block is replicated among multiple data nodes for fault tolerance.

- Streaming data access patterns: i.e. it's expected that the data accessing pattern is a write-once, read-many-times.  It is expected that a large portion of a file is read, so data throughput is more important than the latency to read the first bytes.

- Scaling out, i.e. using commodity HW.

Disadvantages:

- Can't offer low latency access

- Can't offer lots of small files. This is also because the name nodes hold the filesystem metadata in memory, so the amount of memory of a name node limits the number of files.

- Can't offer multiple writers, and can only append to the end of the file (i.e. can't write to arbitrary positions).

- Not suited for running across data centers.

In terms of CAP theorem: We have consistency. But due to the single master, we have neither full availability nor full partition tollerance.

Master slave architecture.  The master is called the name node, the slaves are called data nodes.

The _name node_ (or _primary name node_ or _active name node_) cares about the filesystem meta data: The _file namespace_ (i.e. the file tree), _file to block mapping_ (for each file a list of block ids constituting it), and _block locations_ (for each block id where it is stored).  It keeps all that information in memory.  Later it is described in what ways that information is persisted.

A _data node_ only stores bocks, storing them on its local drives, using a traditional local filesystem.  A data node is identified by an storage id, which does not change if the IP of the data node changes.  A data node stores its storage id.  A data node stores a checksum for each block.  When a client reads/writes blocks from/to a data node, the data sending side always also transmits the checksum, and the receiving side has to verify.

_Client protocol_ (a RPC protocol): Client first makes metadata operation request to name node (master).  Note that a client might be a node within the cluster, e.g. a name node.  For a read/write, as answer it receives the block locations: For each block id, the multiple (see specified number of replicas) node locations (IPs) where the block is stored, sorted by distance, so the client can choose to talk to the closests data node. See data transfer protocol below how the client continuous.

_Data node protocol_ (a RPC protocol):  Between data node and name node, it's always the data node who intitiates the communicuation. E.g. registration ("Hi, I'm a new data node"). Every x seconds a name node sends a hearbeat("I'm still alive"). When the name node wants something from a data node (e.g. a block operation), the name node does so via its response to a heart beat.  When a name node received a block (see write in the data transfer protocol), he acknowledges to the master node with a BlockReceived message.  Every y hours, the data node sends a block report to the name node (the list of block, i.e. their ids, it currently stores).

_Data transfer protocol_ (a streaming protocol):  See client protocol first. See following read and write.

For a _read_, as answer the client receives the block locations: For each block id, the multiple node locations where the block is stored, sorted by distance (see Hadoop's measure of closeness), so the client can choose to talk to the closest data node.  Multiple clients can read in parallel from the same file / blocks.

*to-do* can a client read different blocks in parallel, i.e. block1 from datanode1, block2 from datanode2 etc.

For a _write_, it's analogous.  Recall that writes can only append.  For each new block a client wants to write, it receives a collection of block locations from the name node.  The client doesn't need to care about replication.  Per block, the client talks to the closest name node, tells it also the other name nodes that need to replicate the block, and the name nodes take care of replication themselves by creating a _data pipeline_ which minimizes the distance from the client to the last data node.  The data node receiving the client's write request asynchronously sends an acknowldge to the client once all replicates are successfully written (*to-do* the ack is passed within the pipeline in reverse direction).  Recall from data node protocol that each node receiving a block sends a BlockReceived message to the name node.  For each client-initiated transaction, the change to the filesystem meta data is commited to the client only after the name nodes journal has been flushed to disk (assistant is not sure whether name node sends an acknowledgment to the client).  There is at most one writer to a file at any point in time, ensured by having a lock on each file.  I.e. before the write, the client has to open the file for writing to acquire the lock, and at the end he has to close the file to free the lock.

*to-do* fix: its the client that organizes the pipeline

_Hadoop's measure of closeness / distance_: The network is represented as a tree / hierarchy.  The hierarchy is not fixed, however common is (internet, data center, rack, node).  The distance beween two nodes is the sum of the hierarchy levels between a node and the common anchestor.  Rational: Due Hadoop's design goals and the resulting architecture, throughput is more important than latency, so a possible measure would be bandwith between nodes. That however is difficult to measure. The given metric is an approximation.

*to-do* Is the ack of the write back to the client async to replication? Even prof didnt know.

[[hdfs_replica_placement]]
_Replica placement_ (or _block placement_), i.e. which nodes store a block replica: The first block/replica is stored on the client itself, if the client is a data node in the same cluster, and a `random' (load balancer prefers certain ones) node which is not too busy and not too fully otherwise. The 2nd replica is stored on on a node in a different rack within the same cluster (If it were stored in the same rack, that would mean that the same rack is guaranteed to store two replicas, which is a shame if the rack fails). The 3rd replica is stored on a node in the same rack as the 2nd. The further replicas are stored at `random' nodes, but if possible at most one replica per node (we care about a node failing as a whole, not that only parts of a single drive fail) and at most two replicas per rack.

Replica placement considerations: Reliabilty (how relyable is a node), read/write bandwith (how fast is the network), block distribution (what's the distance from the data node to the client (which might be itself a data node)).

Number of replicas is specified per file. The default is 3.


==== Dealing with master being bottleneck

The name node is bottle neck and single point of failure. The following describes ways how HDFS tries to mitigate the problem. Note that the use case of unexpected failure of a name node is rare, so in practice the use case of planned downtime for maintenance is more important.

The master uses its local filesystem to persist the file namespace and the file to block mappings in a _checkpoint_ and a _journal_ (or _edit log_, log of edits since last checkpoint).  Thus there are kind of three layers: memory (full), edit log, checkpoint (full). Note that the block locations are not persisted, because the name node gets to know them via the heart beats of the name nodes.  The name node always writes to the journal, as opposed to the checkpoint.  The checkpoint is only modified in explicit situations, such as startup or explicit administrator commands.  When restarting the name node, we need to read the namespace file and the edit log, and apply the changes recorded in the edit log on top of the information in the namespace file. Such a restart would take about 30 minutes, which is obviously too long.

The checkpoint and journal can be configured to be stored on multiple places. Recommended practice is to place each a replica on a local disk (preventing loss from failure of a single disk), and one replica replica on a remote NFS server (preventing loss from node failure).

A _secondary name node_ (or _check point node_) shadows the primary name node and has the sole responsibility to make a checkpoint every once in a while, i.e. combine the primary name node's checkpoint and journal into a new checkpoint, and send back the new checkpoint to the primary name node.  When the primary name nodes replaces its checkpoint with the new checkpoint, it also can truncate its journal. Good practice is to create a daily checkpoint. A smaller journal means faster startup time, and less risk that any part of the journal is corrupted.

A _backup name node_ is like a secondary name node, but additionally has the file system meta data in memory, just as the primary name node.  From the view point of the primary name node, a backup name node is just another journal store.  The backup name node thus recievies a stream of file system meta data transactions.  If the primary name node fails, the backup name node can jump in, without having to reply a journal to a checkpoint.  But there's still the issue that the backup name node doesn't know the block locations. It needs some time until all data nodes register at the new name node, telling him the block locations.

A further way to remove the bottleneck (too many clients accessing the same name node) is _HDFS Federation_.  We have now multiple name nodes, each name node being responsible for a top level directory.  This can be seen as a form of scaling out / scaling out name nodes.  Each federated name node has then its own secondary name nodes and backup name nodes.

**to-do** (email the assistant) Is it really the client's problem to know which top level directory is associated to which name node? Because effectively we then just have a collection of completely different HDFS -- from the view of the client at last.  Internally, the data nodes can be shared by the name nodes. But can't they do that also in the case of a set of really different HDFS.

*to-do* read more in "HDFS High Availability" in the book


=== Colossus

Newer version of HDFS.


=== Amazone S3

An object storage; Key value model, but _not_ a Key-value store. Proprietary, i.e. we don't really know how it works internally.

There are buckets, and within buckets objects.  An object is a blackbox.

identfying objects: bucket-id (uri: http://<bucket>.s3.amazonaws.com) + object-id (uri: http://<bucket>.s3.amazonaws.com/<object-name>)

_object size limit_: 5TB, _latency_: few 100ms


=== Azure Blob Storage

Hybrid between object storage and distributed file system. Key value model, but _not_ a Key-value store.

identifying objects: Account-id + Partition-id + Object-id

Limit: 195GB blocks, 1TB pages, block size is limited depending on block type

storage stamp = 10-20 racks +
rack = 18 storage nodes +
storage load of stamp kept below 70-80%

Front-Ends / Account Name (DNS delivers virtual IP address) +
Partition Layer / Partition name +
Stream Layer / Object name

Replication within Partition Layer is aysnchornously* inter storage stamp +
Replication within Stream Layer is synchronous within same storage stamp.

*) I.e. we loose consistency (mind CAP theorem: triangle consistency - availability - partition tollerance, we only can have 2, but not 3), but gain availability. If we wanted consistency, then a put call would be synchronous, i.e. the caller had to wait until we replicated the new object everywhere.

Azure Storage offers three types of blobs. _Block blobs_ store text and binary data, up to about 4.7 TB. Block blobs are made up of blocks of data that can be managed individually. _Append blobs_ are made up of blocks like block blobs, but are optimized for append operations. Append blobs are ideal for scenarios such as logging data from virtual machines. _Page blobs_ store random access files up to 8 TB in size. Page blobs store the VHD files that back VMs.

All storage services are accessible via REST APIs.


=== Amazone DynamoDB

Key value model. Key-value store: state is stored as binary objects (aka blobs), identified by unique keys. ACID is _not_ offered. Offers availability and partition tolerance, giving up consistency (but at least offers eventual consistency).  No isolation guarantees are given.  Efficiency, i.e. meeting stringent SLAs (measured at 99.9% percentile of requests so all customers benefit, guaranteeing few hundred ms latency), is an important requirement. It is assumed that operation is in a non-hostile environment.  Availability is for writes (writes are never rejected), which means that reads are more complex (as always, one has to decide when to resolve update conflicts, at reads or at writes).  Replication is asynchronous, which gives better availability.  Hierarchical namespaces are not directly supported.  Relational schema is not supported.

*to-do* replication is async, which means more risk of completely using all replicas (only in total), right? E.g. when the node dies between acknowledging the write and being able to send out replicas.

Dynamo can be characterized as a zero-hop dynamic hash table. The rational for avoiding many hops is that would increase the variance of the latency, endangering the SLA requirements.

Dynamo treats both object and key as opaque array of bytes.  It applies an MD5 hash on the key to generate a 128-bit ID, which is used to determine the storage nodes that are responsible for serving the key.

Simple API. context is opaque to the caller.

get(key) -> value, context +
put(key, context, value)

Design principles:

- priorize scalability and availability

- incremental stability: i.e. you can easily add/remove nodes

- symmetry: all nodes have the same responsibilities/task and do it the same way

- decentralization: symmetry taken further: there is no master-slave. Note that symmetry allone would allow that: e.g. all node start alike, but then they vote one node to be the new master.

- heterogeneity: the hardware of the nodes might differ (so we e.g. can add nodes with higher performance without having to upgrade all other nodes)

A _preference list_ stores the physical nodes responsible for storing a particular key.

*to-do* Were is/are the preference list(s) stored? Please walk me through 1) a put example 2) a coordinator dies example

*to-do* How many entries are in the preference list? The text often meantions ``... the first N entries ...'', implying that the preference list is longer than N entries.

*to-do* is the put really only successfull _after_ W-1 nodes successfully wrote a replica? Doesn't then latency go down the toillet (also considering that some nodes will be in different data centers)? On the other hand, if only writting to the coordinator node was good enough, then durability would go down the toilet, because imediately after the coordinator's local write and return of put, the coordinator could die, right?

*to-do* is it correct that if M > 1 multiple virtual nodes of a physical node fall within a stretch of N consecutive virtual nodes on the ring, we kind of wasted M-1 virtual nodes since we never replicate within a physical node. It's only kind of since `N consecutive nodes' is a `sliding window', and only for a few positions of this sliding window all M virtual nodes fall within it.

_latency_ few 1ms, _object size_ ?? (smaller than S3)

References:

- http://pages.cs.wisc.edu/~thanhdo/qual-notes/ds/ds9-dynamo.txt

- http://docs.basho.com/riak/kv/2.2.3/learn/dynamo/

- Amazon's Highly Available Key-value Store


== References


- UC Berkeley, CS 186 Introduction to Database Systems, Spring 2015: https://www.youtube.com/playlist?list=PLhMnuBfGeCDPtyC9kUf_hG_QwjYzZ0Am1
