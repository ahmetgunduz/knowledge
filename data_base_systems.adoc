// The markup language of this document is AsciiDoc
:encoding: UTF-8
:toc:
:toclevels: 4


== Intro

Data technology stack:

[cols="1,3"]
|=====
| user interfaces |
      Excel, Access, Tableau, Qlikeview, BI tools
| querying |
      SQL, XQuery, MDX, SPARQL, REST APIs
| data stores |
      RDBMS, MongoDB, CouchBase, ElasticSearch, Hive, HBase, MarkLogic, Cassandra
| indexing |
      Key-value stores, hash indices, b-trees, geographical indicies, spatial indicies
| processing |
      two-hase processing: mapreduce / dag-driven proc: tez, spark / elastic computing: EC2
| validation |
      XML schema, JSON schema, Relational schemas, XBRL taxonomies
| data models |
      Tables: Relational model, column store, wide column store / trees: XML Infoset, XDM / graphs: RDF / Cubes: OLAP / key-value model
| syntax |
      text, CSV, XML, JSON, RDF/XML, Turtle, XBRL
| encoding |
      ASCII, ISO-8859-1, UTF-8, BSON
| storage |
      local FS, NFS, GFS, HDFS, S3, Azure blob storage, DynamoDB
|=====

The history of storage: progress made 1956-2010: capacity: 150'000'000 times more, throughput 10'000'000 times more, latency 8 times more. To increase throughput, we can parallize. To improve latency, we can do batch processing.

How can we get more work done:

1) Make SW efficient. ``You can have a second computer once you've shown you know how to use the first one'' (Paul Barham). We can gain factors of speed, and we have to pay once the development costs, and can apply it to all machines we ever will have.

2) _horizontal scaling_ (or _scale out_): Add more nodes, typically commodity HW. Price grows about linearly with overall computing power.

3) _vertical scaling_ (or _scale up_): Replace a node with a more powerfull node. Either by completely replacing, or by adding more RAM and/or CPUs. Price grows about exponentially with overall computing power.

_Design principles_ of big data (by Fourny Ghislain Gilles)

- Learn from the past

- Simplicity

- Modularize the architecture / make good abstractions

- Homogeneity in the large (e.g. blocks in HDFS, regions in HBASE, virtual nodes in chords). So at the large, things are easy to handle.

- Heterogeneity in the small (a.g. add columns in HBase). Heterogenity gives flexibility to the client.  But the cost of increased complexity does not affect the system in the whole.

- Separate metadata from data

- Abstract logical model from its physical implementation

- Shard the data

- Replicate the data

A database _transaction_, by definition, must be _ACID_: All the following must be guaranteed even in the event of errors, power failures etc. _Atomicity_ (each transaction succeeds completely or fails completely), _Consistency_ (each transaction brings DB from one valid state to another valid state, maintaining DB's invariants), _Isolation_ (result is as if transactions were executed in strict sequence), _Durability_ (once a transactin has been committed, will remain committed).

_Consistency models_: _Strict consistency_: Changes are atomic and appear to take effect instantaneously. _Sequential consistency_: Every client sees all changes in the same order they were applied. _Causial consistency_: All changes that are causally related are observed in the same order by all clients.  _Eventual consistency_: If no updates are made, then eventually all accesses will return the last updated value. However in practice there's a continous stream of updates, so consistency will never happen.  In other words: Every update will eventually be propagated. _Weak consistency_: Clients may see updates out of order, or may not see an update at all.

_Availability_: Measure of the percantage of time the service / equipment is in an operable state. A common measure is "99.99%" (with x many nines).

_Reliability_: Measure of how long the service / equipment performs its intended function. Usually measured by _mean time between failure_ (_MTBF_) which is defined as total time in service / number of failures, or by _failure rate_, which is defined as the inverse of MTBF.

_Durability_: A common measure is "loss of 1 in x objects".

_Response time_: One possible measure is "<10ms 99.9% of cases"

_3Vs of big data_: volume, variety and velocity *to-do*

_Load balancing_: *to-do*, Partition schemes

_batch processing_: *to-do*

_data independence_: *to-do*

_shard_: *to-do*

_Replication_: Rational: Fault tolerance. Local: node failure. With a lot of nodes, you are almost guaranteed that a node will fail. Regional: natural catastrophe. Thus spreading datacenters gives proximity to client (gives smaller latency) and protects against regional failures.

_Storage classes_: High availability at high costs on one end and low availability (hours to access data) at low cost on the other end. The low end is typically for backups.

Random notes:

- Random access to pages is generally expensive, or the other way round, sequencial access is much faster
 * binary search is a bad option

- Dealing with (multi)sets, i.e. unordered collections, as most SQL queries do, has the advantage that it is more parallelizable as when it had to be ordered.

- Typical disk block sizes are 0.5kB to 4kB. Virtual memory page size is typically 4kB. Typicall a DB does I/O in 64kB blocks.

- _data center_: ~1k - 100k machines, 1-100 cores / server, 1-12TB local storage / server, 16GB - 4TB RAM / server. 1GB/s network bandwith for a server. A rack consists of nodes.


=== CAP Thoerem

The _CAP theorem_ is about the following impossibility triangle (you can have at most 2 of 3): you only can have two, but never three.

- _consistency_: every read receives most recent write or an error; if not consistent, we have to deal with conflicts somehow

- _availability_:  every request (except under network partition) receives a non-error response (conversly, not having A at all means always getting an error) with low latency (low being subjective, making availability subjective).

- _partition tolerance_: system continuous to operate despite an abitrary number of messages being dropped/delayed by network between nodes.

*to-do* point out that consistency and availability in the context of the CAP theorem mean different things than the same terms in the context of ACID.

*to-do* If I have AC, what does P matter? The A guarantees that I _always_ (independent of network failures) get a response, no?

CP examples: HBase, MongoDB, Redis, MemcacheDB, Big-table like systems

CA examples: Traditional relational data bases (PostgreSQL, MySQL, etc.)

AP examples: Dynamo-like systems, Voldemort, Riak, Cassandra, CouchDB

A: always a error response

CA: always (except network partition case) non-error repsonse, read always returns most recent write. E.g. one maschine is web server which handles client requests, behind is a node having a traditional DB server providing ACID. As long as there's no network partition, we have consistency and availability. If we have network partition (link between server and DB goes down), then the client's requests are answered by errors (CA says that we don't have the P).

AP: always non-error response, even in case of network partition, but maybe a read doesnt return most recent write.

CP: Something like DynamoDB, where the coordinator writes synchronously to the replicator nodes. During the write, which might take a long time because we might have to wait until the network partition is over, the coordinator can't serve further request, thus availability goes away.

Doing updates (i.e. propagation to other nodes) asynchronous gives you availability, because you still can update. If you are synchronous, you can be consistent, but you are no longer available.

*to-do* `consistency' in CAP and in ACID are not the same? In a distributed data base, where each node replicates the full data base, does consistency refer to a single data base or to the global database?


=== REST

_REST_ API (Representational State Transfer): REST is the way HTTP should be used. It's always a method (GET, PUT, DELETE, POST, ...) plus a resource (URI). PUT must be idempotent (when issued multiple times, the 2nd plus requests have no effect). GET must be side-effect free.  POST is the most generic, it can have side effects.


== Relational model

A _data model_ is a collection of high-level data description constructs that hide many low-level storage details. Most DBMS today are based on the _relational data model_, in which there's a single way to represent data: A _relation_ (or _table_) represents data as a two-dimensional table. The _schema_ of a relation relation describes the relation by specifyinig its name and the name and _domain_ (aka _type_) of its _fields_ (aka _attribute_ or _column_). Think of a relation as a type; concrete instances thereof are called, well _(relation) instances_. An relation instance is a set (not list) of _(data) records_ (or _row_ or _tuple_).  A record has one _component_ for each attribute the relation. _Integrity constraints_ are conditions that each record must satisfy.  A _block_ (or _page_) is the unit of transfer for disk I/O.

Levels of abstraction:

- Views describe how users see the data
- Conceptual schema defines logical structure
- Physical schema describes the files and indexes used

--------------------------------------------------
                 Query Optimization
                 and Execution
                       |
                       V
                 Relational Operators
                       |
                       V
            +--> Files and Access Methods <--+
            |          |                     |
            |          V                     |
Concurrency-+--> Buffer Manager           <--+- Recovery
Controll    |          |                     |  Manager
            |          V                     |
            +--> Disk Space Manager       <--+
--------------------------------------------------

Notation:

- +[T]+: The number of pages needed to store all records of table T.
- +p~T~+: The number of records of table T fitting into a single page.
- +|T|+: Cardinality: the number of records in table T.

_Query optimzer_ translates SQL to _Query Plans_ , an internal language. The
_query executor_ is an interpreter for query plans. Think of query plans and
(dataflow) directed graphs, where nodes are relational operators and directed
edges represent data tuples (columns as specified).

Relational operators may be implemented using the iterator design pattern.

When measuring costs, often asymptotic notations in terms of number of I/O accesses are used, since I/O is much more expensive than CPU, even with flash. Sometimes, as improvement, a distinction is made between random access and sequential access, since also their costs differ substantially.


=== Integrity Constraints

Part of the DDL (data definition language).

A _superkey_ for a relation is a set of columns such that no two distinct tuples can have same values in all these columns. In other words, a superkey is a set of attributes within a table whose values can be used to uniquely identify a tuple.  A _(candidate) key_ (or _unique key_) for a relation is a minimal superkey, i.e. no column can be removed from the superkey such that the new column set is still a superkey.  The attributes / columns constituting the candidate key are called _prime attributes_.   Attributes that doe not occur in _any_ candidate key are called _non-prime attributes_.  A table can have multiple candiate keys, one of which can be choosen to be the _primary key_, all others are then _alternate keys_.  A _foreign key_ is a set of columns in one relation that uniquely identifies a tuple of another, possibly the same, table.  The relation containing the foreign key is called the _child relation_, the relation containing the respective candidate key is called the _parent table_ (or _referenced table_).

primary key vs unique key: It seems that technically the only difference is that a table can have at most one primary key, but zero or more unique keys. Further differences are among typicall defaults associated with these constraints, and the semantic meaning. Primary key is meant to identify a row, unique key is meant to ensure a constraint. Most DBMS will by default create a clustered index for primary key and an unclustered index for each unique key, and by default primary key has a non-null constrained while unique key doesn't. At least in Oracle, when all columns of a key are null, and there is no not-null constraint, then the key constraint is satisfied.

_Domain constraint_: Kind of a type, but with additional conditions attached. (Chapter 5.7.2).

_Primary key constraint_: Key must be unique within table

_Foreign key constraint_ (aka _referential integrity constraint_): A key that establishes a relationship between its table or view and a primary key or unique key, called the _referenced key_, of onther table or view. The table or view containing the foreign key is called the _child_ object, the table or view containing the referenced key is called the _parent_ object. Child and parent can be the same table or view.

_General contstraint_: View CHECK constraint on a table or an ASSERTION which is global / not associated with any table.

Note that being able to write down constraints in the DDL helps to remove redundancy. If we coudn't do that, these constraints would appear at multiple places / multiple programs working with the DB.


=== Normal forms

_Anomalies_ are problems, e.g. problems arising from having redundancy, which in turn arises when to many fields are cramed into a single relation such that it contains many tuples which are nearly identical. The typical way of solving the problem is to _decompose_ such an ill-designed relation into multiple relations.

A _normal form_ is a property of a relation with the intention of avoiding anomalies. A relation is in _1st normal form_ iff the domain of each attribute is an atomic type.  A relation is in _2nd normal form_ iff additionally all functional dependencies are on the whole candidate key, for all candidate keys. A relation is in _3rd normal form_ iff additionally every non-prime attribute is non-transitively dependent on every key of R. Bill Kent: "[Every] non-key [attribute] must provide a fact about the key, the whole key, and nothing but the key.". Requiring existence of "the key" ensures that the table is in 1NF; requiring that non-key attributes be dependent on "the whole key" ensures 2NF; further requiring that non-key attributes be dependent on "nothing but the key" ensures 3NF.


=== Relational algebra

_Relational algebra_ (aka just _algebgra_): Operational (thus procedural), i.e. we can build arbitrary expressions on the basis of operators, each taking one or more operands. The domain and image of each operator are relations. Relations have set semantics (in contrast to multiset), i.e. no relation can have duplicate rows (SQL has multiset semantics, i.e. tables can have duplicate rows. I.e. in pure relational algebra often there's a `remove duplicates' sub step. However in practice that is rather expensive since it involves sorting or hashing). Relation algebra is typically not directly used, but via SQL, which uses it internally.

Useful for representing execution plan semantics. Close to query plans.

_Relational calculus_ (aka just _calculus_): A declarative language -- Describe what you want, rather than how to calculate it. A variant is the _tuple relational calculus_ (aka _TRC_), which heavily influenced SQL.

Exprecity of relational algebra and relational calculus is equivalent.


==== Basic operators

There are only five operators: selection, projection, and 3 set operators: set difference, set union, crossproduct. There are convenience operators being based on these basic operators.

_Selection_ (or _Restriction_) (filter query): σ~_condition_~(_relation_) (s as in sigma/select): Keep matching tuples, cut away the rest.  The (selection) condition is a boolean expression, where primaries are literals and fields of the given relation. The output are the tuples of the input instance which satisfy the condition. The output has the same schema as the input.

_Projection_ (filter query): π~_fieldlist_~(_relation_) (p as in pi/project): Keep given columns, cut away the rest.  Returns new relation, having only the given fields of the input relation. Has to remove duplicates.

_(set) union_ (set query): A ∪ B (row-wise): Row-wise concatenate relations.  A and B must be _union compatible_ (sequence of field domains must be equal). Has to remove duplicates.

_(set) difference_ (set query): A - B (row-wise). Cut away rows which appear in B. A and B must be union compatible. Note that unlike the other basic operators, it cannot be implemented with an online algorithm, because each next tuple from B can remove a tuple from the tentative output.

_(set) intersection_ (set query): A ∩ B. Keep only rows appearing in both.  Defined as A-(A-B). A and B must be union compatible.

_crossproduct_ (aka _cartesian product_) (binary query): A ⨯ B. The output relation instance has each tuple of A, each of which followed by each tuple of B.  The output relation's schema is the concatenation of A's schema plus B's schema. By convention field names are overtaken; in case of name conflicts, corresponding fields are unnamed and must be referred to by position.


==== Some important compound operators

_(conditional) join_ (binary query): A ⨝~condition~ B: Defined as σ~_condition_~(A ⨯ B).

_equi join_ (binary query): A conditional join where the condition solely consists of one or more equalities, combinded by logical and. They can be implemented efficiently; In effect, there is only one equiality, where the rhs and lhs are the concatenation of the individual original lhs/rhs. E.g. (r1.f1=r2.f1 and r1.f2=r2.f2) is equivalent to (concat(r1.f1,r1.f2)=concat(r2.f1,r2.f2)).

_natural join_ (binary query): A ⨝ B: Condition demands equivality (A.fieldx=B.fieldx) for all fields having the same name. I.e. it's an implicit equi join. However, in contrast, also a projection follows which cuts away the duplicate fields. If there are no common field names, the result is the crossproduct.

_Inner joins_ don’t include non-matching rows; whereas, outer joins do include them. _Left outer join_ always has at least one tuple for each tuple of the lhs input relation, and if there are no tuples of the rhs relation matching the condition, fills the components with NULLs. _Right outer join_ is analogous. _Full outer join_

_division_: A / B: Defined as π~x~(A)-π~x~((π~x~(A)⨯B)-A). More informally: Say A tells which supplier supplies which part, and B lists parts. A/B deliviers suppliers which supply all the parts in B.


==== Extended operators

_duplicate-elimination_ δ (d as in duplicate/delta): Eliminates duplicate rows, i.e. turns a multiset into a proper set.

_aggregation_: Apply some operation (e.g. sum, average) to all components of a column.

_grouping_ γ (g as in grouping/gamma): Put tuples matching a condition in the same group, and then perform some aggregation to columns within each group.

_extended projection_: In addition to projecting out some columns, we now can produce new columns.

_sorting_ τ: Turn a relation instance into a list of tuples. Note that not all relational operators accept lists as arguments.

_outerjoin_: *to-do*


=== SQL

See sql.txt


== Local implementation of relational model


=== Implementation of relational operators

==== select

FP: number of pages in file. As always, time analysis is in terms of page I/Os, not considering writing the result.

OMP: in case of ordered input, number of pages containing the matching tuples

MT: number of matching tuples

no index on column, unsorted data:: Scan all tuples. O(FP)

no index on column, sorted data:: Binary search to find first matching tuple, then sequential scan as long as tuples match. O(log FP + OMP)

B+ tree index on column:: Walk B+ tree to find first matching tuple, then scan as long as tuples match. O(log~fanout~

==== join

_Theta join_: Given sets R and S, the theta join R ⨝~Θ~ S delivers all pairs {r,s} where the predicate Θ(r,s) is true, r and s being members of the set R and S respectively. In an _equi-join_ Θ is an equality test; it can be optimed. As a special case of that, even more optimizeable, is when one operand is a key.


===== simple nested loop join algorithm

--------------------------------------------------
foreach record r in R:
  foreach record s in S:
    if theta(r,s): result.add({r,s})
--------------------------------------------------

page I/O cost, assuming arbitrary large [T] and [R], ignoring writing result: |R|*[S]+[R], i.e. _very_ bad.

===== chunk (oriented) nested loop join algorithm


Improvement: Make number of iterations in outer loop as small as possible, so we have to go pages of S as few times as possible. So outer loop reads from R in `chunks', one chunk being B-2 pages large. It's -2 because we need one page for the input streaming buffer for S, and one page for the output streaming buffer of the result.

--------------------------------------------------
foreach chunk in R:
  read in chunk from R
  for each record r in current Rchunk:
    foreach record s in S:
      if theta(r,s): result.add({r,s})
--------------------------------------------------

page I/O cost: [R]/(B-2)*[S]\+[R], becomming [S]+[R] if outer table, i.e. the Rchunk, fits completely into memory, i.e. if [R]<=B-2.


===== indexed nested loop join

For the special case of equi-joins.

--------------------------------------------------
foreach record r in R:
  foreach record s in R where r==s:
    result.add({r, s})
--------------------------------------------------

page I/O cost: [R]+|R|*costOfFindingAKey


===== sort-merge join

For the special case of equi-joins, here R.r_attrib=S.s_attrib

------------------------------------------------------------
sort R on r_attrib -> sortedR
sort S on s_attrib -> sortedS
scan sortedR and sortedS in tandem to find matches
------------------------------------------------------------

page I/O cost: cost(sort R) + cost(sort S) + [R]+[S].

As an optimization, the sorts, each having internally a set of sorted chunks, ommit writing an output. Instead, the `scan sortedR and sortedS in tandem' step operatoes on all these chunks; each chunk is connected to an input buffer. Thus instead of the normal B-1 chunks a sort creates, now it can only create (B-1)/2 chunks. So we saved 2*([R]+[S]), since we saved writing/reading the sortedR and sortedS.

Naturally a good variant if R and S need to be sorted on r_attrib and s_attrib respectively anyway in the query plan.


===== hash join

For the special case of equi-joins, here R.r_attrib=S.s_attrib

----------------------------------------------------------------------
using coarse hash function, partitionate R,
  restriction: no partition might be larger than B-2 pages,
                  so it might be as usual a recursive process
using coarse hash function, partitionate S, partitions can be of any size
for each partition pr of R
  read in partition pr, building an inmemory hashtable (using upto B-2 pages of memory)
  for each record s in partition of S being associated to pr: (nomal streaming using one input buffer)
    if hash table contains key s.s_attrib:
      result.add({r, s}) (normal streaming using one output buffer)
----------------------------------------------------------------------

Often R is called the building table, and S the probing table.

Note that the probing table's partitions can have an arbitrary size (in pages), since they are streamed. Thus you want to make the smaller table the building table, and the larger table the probing table.


=== Files and Access Methods

A _(DB) file_ is a collection of pages. A _page_ is a collection of records. Each _record_ has an _(physical) record id_ (rid), which is a pair (page_id, slot_id). Records can be fixed width or variable width. The file API supports insert/delete/modify/find(via recordid) a record, scan all records.

_System catalogs_ store properties of each table, index, view and other stuff such as statistics, authorization etc.

A DB file is typically implemented as one or more OS files, or as raw disk space, e.g. in POSIX directly a device. Note that a DB file might spawn multiple disks.

[[index]]
==== Index

An _index_ (aka _access path_) is a disk based data structure that organizes data records of a given table, or references to them, on disk to optimize certain kinds of retrieval operations. A table can have multiple indexes on it. A _search key_ is over any subset of columns of that table. In contrast to the key of the table, multiple records can match a search key. An index is implemented as a collection of _data entries_. A data entry with search key value k, denoted as k*, contains enough information to locate the matching records. There are three main alternatives of how to store a data entry: Alternative 1) (k,record). I.e. the index directly stores the records of a table. To avoid redundancy, this alternative is used at most once per table. Alternative 2) (k, rid). Alternative. 3) (k, rid-list). Alternative 2 and 3 obviously introduce a level of indirection. A _clustered index_ is one where the ordering of data records defined by its data entries is roughly the same as the ordering of the data records of the file of the underlying table. By definition alternative (1) is clustered. For alternatives (2) and (3), the file must be roughly (see <<clustered_file>>) or strictly sorted (see <<sorted_file>>). Regarding range search queries, clustered indexes are in general much faster than unclustered, due to the usual contigous access advantages and since more of read in page is actually used, i.e. less pages have to be read. The costs for a clustered index is maintainenance cost to (roughly) maintain the ordering of the data records. Often that means that the pages containing data records are not fully packed (2/3 is a common figure) to accomodate future inserts, which degrates performance since more pages nead to be read/written for a given amount of records.

Common kinds of selections (aka lookups) that indexes support:

- key operator constant, and specifically equality selections, where the operator is =.
- Range selections, where op is a relational operator <, >, ....
- N-dimensional ranges: e.g. points within a given rectangle.
- N-dimensional radii: e.g. points within a given sphere.
- Regular expressions

[[bplus_tree]]
==== B+ tree

_B+ tree_ is an high-balanced n-ary tree. It's the most widely used data structure to implement an index. They have fast lookups and fast range querries. Is typically the most optimized part of an DBMS.

Each node is stored in a page. Unlike with a B tree, internal nodes only
contain pointers to further nodes, never data; only leaf nodes contain data or
pointers to data. Also leaf nodes form a linked list. Together this allows for
more efficient scans over a range of data.

Regarding high-balancedness: Each node contains m entries with the soft restriction d<=m<=2d, i.e. it's always at least 50% full, where d is called the _order_ of the tree. The high balanced property guarantees O(log N) access time, i.e. guarantees that even after insertions/deletions performance can't degenerate to linear time. Then again, since keys can be of variable width (e.g. strings), and the data entries in the leaf nodes can be variable width (e.g. see alternative 3 in <<index>>), in practice this is seen sloppy. sometimes a physical criterion is used (`at least half full' in terms of bytes).

Key compression increases fanout, which reduces height, which reduces access time.

Algorithm to _insert_ into an already full node: split node, which obviously includes allocating a new node, and which makes space for new item. Introducing a new node obviously also means that we need to insert a new item into the parent node which points to the new node. Now this can be a recursive process, where in the worst case it ripples up all the way up and we have to split the root. If data entries are directly data records (see alternative 1 in <<index>>, advantages see there), splits can change record ids, which means having to update referees, which is considerable disadvantage.

Similarly for _deletion_. We should maintain the d<=m<=2d invariant. However in practice m<d is allowed, since in practice it's a rare case that given a big table there are so many deletions which would shrink it to a small table. Note that all leafs have the same depth, and there are no rotations upon insertion/deletion has with other kinds of balanced trees.

Creation of a B+ tree given a collection of keys should no be done via individual inserts, since the resulting page access pattern is very random and thus slow. Instead, we do _bulk loading_: Sort the index's data entries. Then iteratively soak them up and create leaf nodes. A fill-factor parameter determines how full the leaves shall be. Create/update parent nodes as in the insertion algorithm. Looking at the usual tree drawing, we see that always the right-most internal nodes are touched whereas the other nodes aren't at all, an access pattern which works very well together with an LRU page buffer.


=== Buffer management

A cache storing in memory a collection of pages from the disk space management below. Consists of a collection of frames, a frame having the same size as a page. Allocated at startup time.

Each frame has associated: pageid/NIL, pin_count (aka reference_count), dirty_flag.

A request for a page increments pin count. A requestor must eventually unpin it and indicate whether page was modified (-> dirty flag).

pin_count==0 means unpinned means `free to be exchanged by another page from disk'. When pin_count goes to 0, that is the event of `page is now no longer used'.

There different replacement policies for replacing a frame: least-recently-used (LRU), most-recently-used (MRU), clock, ....

As an optimization, pre-fetch is often employed.

Buffer leak: when a page request can't comply because all pages in buffer are pinned. That is considered a bug in the DB; pages should only be pinned for a very brief time.


=== Disk space management

Disk space manager provides about this API: allocate/free a page, read/write a page. Higher levels expect that sequencial access to pages has an especially good performance.


== Data Stores


=== HBase

HBase is the open source version of Google's Bigtable. Based on the wide column store model.

Each table has a row ID column being by definition the primary key. Columns are grouped in a column families.  The idea is to group together whats frequently accessed together.  The column families must be known in advance, but not the columns.  The number of columns can be very high (compared to relational DB).

Rows have an order.

Operations: put/get/delete (row), scan (rows)

Has low latency because of the memstore; latency due to access to underlying DFS falls aften away

*to-do* But isn't there still the network latency to the region server? From a high level perspective, clien-regionsserver is the same as client-datanode, so the two should have about the same latency, no?


==== Implementation of HBase

Partition table firsy horizontally (i.e. group rows), then vertically (as already done by column families).  We need horizontal partitioning because we can have billions of rows not even fitting on a single machine.  A horizontal partition is given by the range (min-incl, max-excl).  Such a range of rows is called a _region_.  Obviously the max-excl equals the min-incl of the next partition.  The intersection of horizontal and vertical partitioning is what is stored together and is called a _store_.

Master slave architecture.  The master is called _HMaster_, a slave is called _region server_.  The HMaster's responsibility is the meta data.  A store is stored as one or more files, called _store file_ (or _HFiles_), on a DFS.

One store file is actually an _SSTable_, a flat sorted list of key-value pairs, one pair also called  _KeyValue_, plus an index for faster key lookukp.  The index is loaded into memory.  The key is logically a (rowid, column-number) pair refering to a cell of the original table, and the value storing the content of that cell.  KeyValues are stored sequentially, forming a bytestream, making it efficient for transfer.  Each KeyValue is stored as tuple (keylength, valuelength, key, value).  The length of the keylength and valuelength elements are fixed width, e.g. 32bit.  Practically the key is a tuple

(rowidlength, rowid, columnfamiliylength, columnfamily, columnqualifier, timestamp, keytype)

Again rowidlength, columnfamiliylength are fixed width, and their value defines the length of the respective tuple element.  Timestamp and keytype are fixed width.  So columnqualifier length can be computed, taking the outer keylength into account.  Technically, the columnfamily is not required, since we already know in which column family we are.  The timestamp is the version.  The keytype is actually a deletion mark.

The key-value pairs of the store file are read in blocks of about 64kB; no pair is ever split.  Note that these are not the same blocks as the ones the underlying DFS might have.

_put_: First write to _HLog_ file (or _write ahead log_), a journal, then to the _memstore_.  The HLog file is a security measure in case we loose what's in the memory before the memory could be flushed to disk.  It is stored on the underlying DFS.  When certain criterions are met, the memstore is flushed to disk, creating a new storefile (as always with sorted rows).  After flushing, the log file can be discarded.  Thus we keep generetaing partially redundant store files (but remember that each KeyValue as an version, and we have a total order).  Every now and then, we do _compaction_:  Replace the existing HFiles by one new HFile by merging them.

*to-do* Is an existing KeyValue updated, or a new one created with an increased version number?

_delete_: Similar to put, where the modification is to check the `is deleted' flag.

*to-do* but isnt the HLog file HUGE very quickly, since KeyValue can be big? And followingly, isnt synchronously appending to the HLog expensive, both in latency and throughput, relative to the other costs of an HBase put?

*to-do* compaction: Isn't it easily possible that the sum of generated store files don't fit into memory? Maybe the answer is online merge sort?

Guarantees ACID on the row level via per-row locks. That gives us total order of row versions.

Overview:

Table +
Region +
Store  +
StoreFile(n) + Memstore(1) + HLog(1) +
Block | - +
KeyValue | KeyValue

*to-do* How to ensure that a store completely fits into memory? After all compaction works by reading all store files in memory, sort, and flush.


==== Optimizations

Besides the memstore, there's also an in-memory _cache_ of KeyValue s.  A unit of the cache is a block.  The MemStore is for KeyValue s not yet flushed to disk, the cache is for  faster access to already persisted KeyValue s.  The cache is composed of two hierarchy levels, the _LRU BlockCache_ and the _bucket cache_.  LRU BlockCache caches the last recently used blocks.

_short circuiting_ / _colocation_ (process data where it is stored):  Is when the requested block of the underlying DFS is stored on the same physical node as the region server requesting that block runs on.  Thus effectively the region server reads the block from its own local drive, without paying network overhead.  This is a situation that occures most of the time as a result of the design of HDFS and HBase, in particular from the <<hdfs_replica_placement>> strategy of prefering to store a block on the client itself.  One could think that due to HDFS having a life, over time the HDFS data node (runing on the same physical node as the HBase region server) will no longer itself store the HDFS block.  But due to the compaction of HFiles and the HDFS replica placement strategy, we will restore colocation over time.

An in-memory _bloom filter_ is used to reduce access to HFiles when searching keys.

LMS-tree *to-do* The LSM-tree structure's purpose is to minimize the number of required compactions.


=== Cassandra

Similar to the one of HBase.


=== Google Bigtable

*to-do*


== Data models


_Key-value model_:  A data model. Some mapping from a key to a value.

_Column store_ (or _column-oriented DB_): A data model. Store data by columns (as opposed to by rows). One advantage is that subsequent cells in the same row tend to be similar, thus compression algorithms tend to work well.

_Wide column store_: A data model. Store data by rows, keys identify rows, group columns in families. E.g. Google's BigTable, HBase, cassandra.  In the tabular model, joins are very expensive.  In the tabular model we love to have data in normal form, and as a consequence there are many joins.  Paradigm of BigTable: store together what is accessed together (i.e. quite the opposite of normal forms). That makes batch processing better, since we only have to pay latency once (recall we want to avoid latency as much as possible), and after that it's just throughput. To fulfill the paradigm, we denormalize. That can also be seen as precomputing the joins we expect to occur often.  Thus reads become faster.  The price is that we introduced anomalies, so writes are now more expensive.


== Storages

=== Key-value store

Same data model as object storage, but implemented differently. Intend to have low latency. Smaller objects (kB sized). No metadata. Note the key-value store is not the same as key-value model.

*to-do* What's the exact difference between key-value store and key-value model.

Much simpler than a relational database. We drop consistency (we only have eventual consistency) and gain availability and partition tolerance and scalability.

Simple things are much easier to scale out than monolithic things (such as a table in the relational model).

In contrast to object storage, no metadata.

*to-do* examples: dynamodb


[[block_storage]]
=== Block storage

*to-do* Same as file storage?

Object is divided into blocks.  Large amount of huge files: millions of PB files.  I.e. limited in number files.  An object (aka file) is a sequence of blocks (or chunks).

Block size on a local file system is \~4kB; in a relational database \~32kB. In a distributed file system such like HDFS it's ~128MB -- good compromise between latency and throughput.  Too small blocks would mean too many blocks to wait for, and since its over the network latency would be bad (relative to the time it takes to transmitt the complete block). Too big means we can't even put it on a single machine.  Also if the number of blocks of a file is smaller than the number of tasks of a mapreduce, we can't parallelize as much.

*to-do* examples: gfs, hdfs


=== Object storage / document database

huge amount of large files: billions of TB files.  I.e. limited in file size.  As a consequence, a file fits on a single machine. An object is a black box.

Object storage lets you scale. Make model of local filesystem simpler. 1) throw away hierachy (file system tree). 2) Metadata is no longer fixed but flexible: assign values to keys. 3) Flat and global key-value model (associate IDs to files). 4) use commodity HW.

on scalability issues with a local drive: A data base on a local machine might work for that machine.  Maybe, if you're lucky, it even works when accessed by multiple people on a (small) LAN.  But it doesn't work on a WAN.  The disk just can't cope with the amount of requests.  Also, on a typical file system you can't have billions of files.

latency is low relative to a database: s3 ~ few 100ms, typical database 1-9ms, both where client is in same region.


*to-do* examples: s3


=== Chord / distributed hash table

A protocol for a peer-to-peer distributed hash table. Used by DynamoDB.

Assigning keys to nodes:  Say the key size is 128bit. Imagine the 128bit numbers on a ring.  Each node uniformily at random chooses a 128bit number.  Then each node stores the keys between itself and the previous N ≥ 1 nodes. If N > 1, we get replication.  Note that this assignment of keys to nodes is very simple and predetermined.  Also note it's only about assigning keys to nodes; there's no relation to how nodes are physically conencted.

Query, i.e. finding a node responsible for key k: The trivial solution would be that the nodes on the ring form a linked list, which would result in linear time query.  Here each node keeps a _finger table_, where the i-th entry stores a `pointer' to the node being 2^i^ nodes away.

Pros:

- highly scalable

  * incremental stability (easy to add/remove nodes)

- robust against failure

- self organizing

Cons:

- being a hashtable there's only lookup by key (e.g. no text search)

- nothing said about data integrity (here replication is about loss, not corruption)

- security issues (you need to have full control over the nodes themselves and the set of existing nodes)

- bad luck when nodes choose randomly their position on the ring and there are large gaps giving big burden on the node at the end of the gap

- not considering that nodes are heterogenous (i.e. have different power)

The last two can be solved by the following extension: Each node gets a number of _tokens_ (or _virtual nodes_), the number proportional to the node's power. Now instead of nodes, we place place the tokens on the ring. Since there are now many tokens, and due to the central limit theorem, it's virtually impossible to have large gaps.  Also, we now adapt to the heterogenous network.  When adding a node, it takes over tokens from existing nodes.  When deleting a node, its tokens are redistributed among remaining nodes.

*to-do* make this `extension' an part of the initial thing

_vector clock_: Each object as associated a set, called _context_, of nodeid-number pairs, where nodeid is unique in the set. The number denotes how many times the given number wrote (put) the object. Multiple contexts for a given object form a partial order (i.e. a DAG).


questions:

- Slides 197+: I don't see how this works in the distributed system with no masters. Where are the preference lists stored? What does partition-aware client mean?

- why not return (C,[(n1,3)]) , (D,[(n1,2), (n2,1)]). Answer: The protocol is such that it's a black box for the client


=== Google File System (GFS)

Requirements:

- Throughput has top priority.

- A capacity of millions of PB files.

- Fault tolerance and robustness (a local disk might fail, in a clustser with 10 tousands nodes, nodes _will_ fail). That means we need monitoring of the disks status, error detection, automatic recovery, so at the top layer we get fault tolerant.

- Latency has secondary priority.

File update model: Only append and upsert, i.e. no random access.  Appending should work for hundreds of clients in parallel.  This is a suitable model e.g. for sensors, logs, intermediate data.

Master slave architecture.


=== Hadoop (HDFS)

Open source distributed file system. Open source version of GFS. MapReduce. Wide column store (HBase). Block storage (by default 128MB blocks (configurable on a file-by-file basis), 64 bit block id, see also <<block_storage>> for pro/cons of block sizes). File hierarchy model.

Designed for:

- Peta byte files. I.e. a single file doesn't fit on a single drive, for that alone we need block storage.  A file is divided into blocks. Each block is replicated among multiple data nodes for fault tolerance.

- Streaming data access patterns: i.e. it's expected that the data accessing pattern is a write-once, read-many-times.  It is expected that a large portion of a file is read, so data throughput is more important than the latency to read the first bytes.

- Scaling out, i.e. using commodity HW.

Disadvantages:

- Can't offer low latency access

- Can't offer lots of small files. This is also because the name nodes hold the filesystem metadata in memory, so the amount of memory of a name node limits the number of files.

- Can't offer multiple writers, and can only append to the end of the file (i.e. can't write to arbitrary positions).

- Not suited for running across data centers (*to-do* why?)

In terms of CAP theorem: We have consistency. But due to the single master, we have neither full availability nor full partition tollerance.

Master slave architecture.  The master is called the name node, the slaves are called data nodes.

The _name node_ (or _primary name node_ or _active name node_) cares about the filesystem meta data: The _file namespace_ (i.e. the file tree), _file to block mapping_ (for each file a list of block ids constituting it), and _block locations_ (for each block id where it is stored).  It keeps all that information in memory.  Later it is described in what ways that information is persisted.

A _data node_ only stores bocks, storing them on its local drives, using a traditional local filesystem.  A data node is identified by an storage id, which does not change if the IP of the data node changes.  A data node stores its storage id.  A data node stores a checksum for each block.  When a client reads/writes blocks from/to a data node, the data sending side always also transmits the checksum, and the receiving side has to verify.

_Client protocol_ (a RPC protocol): Client first makes metadata operation request to name node (master).  Note that a client might be a node within the cluster, e.g. a name node.  For a read/write, as answer it receives the block locations: For each block id, the multiple (see specified number of replicas) node locations (IPs) where the block is stored, sorted by distance, so the client can choose to talk to the closests data node. See data transfer protocol below how the client continuous.

_Data node protocol_ (a RPC protocol):  Between data node and name node, it's always the data node who intitiates the communicuation. E.g. registration ("Hi, I'm a new data node"). Every x seconds a name node sends a hearbeat("I'm still alive"). When the name node wants something from a data node (e.g. a block operation), the name node does so via its response to a heart beat.  When a name node received a block (see write in the data transfer protocol), he acknowledges to the master node with a BlockReceived message.  Every y hours, the data node sends a block report to the name node (the list of block, i.e. their ids, it currently stores).

_Data transfer protocol_ (a streaming protocol):  See client protocol first. See following read and write.

For a _read_, as answer the client receives the block locations: For each block id, the multiple node locations where the block is stored, sorted by distance (see Hadoop's measure of closeness), so the client can choose to talk to the closest data node.  Multiple clients can read in parallel from the same file / blocks.

*to-do* can a client read different blocks in parallel, i.e. block1 from datanode1, block2 from datanode2 etc.

For a _write_, it's analogous.  Recall that writes can only append.  For each new block a client wants to write, it receives a collection of block locations from the name node.  The client doesn't need to care about replication.  Per block, the client talks to the closest name node, tells it also the other name nodes that need to replicate the block, and the name nodes take care of replication themselves by creating a _data pipeline_ which minimizes the distance from the client to the last data node.  The data node receiving the client's write request asynchronously sends an acknowldge to the client once all replicates are successfully written.  Recall from data node protocol that each node receiving a block sends a BlockReceived message to the name node.  For each client-initiated transaction, the change to the filesystem meta data is commited to the client only after the name nodes journal has been flushed to disk.  There is at most one writer to a file at any point in time, ensured by having a lock on each file.  I.e. before the write, the client has to open the file for writing to acquire the lock, and at the end he has to close the file to free the lock.

*to-do* fix: its the client that organizes the pipeline

_Hadoop's measure of closeness / distance_: The network is represented as a tree / hierarchy.  The hierarchy is not fixed, however common is (internet, data center, rack, node).  The distance beween two nodes is the sum of the hierarchy levels between a node and the common anchestor.  Rational: Due Hadoop's design goals and the resulting architecture, throughput is more important than latency, so a possible measure would be bandwith between nodes. That however is difficult to measure. The given metric is an approximation.

*to-do* Is the ack of the write back to the client async to replication? Even prof didnt know.

[[hdfs_replica_placement]]
_Replica placement_ (or _block placement_), i.e. which nodes store a block replica: The first block/replica is stored on the client itself, if the client is a data node in the same cluster, and a `random' (load balancer prefers certain ones) node which is not too busy and not too fully otherwise. The 2nd replica is stored on on a node in a different rack within the same cluster (If it were stored in the same rack, that would mean that the same rack is guaranteed to store two replicas, which is a shame if the rack fails). The 3rd replica is stored on a node in the same rack as the 2nd. The further replicas are stored at `random' nodes, but if possible at most one replica per node (we care about a node failing as a whole, not that only parts of a single drive fail) and at most two replicas per rack.

Replica placement considerations: Reliabilty (how relyable is a node), read/write bandwith (how fast is the network), block distribution (what's the distance from the data node to the client (which might be itself a data node)).

Number of replicas is specified per file. The default is 3.

*to-do* why is the 3rd replica stored in the same rack? We just said that we don't want that the same rack contains two replicas?


==== Dealing with master being bottleneck

The name node is bottle neck and single point of failure. The following describes ways how HDFS tries to mitigate the problem. Note that the use case of unexpected failure of a name node is rare, so in practice the use case of planned downtime for maintenance is more important.

The master uses its local filesystem to persist the file namespace and the file to block mappings in a _checkpoint_ and a _journal_ (or _edit log_, log of edits since last checkpoint).  Thus there are kind of three layers: memory (full), edit log, checkpoint (full). Note that the block locations are not persisted, because the name node gets to know them via the heart beats of the name nodes.  The name node always writes to the journal, as opposed to the checkpoint.  The checkpoint is only modified in explicit situations, such as startup or explicit administrator commands.  When restarting the name node, we need to read the namespace file and the edit log, and apply the changes recorded in the edit log on top of the information in the namespace file. Such a restart would take about 30 minutes, which is obviously too long.

The checkpoint and journal can be configured to be stored on multiple places. Recommended practice is to place each a replica on a local disk (preventing loss from failure of a single disk), and one replica replica on a remote NFS server (preventing loss from node failure).

A _secondary name node_ (or _check point node_) shadows the primary name node and has the sole responsibility to make a checkpoint every once in a while, i.e. combine the primary name node's checkpoint and journal into a new checkpoint, and send back the new checkpoint to the primary name node.  When the primary name nodes replaces its checkpoint with the new checkpoint, it also can truncate its journal. Good practice is to create a daily checkpoint. A smaller journal means faster startup time, and less risk that any part of the journal is corrupted.

A _backup name node_ is like a secondary name node, but additionally has the file system meta data in memory, just as the primary name node.  From the view point of the primary name node, a backup name node is just another journal store.  The backup name node thus recievies a stream of file system meta data transactions.  If the primary name node fails, the backup name node can jump in, without having to reply a journal to a checkpoint.  But there's still the issue that the backup name node doesn't know the block locations. It needs some time until all data nodes register at the new name node, telling him the block locations.

A further way to remove the bottleneck (too many clients accessing the same name node) is _HDFS Federation_.  We have now multiple name nodes, each name node being responsible for a top level directory.  This can be seen as a form of scaling out / scaling out name nodes.  Each federated name node has then its own secondary name nodes and backup name nodes.

*to-do* Is it really the client's problem to know which top level directory is associated to which name node? Because effectively we then just have a collection of completely different HDFS -- from the view of the client at last.  Internally, the data nodes can be shared by the name nodes. But can't they do that also in the case of a set of really different HDFS.

*to-do* read more in "HDFS High Availability" in the book


=== Amazone S3

An object storage; Key value model, but _not_ a Key-value store. Proprietary, i.e. we don't really know how it works internally.

There are buckets, and within buckets objects.  An object is a blackbox.

identfying objects: bucket-id (uri: http://<bucket>.s3.amazonaws.com) + object-id (uri: http://<bucket>.s3.amazonaws.com/<object-name>)

_object size limit_: 5TB, _latency_: few 100ms


=== Azure Blob Storage

Hybrid between object storage and distributed file system. Key value model, but _not_ a Key-value store.

identifying objects: Account-id + Partition-id + Object-id

Limit: 195GB blocks, 1TB pages, block size is limited depending on block type

storage stamp = 10-20 racks +
rack = 18 storage nodes +
storage load of stamp kept below 70-80%

Front-Ends / Account Name (DNS delivers virtual IP address) +
Partition Layer / Partition name +
Stream Layer / Object name

Replication within Partition Layer is aysnchornously* inter storage stamp +
Replication within Stream Layer is synchronous within same storage stamp.

*) I.e. we loose consistency (mind CAP theorem: triangle consistency - availability - partition tollerance, we only can have 2, but not 3), but gain availability. If we wanted consistency, then a put call would be synchronous, i.e. the caller had to wait until we replicated the new object everywhere.

Azure Storage offers three types of blobs. _Block blobs_ store text and binary data, up to about 4.7 TB. Block blobs are made up of blocks of data that can be managed individually. _Append blobs_ are made up of blocks like block blobs, but are optimized for append operations. Append blobs are ideal for scenarios such as logging data from virtual machines. _Page blobs_ store random access files up to 8 TB in size. Page blobs store the VHD files that back VMs.

All storage services are accessible via REST APIs.


=== Amazone DynamoDB

Key value model. Key-value store: state is stored as binary objects (aka blobs), identified by unique keys. ACID is _not_ offered. Offers availability and partition tolerance, giving up consistency (but at least offers eventual consistency).  No isolation guarantees are given.  Efficiency, i.e. meeting stringent SLAs (measured at 99.9% percentile of requests so all customers benefit, guaranteeing few hundred ms latency), is an important requirement. It is assumed that operation is in a non-hostile environment.  Availability is for writes (writes are never rejected), which means that reads are more complex (as always, one has to decide when to resolve update conflicts, at reads or at writes).  Replication is asynchronous, which gives better availability.  Hierarchical namespaces are not directly supported.  Relational schema is not supported.

*to-do* replication is async, which means more risk of completely using all replicas (only in total), right? E.g. when the node dies between acknowledging the write and being able to send out replicas.

Dynamo can be characterized as a zero-hop dynamic hash table. The rational for avoiding many hops is that would increase the variance of the latency, endangering the SLA requirements.

Dynamo treats both object and key as opaque array of bytes.  It applies an MD5 hash on the key to generate a 128-bit ID, which is used to determine the storage nodes that are responsible for serving the key.

Simple API. context is opaque to the caller.

get(key) -> value, context +
put(key, context, value)

Design principles:

- priorize scalability and availability

- incremental stability: i.e. you can easily add/remove nodes

- symmetry: all nodes have the same responsibilities/task and do it the same way

- decentralization: symmetry taken further: there is no master-slave. Note that symmetry allone would allow that: e.g. all node start alike, but then they vote one node to be the new master.

- heterogeneity: the hardware of the nodes might differ (so we e.g. can add nodes with higher performance without having to upgrade all other nodes)

A _preference list_ stores the physical nodes responsible for storing a particular key.

*to-do* Were is/are the preference list(s) stored? Please walk me through 1) a put example 2) a coordinator dies example

*to-do* How many entries are in the preference list? The text often meantions ``... the first N entries ...'', implying that the preference list is longer than N entries.

*to-do* is the put really only successfull _after_ W-1 nodes successfully wrote a replica? Doesn't then latency go down the toillet (also considering that some nodes will be in different data centers)? On the other hand, if only writting to the coordinator node was good enough, then durability would go down the toilet, because imediately after the coordinator's local write and return of put, the coordinator could die, right?

*to-do* is it correct that if M > 1 multiple virtual nodes of a physical node fall within a stretch of N consecutive virtual nodes on the ring, we kind of wasted M-1 virtual nodes since we never replicate within a physical node. It's only kind of since `N consecutive nodes' is a `sliding window', and only for a few positions of this sliding window all M virtual nodes fall within it.

_latency_ few 1ms, _object size_ ?? (smaller than S3)

References:

- http://pages.cs.wisc.edu/~thanhdo/qual-notes/ds/ds9-dynamo.txt

- http://docs.basho.com/riak/kv/2.2.3/learn/dynamo/

- Amazon's Highly Available Key-value Store


== References


- UC Berkeley, CS 186 Introduction to Database Systems, Spring 2015: https://www.youtube.com/playlist?list=PLhMnuBfGeCDPtyC9kUf_hG_QwjYzZ0Am1
