// The markup language of this document is AsciiDoc
:encoding: UTF-8
:toc:
:toclevels: 4


= Statistics & machine learning

== Misc statistics

=== Formula table

[cols="1,3"]
|=====
| 𝔉 = { f(x;θ) : θ ∈ Θ }  | Parametric model
| Pr~θ~[·],  E~θ~[·],Var~θ~[·]  | Probability is with respect to PDF/PMF f(x;θ)
| N | Size of population
| n | Size of sample
| μ | Population mean
| μ̂ = x̄ | Common estimator for μ
| σ² | Population variance
| σ̂² = S² | Common estimator for σ²
| x̄ = 1/n ∑x~i~ | Sample mean
| S² = 1/(n-1) ∑(x~i~-x̄)² | Unbiased sample variance
| 1/n ∑(x~i~-x̄)² | Biased sample variance
| s²~p~ = (∑^k^(n~i~-1)s²~i~) / (∑^k^(n~i~-1)) | Pooled variance
| se[·] = sd[·] | Standard error of a statistic = standard deviation that statistic
| SEM = se[x̄] = sd[x̄] = σ / √n | Standard error of the mean, assuming independence and same variance σ²
| SEM̂ = sê[x̄] = S / √n | Common estimator for se[x̄]
| g(X~1~, ..., X~n~) | Statistic: Result of function g on a random sample
| θ̂ or θ̂~n~ | Estimator for quantity θ. Estimator = a statistic plus stating which quantity is estimated.
| Bias~θ~[θ̂] = E~θ~[θ̂] - θ = E~θ~[θ̂ - θ] | Bias of estimator θ̂ with respect to θ
| θ̂ is said to be consistent if θ̂ P→ θ |
| MSE[θ̂] = E~θ~[(θ̂-θ)²] |
| MSE[θ̂] = Bias²~θ~[θ̂] + Var~θ~[θ̂] |
| (X̄-μ) / se[X̄] ~ 𝓝(0,1) | For random variables {X~i~:i∈[n]} iid ~ 𝓝(μ, σ²)
| (X̄-μ) / sê[X̄] \~ t~n-1~ | For random variables {X~i~:i∈[n]} iid ~ 𝓝(μ, σ²)
|=====


=== Population & Sample

[[statistical_parameter]]
A _statistical parameter_ is a numeric characteristic of a population or statistical model.  Typically unkown. Often denoted using Greek letters.

[[population]]
A _(statistical) population_ is the same as the <<PDF>> / <<PMF>>.  So a population can be finite or infinite.  That's my personal definition.  Commonly the population is defined as the set of all possible observations of a random variable.  Personally I find that misleading. At least such an definiton should add ``where different observations having the same value are still different members in the set''.

A _population parameter_ is a specialication of <<statistical_parameter>> describing a numeric characteristic of a population. Often unobservable because the population is to large to evaluate every member.  Prominent examples are population mean μ and population variance σ².

[[population_mean]]
The _population mean_ μ is a population parameter and is the same as the expectation of the corresponding distribution.  A common estimator for the population mean is the sample mean X̄.

[[population_variance]]
The _population variance_ σ² is a population parameter and is the same as the variance of the corresponding distribution.

In general a _sample_ is a `subset' (however elements might be repeated) of a population optained through _sampling_.  Sampling is some process of selecting members of the population, possibly randomly, possibly based on a certain criteria.

A _(simple random) sample_ (_SRS_) is a set of n random variables X~1~, ..., X~n~ iid~ P, where P is some population.  Often a simple random sample is also defined as a subset of the population, drawn uniformly with replacement.  However that important part ``with replacement'' is unfortunately often omitted.

A _statistic_ is a numeric characteristic of a sample, as explained in detail in chapter <<statistic>>.

The _sample mean_ (or _empirical mean_), denoted X̄, of a sample X~1~, ..., X~n~ is the arithmetic mean, as defined below.  Is a statistic, i.e. a random variable.  The sample mean is a consistent estimator for the population mean μ, by the LLN.

X̄ = 1/n ∑X~i~ +
X̄ P→ μ +
E[X̄] = μ +
Var[X̄] = σ²/n

The _unbiased sample variance_ (or _Bessel-corrected sample variance_), denoted S², is definied as follows.  Is a statistic, i.e. a random variable.  Can be used as unbiased estimator for the population variance.

S² = 1/(n-1) ∑(X~i~-X̄)² +
E[S²] = σ²

Similarily, the _biased sample variance_ is defined by 1/n ∑(X~i~-X̄)².  Is a statistic, i.e. a random variable.

[[pooled_variance]]
Given k samples of k populations with common variance σ² and possibly different means.  Let s²~i~ denote the unbiased sample variance of the i-th sample, and n~i~ the size of the i-th sample.  The _pooled variance_ (or _combined variance_ or _composite variance_ or _overall variance_) is the weighed average of the individual unbiased sample variances, weighed by (n~i~-1): s²~p~ = (∑^k^(n~i~-1)s²~i~) / (∑^k^(n~i~-1)).  In the special case of k=2 and n~1~ = n~2~,  s²~p~ = (s²~1~+s²~2~)/2.  The pooled variance s²~p~ can be used as unbiased estimator for the common populaton variance σ².


[[statistic]]
=== Statistic

A _statistic_, often denoted T (or T~n~), is a function, often denoted g, which has a sample X~1~, ..., X~n~ as its domain. Formally: T = g(X~1~, ..., X~n~).  Thus a statistic is a random variable since it depends on the random sample X~1~, ..., X~n~ of the population.  In other words, a statistic is an attribute of a sample.  Unfortunately the term statistic can mean two things.  The term statistic can mean the random variable as described before, in which case it's often denoted uppercase T.  The term statistic can also mean the _observed value_ (or _realized value_) of that random variable, in which case it's often denoted lowercase t (or t~obs~).  Prominent examples are sample mean and (unbiased) sample variance.

The _sampling distribution (of a statistic)_ (or _finite-sample distribution_) is the probability distribution of a given statistic.  Recall that a statistic is a random variable, and thus has a distribution.  If we would take infinitly many same sized samples and calculate the statistic each time, we would get the sampling distribution.

The _standard error_ (or _SE_) of a statistic is defined by the standard deviation of that statistic, i.e. by the standard deviation of its distribution.  Standard error can be used to compute confidence intervals.  The 95% confidence interval for some variable a is approximately mean(a) ± 2SE(a), assuming a is normal distributed. The _68-95-99.7_ rule says that those are the approximate percentage values of the confidence intervals for 1SE(a), 2SE(a) and 3SE(a) respectively.

If the statistic is the mean, the standard error is called the _standard error of the mean_ (_SEM_) and is defined as follows.  However the population variance σ² is seldom known, thus the SEM is often estimated via estimating the population variance σ² by the unbiased sample variance S².

SEM = se[x̄] = sd[x̄] = +
σ / √n (if indpendent and same variance σ²)

SEM̂ = sê[x̄] = S / √n

Proof for sd[x̄] = σ / √n if independent and same variance σ²:  Var[x̄] = Var[1/n ∑x~i~] = 1/n² Var[∑x~i~] =(independent) 1/n² ∑Var[x~i~] =(same variance) 1/n² n Var[x~i~] = Var[x~i~] / n = σ² / n.

*to-do* ISLR p. 65 says that SE can be use to estimate how far off a single μ̂ might be from the true μ. But then the SE doesn't make sense if we calculate it on the basis of the population, since there we know μ exactly. Similarily, why is SE independent of the ratio populationsize:samplesize?

**to-do**(5) What is done in the R script from lecture week 2?


=== Estimator

An _estimator_ (or _point estimator_ or _(point) estimate_), denoted θ̂ (or θ̂~n~), of a parameter θ, is technically a statistic g(X~1~, ..., X~n~) plus conceptually stating which paramater θ its an estimator of.  In other words, an estimator θ̂ is a single ``best guess'' of parameter θ.  An estimator is a random variable since a statistic is one, see there.

Note that a program such as an machine learning algorithm that learns the parameters of a model is also an estimator.  Thus statements about how good an estimator can be, see e.g. Cramér-Rao bound, are important to machine learning. 1/(var[θ̂]𝓘(θ))

Recall that an estimator is a statistic and thus a random variable, so the _mean_ E~θ~[θ̂] and the _variance_ Var~θ~[θ̂] of an estimator are defined the usual way.

The _bias_ of an estimator θ̂ with respect to an unknown parameter θ is defined as Bias~θ~[θ̂] = E~θ~[θ̂] - θ = E~θ~[θ̂ - θ].  An estimator with zero bias is called _unbiased_.  Otherwise the estimator is said to be _biased_.  It can be shown that unbiasdness is not necessarily the goal. There exists biased estimators which are better in the least sequares sense than any unbiased estimators, see Cramér–Rao bound.

[[MSE_of_estimator]]
The _mean squared error_ (or _MSE_) of an estimator θ̂ with respect to an unknown parameter θ is defined as follows. The MSE can be used to assess the quality of the estimator θ̂. Note that there's also an analogously defined MSE for the estimate f̂ of an regression function f, see there.

MSE~θ~[θ̂] = E~θ~[(θ̂-θ)²] = +
Bias²~θ~[θ̂] + Var~θ~[θ̂] (see also <<bias_variance_trade_off>>)

_Consistent estimator_: An estimator θ̂ with respect to an unknown parameter θ is said to be _consistent_ if θ̂ P→ θ.  We almost always want an estimator to be consistent.  If we don't have that, then even with an infinitely large sample we still don't get the true parameter value θ.

If bias[θ̂]→0 and se[θ̂]→0 as sample size n→∞, then estimator θ̂ is consistent.

_Cramér–Rao bound_ (_CRB_) or (or _Cramér–Rao inequality_, or _information inequality_):  Tells you how good (in the least squares sense) an estimator possibly can be.  𝓘 is the <<fisher_information>>.  Note that in general it cannot be computed, since for that we would need to know p(𝓓|θ), but it's still important that there is a lower bound.

Var~θ~[θ̂] ≥ 1/𝓘(θ) [if unbiased estimator and some other assumptions] +
Var~θ~[θ̂] ≥ (1+∇~θ~Bias~θ~[θ̂])²/𝓘(θ) +
MSE~θ~[θ̂] ≥ (1+∇~θ~Bias~θ~[θ̂])²/𝓘(θ) + (Bias~θ~[θ̂])² +
MSE~θ~[θ̂] ≥ (1+∇~θ~Bias~θ~[θ̂])²/(nE~θ~[Λ²(θ)]) + (Bias~θ~[θ̂])² [which assumptions?]

*to-do* assumptions for 𝓘(θ) = nE~θ~[Λ²(θ)]
𝓘(θ) = Var[Λ(θ)]
V~𝓓~(θ) = ∇~θ~ ℒ~𝓓~(θ) = ∇~θ~ log p(𝓓|θ)

_Efficiency_ of an estimator e(θ̂) = 1/(var[θ̂]𝓘(θ)), where 𝓘 is the <<fisher_information>>.  The Cramér-Rao bound can be used to prove that e(θ̂) ≤ 1.  Or the other way round, the efficincy is a measure how close var[θ̂] comes to the lower bound 1/𝓘(θ).  A more efficient estimator needs fewer observations than a less efficient one to achieve a given performance.  An _efficient estimator_ (or _fully efficient estimator_) has e(θ̂) = 1.  An _asymptotically efficient_ estimator θ̂ has lim~n→∞~e(θ̂) = 1.

An estimator is called _asymptotically normal_ if √n(θ̂-θ) D→ 𝓝(0, V) for some V.  The _asymptotic variance_ is by some authors defined as V, by others as V/n.

An estimator is called _roboust_ if *to-do*


==== Notable estimators

See also <<map>> and <<mle>>.

Given an d-variate random variable X ∈ ℝ^d^, X \~ 𝓝~d~(θ, σ²I), where θ ∈ ℝ^d^ is unknown and σ² is known, and given a single observation x ∈ ℝ^d^.

The maximum likelihood estimator is θ̂^ML^ = x.

The _James-Stein_ estimator is given θ̂^JS^ = (1 - (d-2)σ²/‖x‖~2~²)·x, i.e. x is weighted. For m≥3 θ̂^JS^ dominates θ̂^ML^, i.e. MSE~θ~[θ̂^JS^] ≤ MSE~θ~[θ̂^ML^].


[[jacknife]]
==== Jacknife

A method to estimate the bias of an estimator θ̂. That can then be used to get an improved estimator θ̂^JK^, called the _jacknife estimator_.  Let θ̂^(-i)^ denote the estimator computed using the data except the i-th observation.

θ̂^JK^ = θ̂ - Biaŝ[θ̂] = nθ̂ - (n-1)θ̃ +
Bias[θ̂] ≈ Biaŝ[θ̂] = (n-1)(θ̃ - θ̂) +
θ̃ = 1/n ∑~1≤i≤n~θ̂^(-i)^

*to-do* slide 23, jacknife for bootstrap

Con: Reducing Bias comes at the cost of larger variance


=== Confidence Interval & Prediction Interval

Let C~n~ = (a,b) denote a 1-α _confidence interval_ for an unknown parameter θ, where a and b are statistics, and where 1-α is called the _confidence level_ (or _coverage_ of the interval).  A 1-α confidence interval is an interval such that in (1-α)·100% of the times you make an 1-α confidence interval for some parameter,  possibly each time for another parameter, the interval contains the true parameter.  See next paragraph for further explanations.  Common choices for the confidence level are 95% or 1%.

Note that a 1-α confidence interval does _not_ mean that given a realized interval there is a 1-α probability that it contains the true parameter.  The probability statement is about the interval which is defined by the statistics a and b, i.e. random parameters.  The probability statement is not about the fixed unknown parameter θ.  No probability statement concerning its value may be made.  (*to-do* 1) I don't get the difference.  What's the consequence whether (a,b) are random and θ is fixed or vice versa?  If you are given a 1-α confidence interval and the game is to predict whether it contains the true parameter, what percentage of your bet must the casino give you in order for the game to be fair?  At least in this example, I think it doesn't make a difference.  2) See forumula (6.9) on p. 92 in book "all of statistics". I'd say its _not_ P~θ~, its P~a,b~  3) See also Example 6.14 p. 93 in Book "All of statistics")

Note that confidence intervals are always for things we don't know (the unknown parameter θ), never for unknown things like an estimator θ̂.

_prediction interval_: An estimate of an interval in which a realization of a random variable (in other words, a future observation) will fall with a given probability.  E.g. given X ~ 𝓝(μ, σ²), the 95% prediction interval [μ-1.96σ, μ+1.96σ] for X will contain the next realization of X with a probability of 95%.  Often used for regression, where it's often used for Ŷ.

**to-do**(3) In a prediction interval it's really about the percantage of future observations (Y is a random variable after all) being within the one calculated interval (which however is based on random variables X Y), opposed to percentage of prediction intervals that will cover a value (there's no single true value, as said, Y is a random variable), right?


=== Statistical Model

A _statistical model_ 𝔉 is a set of distributions or regression functions (*to-do* but regression functions are quite a different thing than distributions; I don't understand). A _parametric model_ is set 𝔉 that can be parameterized by a finite number of parameters: 𝔉 = { f(x;θ) : θ ∈ Θ}, where θ is an _parameter_, or vector of parameters, that can take values in the _parameter space_ Θ. f is a function of x, parameterized by θ.

*to-do* clean up relation to <<statistical_parameter>>.

There's an loose distinction between parameters determined during fitting the model and _hyper-parameters_ which are determined before fitting the model, e.g by the user or during the higher level process of model selection.  You may think of splitting complete set of parameters into two subsets.  The values of the subset labeled parameters is computable cheaply when being provided with the values of the subset labeled hyper-parameters. References: https://stats.stackexchange.com/questions/149098/what-do-we-mean-by-hyperparameters?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa (*to-do* isn't a further difference that hyper-paremeters can influence the number of parameters, such as in polynomial regression?)

A _tuning parameter_  if the parameter's job is primarily a transient parameter of the learning algorithm.  Tuning parameters are also called hyper-parameters, conflicting somewhat the previous definition.  (*to-do* 1) But in this sence, a hyper 2) Clean up that parameter - hyper-parameter - tuning parameter mess)

The notations Pr~θ~[·],  E~θ~[·] and Var~θ~[·] mean that the probability is with respect to PDF/PMF f(x;θ), i.e. averaging over all possible observations x, the generating PDF/PMF being f(x;θ). (*to-do* in the context of an estimator θ̂, what if θ is not a parameter of a model, but some other population parameter)

p (or D) denotes the number of predictors and n (or N) the number of data points.  Predictors and data points will be defined shortly.  Given is a n ⨯ p matrix X (unfortunately this document uses X also to denote the random variable representing a row of this matrix) called _design matrix_ (or _model matrix_ or _regsessor matrix_).  Each column of X represents a _predictor_ (or _feature_ or _regressor_ or _attribute_ or _covariate_ or _covariable_ or _explanatory variables_ or _input variable_ or _independent variable_ or just _variable_). That is each column describes a feature / attribute of the thing at hand, for example height and weight of a person.  Given is a n ⨯ 1 vector Y (unfortunately this document uses Y also to denote the random variable representing an element of this vector) of _response variables_ (or just _reponse_ or _output variables_ or _dependent varables_).  The tuple (Y[i], X[i-th row]) represents the i-th _observation_ (or _data point_).

Let X denote the random variable representing one row of the design matrix X (unfortunately this document uses X for both things), and let Y denote the random variable representing an element of response vector Y.  We assume that there is some fixed but unknown relationship between the response Y and the predictors X.  We model that by the _regression function_ f (or _population regression function_ or _PRF_) by writing Y = f(X) + ε. This can be read as ``__is modeled as__'' or Y _is regressed_ on X.  f represents the _systematic_ information that the predictors provide about the response.  Ŷ is the resulting _prediction_ for Y.  The elements ŷ~i~ of Ŷ are called _fitted values_ (or _predicted values_).

[[error_terms]]
ε is a n ⨯ 1 vector (unfortunatly in this document also a random variabe denoting a single error term) of a _error terms_ (or _noise_ or _disturbance_), which are independent of X.  Each error term ε~i~ is an unobservable random variable.  It is a catch-all for all we miss with our model f.  The true relationship might not according to model f, there might be other variables that cause variation in Y that we didn't measure, and there may be measurement error....  If the model f is the correct model, then these error terms are random and have no systematic error (i.e. E[ε~i~] = 0 ∀ ε~i~).  We liked to have an estimate f̂ for f and use it like so Ŷ = f̂(X).

*to-do* better merge the above paragraph with the first few paragraphs of this chapter

The variance Var(ε~i~) of the error terms ε~i~ is in general not known.  Often it is assumed that all error terms have the same constant variance σ²,  and that constant variance often is estimated via σ̂ = RSE.  Note that the error terms are in direction of the y axis, as opposed to perpendicular to a linear regression hyperplane.  This is important to note because the later is what most humans intuitively do in the 2D case when guessing which of multiple regression lines is a better fit.

[[residual]]
e~i~ = y~i~ - ŷ~i~ is the i-th _residual_.

_Studentized residual_ (or _standardized residual_) t~i~ = e~i~ / sê[e]. Can be used to dedect outliers, see there.

In general, we can use regression only for prediction of a response variable given new predictors.  In general the observiations on which the regression is based do not allow for conclusions about causal relations. (*to-do* Some reference to a trusted source which concisely accurately states this)

See more statistics and definitions in <<measuring_model_performance>>.

[[trainingsampe_testsample_notation]]
Notation: In Pr~train~[·], E~train~[·], Var~train~[·], Bias~train~[·] etc. the sample space is the set of all possible training samples taken from the population.  Each training sample trains the estimate f̂.  Thus f̂, or more specically its estimated coefficients β̂, are random variables with a sample space as described before.  In Pr~test~[·], E~test~[·], Var~test~[·], Bias~test~[·] etc., the sample space is the set of all possible test samples taken from the population.

|=====
| n (N) | Number of samples
| p (D) | Number of predictors
| X | Predictor(s). n⨯p matrix or random variable
| Y | Response(s). n⨯1 vector or random variable
| ε | Error term(s). n⨯1 vector or random variable
| Often: Var[ε] = const = σ² | σ is in general not known. Often assumed to be constant.
| Often: Var̂[ε] = σ̂² = RSE² | Common estimator
| f | (True) regression function
| Y ≈ f(X) | ``Approximately modeled as'' or ``X is regressed on Y''.
| Y = f(X) + ε |
| f̂ | Estimate for f
| Ŷ = f̂(X) | Predictions (or fitted values). n⨯1 vector or random variable
| e = Y - Ŷ | Residuals. n⨯1 vector or random variable
| t~i~ = e / sê[e] | Studentized (or standardized) residuals. For sê[e] see your specific model.
|=====

References:

- Statisitic Cheat Sheet: http://web.mit.edu/~csvoss/Public/usabo/stats_handout.pdf


[[bias_variance_trade_off]]
=== Bias-variance tradeoff

The _bias-variance trade-off_ (or _bias-variance dilemma_) means two related things

- The expected test MSE can be decomposed in three terms, bias, variance and irreducable error, as the following equation shows.  We directly can see that the expected test MSE cannot become better than the irreducable noise.

- That in general, if variance goes down, bias goes up (underfitting), and vice verca, i.e. if bias goes down, variance goes up (overfitting).  As model flexibily increases, bias decreases, variance increases, and the expectedTestMSE will be convex, i.e. have a U-shape.  Thus the goal is to find the model with minimal expectedTestMSE.  When we say ``optimizing the bias-variance trade-off'', we really mean finding the minimal expected test MSE.

Simplified, looking only at a given point x~0~:

expectedTestMSE(x~0~) = (Bias~train~[f̂(x~0~)])² + Var~train~[f̂(x~0~)] + Var[ε]

More generally:

E~train,X,Y~[(f̂(X)-Y)²] = +
(E~X~[E~train~[f̂(X)]-E~Y~[Y|X]])² + [bias²] +
E~X,train~[(f̂(X)-E~train~[f̂(X)])²] + [variance] +
E~X,Y~[(Y-E~Y~[Y|X])²] [noise]

Recall that we saw the same pattern also with an estimator.


== Testing

=== Hypothesis testing, basic idea and algorithm

A _statistical hypothesis test_ is a method of statistical inference.

A _two sided test_ (or _two tailed test_) is concerned with both regions of rejection, of the distribution.  A _one sided test_ (or _one tailed test_) is concerned with the region of rection for only one of the two tails of the distribution, and it states which one it is concerned with.  The researcher has to decide which variant he prefers.  He can do it based on his educated guess what the alternate hypothesis is, and more specifically, what distribution of the alternate hypothesis is.  The goal is to maximize power, given a type I error rate.  For a concrete alternate hypothesis, power could be calculated by simulating: Do multiple times: Simulate data under the alternate hypothesis, calculate p-value, count H~0~ rejecetions (i.e. `H~a~ acceptances'). Over all this delivers power = H~0~-rejection-count / simulation-count. This way one can calculate power for multiple alternate hypothesises.

one sided vs two sided:

pro one sided test: higher power, i.e. less type II error rate.

*to-do* more pros & cons

Hypothesis test algorithm:

- Choose a suitable test statsistic T.  Compute its observed value t~obs~.

- Define the _null hypothesis_ and the complementary _alternate hypothesis_.  The null hypothesis (the hypothesis to be nullified), denoted H~0~, is a statement usually along the lines ``there is no relationship'' or ``there is no effect''.  The complementary alternate hypothesis is denoted H~a~ (or H~1~).  Note that in a one side test, H~0~ should not use =, but ≤ or ≥, while the complementary H~a~ then uses > or < respectively.  However it's mathematically still correct for the H~0~ to use = (*to-do* why is that?)

- Compute the p-value, see definition below.

- Choose a significance level α, see definition below.  Typically the significance level is chosen to be 5% or 1%.

- _Reject H~0~_ iff p-value < α.  Otherwise you _fail to reject H~0~_; you can't accept H~0~, see below.  An equivalent alternative criterion is to reject H~0~ when t~obs~ lies within the critical region, see definition below.

Hypothesis testing really is ``__proof by contradiction__''.  Only that we can't really proof or disprove anything,  since we only work with probabilities.  We only can gather evidence.  We start out assuming H~0~ is true and try to build a contradiction.  If we observe a t~obs~ such that p-value < α, then that is a `contradiction' to our assumption.  It's not a contradiction in a strict sense, but it's evidence that our assumption was incorrect.  In the other case, if p-value > α, we fail to build a contradiction, i.e. we fail to reject H~0~.  However we do not accept H~0~ either.  No conclusion can be drawn if you fail to build a contradiction.  The evidence is insufficient to support any conclussion about either H~0~ or H~a~.  Recall that we optained the p-value by assuming H~0~ is true, so we certainly can't derive from a p-value that H~0~ is true.

The _p-value_ (or _probability value_ or _asymptotic significance_) for a two sided test is Pr(T≥|t~obs~-E[T]| | H~0~), for a one sided test it is Pr(T≥t~obs~|H~0~) or Pr(T≤t~obs~|H~0~) respectively.  The interpretation of the p-value is: _Given_ H~0~ is true, then in (p-value)·100% of any hypothesis tests we see an result as extrem or more extrem (further away from mean) than t~obs~.  I.e. _given_ H~0~ is true, in (p-value)·100% of these tests we would incorrectly reject the null hypothesis.  The p-value is _not_ the probability that either hypothesis is correct.  Regarding the case of a one sided H~a~, where the very unlikely case occures that t~obs~ is of on the `other' side of H~0~'s distribution:  then the p-value will be very large, and we will not reject H~0~, which is correct in that we didn't accept H~a~.

The _significance level_ (or _type I error rate_) α is the probability of rejecting H~0~ given that H~0~ is true. Or in other words, the probability of a false discovery.  Or equavilently, α is the area below the H~0~ distribution in the critical region.  α is choosen by the user, see algorithm above.  Typically we want to control type I error rate, since a false discovery is worse than accidentaly not making a discovery.

The _type II error rate_ β is the probability of not rejecting H~0~ given that H~a~ is true.  Or equivalently, β is the area below the H~a~ distribution in the acceptance region.  Note that the distribution of H~a~ is unknown. β = 1 - power.

The _power_ (or _statistical power_) of a test is the probability of making a true discovery, given that H~a~ is true.  I.e. it is the probability of rejecting H~0~ given that H~a~ is true.  Or equivalently, power equals the area below the distribution of H~a~ in the critical region.  power = 1 - β.

The _critical region_ (or _rejection region_):  In a two sided test the critical region is [-∞,t~crit_a~] ∪ [t~crit_b~,∞],  where the _critical values_ crit_a and crit_b are defined via Pr(T≤t~crit_a~|H~0~) = α/2 and Pr(T≥t~crit_b~|H~0~) = α/2.  Or equivalently via the H~0~ distribution's quantile: t~crit_a~ = H0_dist_quantile(α/2) and t~crit_b~ = H0_dist_quantile(1-α/2).  In a onesided test its [-∞,t~crit~] where Pr(T≤t~crit~|H~0~) = α, or the other way round.  See also definition of significance level.

The _acceptance region_ is the complement to the critical region.

|=====
|                       | H~0~ really true | H~a~ really true
| failed to reject H~0~ | true positive | false negative, type I error, β
| H~0~ rejected         | false postive, type II error, false discovery, significance level α | true negative, true discovery, power
|=====

[[likelihood_ratio_test]]
_likelihood ratio test_: Only valid for nested models. *to-do*

The _Neyman-Pearson lemma_ states that a likelihood ratio test is the most powerful (i.e. has largest power) test among all tests having significance level α = Pr(likelihoodratio < c|H~0~ really true), where c is the likelihood ratio test's threshold.


=== Data snooping / selection effect

[[data_snooping]]
_Data snooping_ (or _data dredging_, _data snooping_, _p-hacking_) is searching patterns in data that then can be presented as statistically significant, without first devising a hypothesis.  The proper way is to first come up with a hypothesis, independently of the test data, and only afterwards test that hypothesis with test data.  Some patterns contained in large amounts of data (especially when number of predictors is huge and the number of observations is moderate) will be only due to chance.  When doing data snooping and actively searching for patterns, we are likely to find patterns, maybe ones that are there only due to chance (e.g. the few values of a predictor happen to correlate with the response by chance).  When then doing a hypothesis test with that same data, the p-value is meaningless, because the hypothesis is based on that data.

[[selection_effect]]
_Selection effect_: Any time we use the data to make a decision (e.g. select a model), we introduce a selection effect (bias). E.g. forward stepwise, lasso etc.


See also <<inference_after_model_selection>>


=== Multiple testing

The problem we're trying to solve here is this: If we make many hypothesis tests, each with significance level α, we're bound to make a false discovery α·100% of the times, because that's what significance level α says.  See also https://xkcd.com/882/ :-).

As in the case of finding the best expectedTestMSE, i.e. the best trade-off between increasing variance and decreasing bias, we now liked to find the best trade-off between inceease in type I error and increase in power.

_Classificaton of multiple hypothesis tests_: Consider m hypothesis tests. The following table defines variables counting how often each case occures. Upper case variables (U V T S and R) are random variables, lower case variables (m and m~0~) are fixed. The number of tests m is known, number of tests m~0~ where H~0~ is really true is unknown, the number of rejected H~0~ R is observable, the others are unobservable.

|=====
|                       | H~0~ really true | H~a~ really true | Total
| failed to reject H~0~ | U                | T                | m-R
| H~0~ rejected         | V                | S                | R
| Total                 | m~0~             | m-m~0~           | m
|=====

Q = V/R is the _false discovery proportion_ (_FDP_). By convention, if V = R = 0, then Q = 0.

The case of that H~0~ is always true, i.e. m = m~0~, is called the _gobal null_ (or _complete null_).

The _False discovery rate_ (_FDR_) is defined as FDR = E[Q] = E[V/R]. I.e. FDR is the expected proportion of type I errors (aka false discoveries) relative to all discoveries.

The _Famility wise error rate_ (_FWER_) is defined as FWER = Pr[V≥1].  I.e. FWER is the probability that we make an type I error (aka false discovery) at all.

δ = per test type I error rate +
FWER ≥ FDR +
FWER = FDR given global null +
FWER = 1 - (1-δ)^m^  given global null and independend tests +
FWER ≈ δm given global null and independend tests and small δ +
δ ≤ FWER ≤ δm

A procedure offers _weak control_ at level α if FWER ≤ α holds is guaranteed only under global null.  A procedure offers _strong control_ at level α if FWER ≤ α holds always.  Note that here α denotes _not_ the same thing as the significance level α of an individual test; here, it's the ``overall significance level''.

Techniques which control FWER: <<bonferroni_correction>>, <<westfall_young>>

Techniques which control FDR: <<bejamini_hochberg>>


==== No correction

*to-do*


[[bonferroni_correction]]
==== Bonferroni correction

Control of the FWER: goal is to get an FWER ≤ α.  Do each of the m individual tests at a significance level δ = α / m. As a result we get FWER ≤ α.

Neutral: Sensible if all tests are independent, because then FWER ≈ δm (assuming global null), see formulas after definition of FWER.

Contra: Can be too conservative (i.e. δ is smaller than needed), especially if the test statistics are positively correlated.  This is because the Boferroni correction assumes the worst case, which is mutually independent tests.  As an extreme example, under perfect positive dependence, there is effectively only one test, and thus we could choose δ = α and still have FWER = α, but instead we `needlessly' did choose δ = α / m.

Contra: As always wenn decrasing the siginificance level α, that comes at the cost of decreased statistical power, or equivalently, at the cost of increasing type II error rate.

*to-do* How much of the above applies to controlling FWER in general, and how much applies to Bonferroni in particular?


[[benjamini_hochberg]]
==== Benjamini Hochberg

Controls FDR.  *to-do*


[[westfall_young]]
==== Westfall Young permutation procedure

Weak control of FWER. Strong control of FWER under some assumptions.  Computes a significance level δ to be used for each test.

*to-do* what are these assumptions?

For all (or some, to save time) permutations allowed under H~0~: Compute p-value for each test, and find the minimum p-value. Overall this gives us an empirical distribution D of the minimal p-values. Compute δ = quantile~D~(α). Use δ as significance level for each of the tests.

This works because: FWER = P(V≥1) = P(p~i~≤δ for some p~i~) = P(min(p~1~, ..., p~m~)≤δ)

*to-do* properly understand why this works; why does the formula for δ work. see my lecture notes.

References:

- Slides7.pdf


=== Misc. test properties / characteristics

_paramtetric test_: Assumes distribution family of the test statistics

_non-parametric test_ (aka _distribution free_): No assumpotions on the distribution of the test statistic.


=== One sample test

_one sample test_: Only one sample, only one test statistic, treat every member of the sample the same way.


=== Unpaired two sample test

_unpaired two sample test_ (or _independent two sample test_): Two samples, e.g. one treated with treatment A and the other with treatment B (which might be `no treatment at all'). More formally, each of the two samples is drawn from another population, and the two populations have potentially different distributions.  Often the test statistic d is the difference or some kind of `difference', often standardized in some way, between the two sample means. The H~0~ is that the two population distributions are equal, which often means d = 0.

Disadvantage:

- The groups need to be really similar.  E.g. by chance the elements in either group might have something in common which has nothing to do with their treatment, but still influences the outcome of the test statistic.

- There might be a big variance in the test static.  E.g. if we measure how long people sleep, after treatment A and after treatment B: there is anyway a rather large variance in how long different people sleep on average (opposed to how long a given person sleeps in a given night).   We don't want that variance to have an influence on our result.  In the paired two sample test, that variance cancels out in the step of building the difference.

Examples:

- parametric unpaired two sample tests: H~0~: X̄~1~ and X̄~2~ are equal

  * <<z_test>> (assumes normal distr. with known variance): z = (X̄~1~ - X̄~2~) / (σ√(1/n~1~ + 1/n~2~)) ~ N(0, 1)

  * <<t_test>> (assumes normal distr. with unknown variance):

    ** equal sample sizes, equal variance: test statistic t = (X̄~1~ - X̄~2~) / (s~p~·√(2/n)) \~ t~2n-2~

    ** equal variance: test statistic t = (X̄~1~ - X̄~2~) / (ŝ~p~√(1/n~1~ + 1/n~2~)) \~ t~n1+n2-2~, where s~p~ denotes the pooled variance.

    ** general: Welch's t-test *to-do*

- non-parametric unpaired two sample tests:

  * <<permutation_test>>

  * <<wilcoxon_rank_sum_test>>


=== Paired two sample test

_paired two sample test_  (or _paired difference test_ or _paired sample test_): Treat every element in the sample with treatment A and with treatment B (again, can be `no treatent at all').  The test statistic is for example the mean of the differences of each pair.

Alternatively, we can match _match_ (or _pair_) every element in the treatment group with an element of the control group, the control group and the matching in a way that the matched pair shares similat observable characteristics.  Matching is however prominently critized.

*to-do* I don't see how the term two sample test still applies here -- the whole point is that its _not_ two samples

**to-do**(5) Are the terms "paired difference test" and "unpaired two sample test" really refering to exactly the same thing?

*to-do* In case of matching, what is then the difference to unpaired two sample test?

Examples: <<wilcoxon_signed_rank_test>>


[[permutation_test]]
=== Permutation test / randomization test

A non-parametric two sample test. General idea: Use permutations of group assignments to destroy the relationship that is to be tested under H~0~ while keeping all other relevant structure.  For each permutation, compute the test statistic, which overall delivers an empirical distribution called _permutation distribution_. Provides type I error control, proof below.

Informal proof for type I error control: When the data does come from H~0~, then the obtained permutation distribution is the distribution of the test statistic under H~0~. This is all we need for type I error control, since we need to control the probability of a false decision under H~0~.

t-test is an approximation to a permutation test.  Permutation tests are known since long, but for a long time we didn't had the computational power to make them feasible, and as a consequence were forced to use approximations like t-test.  Nowadays permutation tests are feasible.

Pro: No parametric assumptions

Pro: Free to use any test statistic

Pro: p-values and type I error control are exact if all permutations are considered. If only a subset of permutations are considered, it's an approximation.

**to-do**(3) Also the lecture scripts list "Paired two sample test / one-sample test for symmetry" as an example (or examples?) for perumatation test.  I don't understand that.

Contra: Computationally expensive

Contra: Not everything can be formulated as permutation test. E.g. in linear regression, there is no straightforward permutation test for individual coefficients.


==== As unpaired two sample test

Given population F~1~ and F~2~, and a sample from each, Y~1~^(1)^, ..., Y~n1~^(1)^ \~ F~1~ and Y~1~^(2)^,...Y~n2~^(2)^ \~ F~2~. H~0~: F~1~ = F~2~ (i.e. treatment has no effect), H~a~ : F~1~ is a shifted version of F~2~ (either in a two tailed or one tailed way).  The test statistic is a function of two samples, measuring some kind of difference between the two samples. For example sum of ranks (ranks with respect to combined sample) of sample1 (i.e. <<wilcoxon_rank_sum_test>> as permutation test), or median(sample1) - median(sample2).

- Compute t~obs~ using the original two samples.

- For all possible permutations (i.e. group/sample assignments) (or, computationally cheaper, repeatedly for a permutation selected uniformely at random from all possible permutations): compute t~i~, where i denotes the i-th permutation.  We can permute since under H~0~ assignment to sampe 1 or sample 2 is irrelevant.

- The set of t~i~ s form the emprical conditional distribution of test statistic T given the data, also calle the _permutation distribution_.

- Compute the p-value using t~obs~ and the obtained permutation distribution.

*to-do* What are properties of a good test statistics?  It seems often to be same sort of difference.  Note that rank sum of group1 is also sort of a difference.  It must be a function where the permutation has no effect under H~0~.

*to-do* add or replace with alternative version where instead an combinedsample we have sample1 and sample2 seperately.

------------------------------------------------------------
  combinedsample <- ... # sample1 concatenate sample2
  n1 <- ... # size of sample1
  repetitioncount <- ... #

  # function underlying test statistic T
  g <- function(combinedsample, n1) { ... }

  g_on_permuted_sample <- function(combinedsample, n1) {
    n <- nrow(combinedsample)
    permutedcombinedsample <- combinedsample[sample(1:n, n, replace=F)]
    return(g(permutedcombinedsample));
  }

  t.obs.all <- replicate(repetitioncount, g_on_permuted_sample(combinedsample, n1))
  t.obs <- g(combinedsample)
  pvalue <- (sum(t.obs.all<=t.obs)+1) / (repetitioncount+1)

  hist(t.obs.all)
  abline(v=t.obs)
------------------------------------------------------------


==== As paired two sample test

Same concept as before. However as in any paired two sample test, we no longer have two populations and thus two samples.  We have one single sample from one population, each element being the difference of a elementpair from sample A and sample B.  The test statistic t is a function on that sample consisting of differences.  A possible concrete test statistic is the mean (of the differences).

Under H~0~, the signs of the observations are random, so we can permute them, which overall delivers the empircal distribution of t.  With that, we can conduct a normal hypothesis test.

------------------------------------------------------------
  g <- function(sample) { ... }

  g_on_permuted_sample <- function(sample) {
    n <- nrow(sample)
    signs <- sample(c(-1,1), n, replace=T)
    sample.new <- signs * sample
    return(g(sample.new))
  }
------------------------------------------------------------


==== Permutationt test example: Correlation

Regression/classification setting. H~0~: no relationship between X and Y. Thus under H~0~, we can permute the Y values (or the X values/rows). As test statistic, we can for example use a rank correlation test statistic, for example Spearman's rank correlation coefficient.


==== Permutationt test example: Correlation / Linear regression

Given Y = β~0~ + β~1~X~1~ + ... + β~p~X~p~ + ε. H~0~: β~0~ = ... = β~p~ = 0.  Thus under H~0~, we can permute the Y values (or the X values/rows).  As test statistic, we can use for example the f-statistic of the linear regression fit.

Example: Exercise series 7, exercise 3


=== Monte Carlo test

We want to make an hypothesis test, but when the distribution of the test statistic is unknown or infeasible to work with, we may can simulate it instead.  For example number of duplicates in a set of n numbers choosen from [m].  We do a number of simulations.  Each simulation chooses n numbers out of [m] and we count the duplicates.  That delivers an empirical distribution, and we can then finaly conduct a hypothesis test using that empirical distribution.


=== Comparison of tests

*to-do* flow chart with all the test: t-test, z-test, Wilcoxon, the Wald, .... Overview with pros and cons. E.g. http://health.uottawa.ca/biomech/courses/apa3381/hyp_test.pdf


[[z_test]]
=== Z-test

A _Z-test_ is any statistical hypothesis test in which the test statistic follows approximately a Normal distribution under the null hypothesis.  Because of the central limit theorem, many test statistics are approximately normally distributed for large samples.

Examples: see those of Student's t-test. Only that in an Z-test, we know the variance σ² of the population, or have a good enough estimator for it, which is often the case for large samples.  So e.g. building on t-test's example of a one sample test, see below, we just would change the test statistic to z = (x̄ - μ~0~) / sd[x̄], which is standard Normal distributed.  Recall that sd[x̄] = σ/√n, see standard error of the mean.


[[t_test]]
=== Student's t-test

A _Student's t-test_ (or simply _t-test_) is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis.

_As one sample test_:  Given one sample with sample mean x̄.  We hypothise that μ~0~ is the population mean and want to test that.  Let μ denote the (true) population mean and S² the unbiased sample variance.  H~0~: μ = μ~0~.  As test statistic we use the t-statistic t = (x̄ - μ~0~) / sd̂[x̄], where sd̂[x̄] = S/√n, see also estimator for standard error of the mean.  Under H~0~ it's distribution is t~n-1~.

_As unpaired two sample test_:  Given two samples of equal size n and equal variance, one treated with treatment A and the other with treatment B (no treatment at all, or different treatment).  We want to test whether treatment A has an effect.  Let s~p~ denote the <<pooled_variance>>, X̄~A~ and X̄~A~ are the sample means.  H~0~: X̄~A~ = X̄~B~.  As test statistic we use the t-statistic t = (X~A~ - X~B~) / s~p~√(2/n).  Under H~0~ it's distribution is t~2n-2~.

_As paired two sample test_:  Given one sample, for each member, we calculate the difference of some test statistic after treatment A and after treatment B (no effect / controll), see also paire two sample test.  We want to test whether treatment A has an effect.  Let n denote the sample size, X~D~ the average of the differences and s²~D~ the variance of the differences.  H~0~: X~D~ = μ~0~ (often 0):  As test statistic we use the t-statistic t = (X~D~ - μ~0~) / sd̂[X~D~], where sd̂[X~D~] = s~D~/√n, see also estimator for standard error of the mean.  Under H~0~ its distsribution is t~n-1~.

_Linear regression_, testing wether a coefficient has an effect: see <<linear_regression_models>>


*to-do* See also Wilcoxon, The Wald test


=== The Wald test

*to-do*


=== F-Test & ANOVA

An F-test is a generic name for a class of statistical tests that share the property that the test-statistic follows an F-distribution (given the null-hypothesis).

One of the most common cases where a test-statistic `ends up' having an F-distribution, is when the ratio between two variances is calculated.

An ANOVA is a specific type of procedure that produces an F-statistic, because it tests the ratio between systematic variance and error-variance.


[[wilcoxon_rank_sum_test]]
=== Wilcoxon rank sum test (or Mann-Whitney U test)

A two sample test using the test statistic U which is the sum of ranks (ranks with respect to the combined sample) in smaple/group 1 (or sample/group 2, doesn't matter), and the null hypothesis H~0~ that the distributions of the two samples are equal.  If the two sample sizes are equal, the distribution of U under H~0~ is known.  For small sample sizes (~20), it's given by tables, for large sample sizes it can be approximated by a Normal distribution.

Don't confuse with <<wilcoxon_signed_rank_test>>.

Pro: No parametric assumptions

Pro: Robust, because the sum of ranks of group 1 statistic is robust.  E.g. if the largest value in a sample gets even larger, the mean would change, but the sum of ranks doesn't.

Pro: Doesn't require the two populations to be normally distributed, which is an advantage over the t-test.

Neutral: Power almost identical to that of t-test if distributions are Normal.

Pro: The null distribution (i.e. U under H~0~) is independent of F~1~ and F~2~.

_As a non-parametric unpaired two sample test_:  Regular hypothesis test. Given population F~1~ and F~2~, and a sample from each.  H~0~: F~1~ = F~2~, H~a~ : F~1~ is a shifted version of F~2~ (either in a two tailed or one tailed way).  Compute u~obs~ from the given sample, and from u~obs~, using the known distribution of U, the p-value.

_As unpaired two sample permutation test_:  An unpaired two sample permuatation test where the test statistic is U.


[[wilcoxon_signed_rank_test]]
=== Wilcoxon signed rank test

Don't confuse with <<wilcoxon_rank_sum_test>>.

Is a non-parametric paired two sample permutation test.  Let X~1~, ..., X~m~ \~ F~X~ and Y~1~, ..., Y~m~ \~ F~Y~ be independent, where (X~i~, Y~i~) is measured on the same subject i. Let D~i~ = X~i~ - Y~i~. The test statistic V is the following.  First remove all D~i~ = 0, resulting in a set of Dʹ~i~.  V = ∑rank~i~·H(Dʹ~i~), where rank~i~ is the rank of |Dʹ~i~| among all |Dʹ~i~|, and H(x) is the heavyside step function (0 for x < 0, 1 for x > 0).

The null hypothesis H~0~: The distribution of the test statistic V is symmetric around a = 0 (or equivalently, F~X~ = F~Y~).

See <<permuatation_test>> how to conduct the test as a whole. In brief: Exercise all possible permutations (or exercise a subset of N of those permutations).  Permuting here means permute (X~i~, Y~i~) for any i (or alternatively, for each D~i~, at random flip sign).  For each permutation, compute the test statistic V~i~.  Overall this delivers a empirical permutation distribution of V.  With this distribution and v~obs~ we can compute the p-value.

Pro: Doesn't require the two populations to be normally distributed, which is an advantage over the t-test.

Sidenote: Under H~0~, the distribution of V is a known distribution, however with no simple expression.  As the sample size increases, it converges to a normal distribution.  Thus we could also conduct a non-permutation test, and use that known distribution instead of the permuatation distribution presented here.


== Bootstrap

The _bootstrap_ is a method for estimating the distribution of an estimator θ̂ derived from on a population with unknown distribution (or known distribution family with unknown parameter).  We want the distribution of θ̂ for inference, e.g. for estimating se[θ̂] and computing confidence intervals for θ̂.  Let P be an unknown distribution, Z~1~, ..., Z~n~ iid \~ P a sample, and θ̂ (or θ̂~n~) an estimator of an unknown parameter θ.  Using the given sample we can calculate a realization of θ̂.  But we don't yet know the uncertainty involved with the estimator θ̂.  The bootstrap will deliver an empirical distribution of θ̂.

If we had P, we could simply simulate using P, i.e. take multipile samples, compute an θ̂ for each sample, delivering the empirical distribution of θ̂.  The key idea of bootstrap is to estimate P, denoted P̂ (or P̂~n~), and then use P̂ to simulate.  Estimating P is simple.  The estimate P̂ is an empirical distribution of Z~1~, ..., Z~n~ which places mass 1/n at each data point.  It is a discrete distribution on the data points, and each point is equally likely.  To sample from P̂ means to uniformily sample with replacement from {Z~1~, ..., Z~n~}.

Sampling from P̂ yields a so called _bootstrap sample_ Z~1~^∗i^, ..., Z~n~^∗i^ iid\~ P̂, where i denotes the i-th bootstrap sample.  Typically the bootstrap sample size is choosen to be the same size as the original sample size n.  Note that Z~1~^∗i^, ..., Z~n~^∗i^ are still random variables.  Side note: The probability that an observation z~i~ of the original sample is contained in the bootstrap sample is about 2/3.

The _bootstrapped estimator_ θ̂^∗^ is is then simply θ̂^∗^ = g(Z~1~^∗i^, ..., Z~n~^∗i^).  Is an estimator for θ̂; the point being that θ̂^∗^'s distribution can be simulated and so we can estimate θ̂'s distribution, expectation, variance etc.  Note that θ̂^∗^ is still a random variable, the (probability) sample space being the original sample, opposed to usual (probability) sample space being the population.

The _bootstrap distribution_ P^∗^ is the distribution of the bootstrapped estimator θ̂^∗^.  It is a conditional distribution given the (random) original sample.  P^∗^ is induced by sampling from P̂.

From now on its hard to use good terms.  One one side literature also uses only a few terms, and the few terms that are used are far from universal agreement.  And on the other side, when I tried to come up with a coherent set of terms, the individual terms become a so long sequence of words, that they become too unpractical.  I gave up and will mostly just use symbols.  The terms I do use I more or less liked, but they are not at all in universally used in literature.

_Bootstrapping an estimator_ θ̂ means sampling from P̂, delivering B bootstrap sample realization, the i-th denoted z~1~^∗i^, ..., z~n~^∗i^.  That then delivers B bootstrap estimator realizations (or _bootstrap replications_): θ̂^∗i^ = g(z~1~^∗i^, ..., z~n~^∗i^).  These B bootstrap estimator realizations then constitute the bootstrap distribution P^∗^.

Pr^∗^[·] is a conditional probability given the original sample.  The (probability) sample space is the original sample, as opposed to the usual (probability) sample space being the population.

The _bootstrap expectation_ E^∗^[·] (or E~P^∗^~[·]) is a conditional expectation given the original sample; the (probability) sample space is the original sample, as opposed to the usual (probability) sample space being the population.  I.e. changing the original sample will change the value of E^∗^[·].

Likewise the _bootstrap variance_ Var^∗^[·] (or Var~P^∗^~[·]) is a conditional variance given the (random) original sample.

The original problem included that we liked to know E[θ̂].  We can estimate E[θ̂] using E^∗^[θ̂^∗^] as an estimator. E^∗^[θ̂^∗^] is a consistent estimator for E[θ̂] if n is large, i.e. when <<bootstrap_consistency>> kicks in.  However we don't know that one either.  We can estimate E^∗^[θ̂^∗^] by averaging over the bootstrap replications θ̂^∗i^.  Ê^∗^[θ̂^∗^] is a good estimator for E^∗^[θ̂^∗^] if B is large. (*to-do* 1) is E^∗^[θ̂^∗^] ≈ Ê^∗^[θ̂^∗^] due to WLLN? 2) is Ê^∗^[θ̂^∗^] a consistent estimator for E^∗^[θ̂^∗^] -- or what is a a better term than `good' in ``Ê^∗^[θ̂^∗^] is a good estimator for E^∗^[θ̂^∗^] if B is large'')

E[θ̂] ≈(large n) Ê[θ̂] = E^∗^[θ̂^∗^] ≈(large B) Ê^∗^[θ̂^∗^] = 1/B ∑^B^ θ̂^∗i^

Likewise for variance:

Var[θ̂] ≈(large n) Var̂[θ̂] = Var^∗^[θ̂^∗^] ≈(large B) Var̂^∗^[θ̂^∗^] = 1/(B-1) ∑^B^(θ̂^∗i^ - Ê^∗^[θ̂^∗^])²

And for bias:

bias[θ̂] = E[θ̂] - E[θ] ≈(large n) E^∗^[θ̂^∗^] - θ̂ ≈(large B) Ê^∗^[θ̂^∗^] - θ̂

The _bootstrap quantile_ *to-do* it's conditional, right? Must be, since it's based on the conditional distribution P^∗^.


[[bootstrap_consistency]]
=== Bootstrap consistency

The boostrap is called to be _consistent_ with rate a~n~ for estimator θ̂ if for an increasing sequence a~n~, for all x:

Pr[a~n~·(θ̂-θ)≤x] - Pr^∗^[a~n~·(θ̂^∗^-θ̂)≤x] P→ 0 (as n→∞)

Explaining the formula:  For Pr^∗^[·] see previous chapter.  In the left Pr[...], the estimator θ̂ is a random variable and θ is fixed.  In the right Pr^∗^[...], the bootstrap estimator θ̂^∗^ is a random variable and θ̂ is the fixed realization of estimator θ̂ using the original sample.  Typically a~n~ = √n.  An oversimplified way is to think that θ̂-θ and θ̂^∗^-θ̂ must have the same CDF.

Bootstrap consistency is important because it makes things work.  Consistency of the boostrap typically holds if the limiting distribution of θ̂ is Normal, and if the original data Z~1~, ..., Z~n~ are iid. (*to-do* 1) What is exactly meant with `the bootstrap'? Isn't it more precise to say that the bootsraped estimator θ̂^∗^ is consistent under the following condition - but then why use another formula than that of an consistent estimator? 2) Which things in the context of our lecture do work only due to bootstrap consistency? The estimators for E[θ̂], Var[θ̂] etc (the first approximation/estimation) 3) Why is the bootstrap usefull if I almost only can use it when θ̂ is asymptotically normal -- the problem statement was that I don't know θ̂'s distribution)

Implication of bootstrap consistency: The shape of P^∗^ is that of θ̂. So they have same expectation, i.e. centered around same point, and same variance.  (*to-do* correctly phrased? see lecture slide5)

Consistency of the boostrap implies consistent variance and bias estimation, i.e.:

(E^∗^[θ̂^∗^]-θ̂) / (E[θ̂]-θ) P→ 1 +
Var^∗^[θ̂^∗^] / Var[θ̂] P→ 1


=== Bootstrap confidence intervals

We want to compute an estimate of 1-α confidence interval for the estimator θ̂.  As described in the bootstrap introduction chapter, we can optain the conditional distribution P^∗^ of the bootstraped estimator θ̂^∗^, and from that also its conditional quantile q~θ̂^∗^~. (*to-do* 1) all these are not true confidence intervals, each is an estimate of the confidence interval, right? What's the correct wording? 2) its a conditional quantile, right?)

_normal_: θ̂ ± q~Z~(1-α/2) · sd̂[θ̂], where Z ~ N(0, 1) and sd̂[θ̂] = √Var̂[θ̂] ≈ √Var̂^∗^[θ̂^∗^] as described in the bootstrap intro chapter.  Note θ̂ does lie in the middle of the interval.

_quantile_ (or _percentile_): [q~θ̂^∗^~(α/2), q~θ̂^∗^~(1-α/2)]. Special case of reversed quantile: it equals reversed quantile if the distribution of θ̂^∗^ - θ̂ is symmetric.  It's proovable that coverage is not 1-α.  Note that θ̂ might not lie in the middle of the interval.

_reversed quantile_ (or _basic_):  [θ̂ - q~θ̂^∗^-θ̂~(1-α/2), θ̂ - q~θ̂^∗^-θ̂~(α/2)].  If bootstrap consistency holds, its proovable that coverage is 1-α.  Note that θ̂ might not lie in the middle of the interval.  Performance in practice is sometimes critized.

*to-do* why do we make it so complicated - what's the intuition why we now get a better coverage that the quantile CI

*to-do* lecture notes say that performance is sometimes critized -- but nobody says that normal or quantile has better theoretical performance, right? Or whas performance meant in sence of computationally expensive?

_bootstrap T_ (or _studentized_):  [θ̂ - q~(θ̂^∗^-θ̂)/sd̂[θ̂^∗^]~(1-α/2) · sd̂[θ̂], θ̂ - q~(θ̂^∗^-θ̂)/sd̂[θ̂^∗^]~(α/2) · sd̂[θ̂]].  sd̂[θ̂] as in normal CI.  We approximate (θ̂^∗^-θ̂)/sd̂[θ̂^∗^] by the empirical distribution (θ̂^∗1^-θ̂)/sd̂[θ̂^∗1^], ..., (θ̂^∗B^-θ̂)/sd̂[θ̂^∗B^].  Bootstrap T has best theoretical properties, but is computationally very expensive.  Intuition:  Look at θ̂-θ ≈ θ̂^∗^-θ̂, where θ̂-θ is what I would like to have and θ̂^∗^-θ̂ is my observation.  If we instead take (θ̂-θ)/sd(θ̂) ≈ (θ̂^∗^-θ̂)/sd̂[θ̂^∗^], the two sides get similar more quickly.  Note that θ̂ might not lie in the middle of the interval.

References:

- Slides5.pdf


=== Parametric bootstrap

In parametric boostrap (or model-based bootstrap), the sample is Z~1~, ..., Z~n~ iid \~ P~δ~, where δ is an unknown parameter of a known distribution family P~δ~.  We make an estimate δ̂ of δ, and can then create B samples from P~δ̂~.  Now we're in the same situation as in non-parametric bootstrap: we have B new samples constituting an empircal distribution.  Thus confidence intervals etc work the same for parametric bootstrap.

Pro: Good _if_ parametric model is approximately correct, then P~δ̂~ is closer to P~δ~ than P̂ is to P.

Pro: For small n, non-parametric bootstrap might be poor, because estimates (Ê[θ̂], Var̂[θ̂] etc.) are only good for large n.

Contra: We need to make assumptions (the family P~δ~ and the estimate δ̂). Non-parametric bootstrap doesn't need assumptions (i.e. only those of bootstrap consistency)

Contra: Bad if parametric model is far from the truth, then P~δ̂~ is farther from P~δ~ than P̂ is from P.  I.e. parametric bootstrap is sensitive to model misspredictions.


=== Bootstrap for regression

The model is Y = f(x) + ε, where the error terms have unknown distribution, in general maybe not even iid.  There are multiple options, ranging from fully parametric to fully non-parametric. For example:

- Fully parametric regression: We assume parametric model f(x) = Xβ and ε iid\~ N(0,σ²), with the parameters β and σ. We estimate these parameters giving us β̂ and σ̂.  We then can sample like this: Y^∗^ = f̂(x) + ε^∗^, where f̂(x)=Xβ̂ and ε^∗^ iid~ N(0,σ̂²).

- Non-parametric residuals: No assumptions on error terms ε, but still assuming parametric model f(x) = Xβ.  We make estimation β̂.  This delivers residuals e = Y - f̂(x), where f̂(x)=Xβ̂.  Sampling with replacement from e~1~, ..., e~n~ gives sample of residuals e^∗^. Y^∗^ = f̂(x) + e^∗^.

- Non-parametric: Resample observations (i.e. rows in cbind(Y,X) matrix), i.e. vanilla non-parametric bootstrap


=== Bootstrap for model selection / model validation

See <<bootstrap_model_validation>>.


=== Bootstrap summary

non-parameteric case:

_Real world_: +
we have a sample (Z~1~, ..., Z~n~) \~ P (unknown) +
estimator θ̂ = g(Z~1~, ..., Z~n~) \~ unknown-distribution +
θ̂'s value known since g(Z~1~, ..., Z~n~) can be computed

_Boostrap world_: +
sampling from P̂ delivers bootstrap sample (Z~1~^∗^, ..., Z~n~^∗^) +
bootstraped estimator θ̂^∗^ = g(Z~1~^∗^, ..., Z~n~^∗^) ~ P^∗^ +
Distribution P^∗^ given by the B bootstrap sample realizations

parametric case:

_Real world_: +
P~δ~ is a known parameterized family of distributions, δ is unknown +
we have a sample (Z~1~, ..., Z~n~) \~ P~δ~ +
estimator θ̂ = g(Z~1~, ..., Z~n~) \~ unknown-distribution +
θ̂'s value known since g(Z~1~, ..., Z~n~) can be computed

_Boostrap world_: +
make estimate δ̂ +
sampling from P~δ̂~ delivers bootstrap sample (Z~1~^∗^, ..., Z~n~^∗^) +
bootstraped estimator θ̂^∗^ = g(Z~1~^∗^, ..., Z~n~^∗^) ~ P^∗^ +
Distribution P^∗^ given by the B bootstrap sample realizations


[cols="1,3"]
|=====
| B                          | Number of bootstrap samples
| P                          | The unknown distribution of the (original) sample Z~1~, ..., Z~n~
| P̂                         | Estimate of P. Empirical distribution of Z~1~, ..., Z~n~ which places probability mass 1/n on very data point Z~i~.
| Z~1~, ..., Z~n~ iid ~ P    | (Original) sample
| Z~1~^∗^, ..., Z~n~^∗^ iid ~ P̂ |  Bootstrap sample (or simulated data)
| z~1~^∗i^, ..., z~n~^∗i^    | i-th bootstrap sample realization
| θ (or θ~0~)                | Unknown parameter
| g                          | Function underlying the estimator θ̂
| θ̂ (or θ̂~n~) = g(Z~1~, ..., Z~n~) | Estimator for θ.  θ̂'s distribution is unknown, but we would like to know it.
| Pr^∗^[·]                   | Conditional probability given the original sample.
| E^∗^[·] (or E~P^∗^~[·])    | Bootstrap expectation. Conditional expectation given the original sample
| Var^∗^[·] (or Var~P^∗^~[·])| Bootstrap variance. Conditional variance given the original sample
| θ̂^∗^                      | Boostraped estimator. Random variable, the (probability) sample space being the original sample.
| θ̂^∗i^ = g(z~1~^∗i^, ..., z~n~^∗i^) | i-th bootstraped estimator realisation
| P^∗^                               | Bootstrap distribution. Distribution of θ̂^∗^.  Is a conditional distribution given the original sample.
| E[θ̂] ≈ Ê[θ̂] | large n
| Ê[θ̂] = E^∗^[θ̂^∗^] ≈ Ê^∗^[θ̂^∗^] | ≈ large B
| Ê^∗^[θ̂^∗^] = 1/B ∑^B^ θ̂^∗i^ |
| Var[θ̂] ≈ Var̂[θ̂] | large n
| Var̂[θ̂] = Var^∗^[θ̂^∗^] ≈ Var̂^∗^[θ̂^∗^] | ≈ large B
| Var̂^∗^[θ̂^∗^] = 1/(B-1) ∑^B^(θ̂^∗i^ - Ê^∗^[θ̂^∗^])² |
| bias[θ̂] = E[θ̂] - E[θ] ≈ biaŝ[θ̂] | large n
| biaŝ[θ̂] = E^∗^[θ̂^∗^] - θ̂ ≈ biaŝ^∗^[θ̂^∗^] | ≈ large B
| biaŝ^∗^[θ̂^∗^] = Ê^∗^[θ̂^∗^] - θ̂ |
|=====


References:

- Book ``An introduction to statistical learning'', chapter ``5.2 The Bootstrap''

- Book ``All of statistics'', chapter ``8 The Bootstrap''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``5 Bootsrap''


== Misc machine learning

_parameter_ θ (read hypothesis), _parameter space_ Θ, _hypothesis_ h(x) (parameterized by θ): *to-do*

_empirical risk minimizer_ (_ERM_): ĥ = argmin~h~(R̂~trainset~(h))

_likelihood function_ (or _sampling distribution_) ℒ~𝓓~(θ) = p(𝓓|θ) =~iid~ ∏p(d~i~|θ): Probability of seeing the data 𝓓 given the model parameters θ.

*to-do* often p(𝓓|θ) is viewed as p(y~i~|x~i~,θ), but that isn't the same as p(y~i~,x~i~|θ). Why can we do that? Or is it just that we apply the term likelihood also to p(y~i~|x~i~,θ).

_log likelihood_ log p(𝓓|θ)

_negative log likelihood_ (_NLL_) -log p(𝓓|θ). Often used instead of the log likelihood because the negative log likelihood can be used as a cost function.  The `better' θ is, the smaller the NLL becomes.

_prior (probability)_ p(θ) (or more accurately, p(θ|Θ)): Probability of model paramters θ without having seen any data yet.

[[score_function]]
_score function_ (or _score_, or _efficient score_, or _informant_) Λ (or V) Λ~𝓓~(θ) = ∇~θ~ ℒ~𝓓~(θ) = ∇~θ~ log p(𝓓|θ) = 1/p(𝓓|θ) ∇~θ~p(𝓓|θ) =~iid~ ∇~θ~ ∑ log p(d~i~|θ) = ∑((∇~θ~p(d~i~|θ))/p(d~i~|θ))

[[fisher_information]]
_Fisher information_ 𝓘(θ) = Var[Λ(θ)], where Λ(θ) is the <<score_function>>.

_model evidence_ (or _marginal likelihood_) p(𝓓) = ∫p(𝓓,θ)dθ

_posterior (probability)_ p(θ|𝓓) = p(𝓓|θ)p(θ) / p(𝓓) ∝ p(𝓓|θ)p(θ): Is the likelihood times the prior, normalized.

Or recursively, assuming iid data, p(θ|𝓓) = p(x~n~|θ)p(θ|𝓓^n-1^) / evidence, evidence = ∫p(x~n~,θ)p(θ|𝓓^n-1^)dθ, with the base case p(θ|𝓓^0^) = p(θ), where x~n~ denotes the n-th observation and 𝓓^n^ denotes all observations up to and including the n-th observation.  This is based on the idea that the likelihood can be recursively expressed as p(𝓓|θ) = p(𝓓^n^|θ) = p(x~n~)p(θ|𝓓^n-1^), with the same base case.  Thus with each new observation, we improve the posterior probability p(θ|𝓓).

[[KL_divergence]]
The _Kullback-Leibler divergence_ (_KL divergence_) is a measure of difference of two probability distributions.

[[mle]]
_maximum likelihood estimator_ (_MLE_) θ̂^MLE^ = argmax~θ~(p(𝓓|θ)): A frequentist point estimator which sets the model parameters θ to the values that maximize the likelihood function p(𝓓|θ).  This corresponds to choosing the model parameters θ for which the probability of the observed data set is maximized. Properties when the parametric model is correct (i.e. the process generating the data can correctly be described by the parameteric model having parameter space Θ):

- consistent (θ̂^MLE^ P→ θ)
- asymptotically efficient
- asymptotic normality: √n(θ-θ̂^MLE^) → 𝓝(0, J^-1^IJ^-1^), where I=Var~θ~[Λ(θ)], J=-E~θ~[∇~θ^T^~Λ(θ)], and Λ denoting the score function
- equivalence: f(θ̂^MLE^) is MLE of f(θ)
- prone to overfitting if model is flexible (i.e. has many parameters)
- Λ(θ̂^MLE^) = 0. Since θ̂^MLE^ is a maxima of the likelihood.

If the parameteric model is not correct:

- θ̂^MLE^ P→ θ̂^KL^, where θ̂^KL^ minimizes the <<KL_divergence>> w.r.t. to the true distribution of the data.


Relation to other estimators:

- MLE is a special case of MAP estimator, where the prior is uniform. Recall θ̂^MAP^ = argmax~θ~(p(θ|𝓓)) = argmax~θ~(p(𝓓|θ)p(θ)) and θ̂^MLE^ = argmax~θ~(p(𝓓|θ)).

- See also <<MLE_vs_bayes>>

Lets express p(𝓓|θ) as p(y~1:n~|x~1:n~,w). θ̂^MLE^ = argmax~w~(p(y~1:n~|x~1:n~,w)) = argmin~w~(-log p(y~1:n~|x~1:n~,w)) =~iid data~ argmin~w~(-∑~1≤i≤n~log p(y~i~|x~i~,w)).


[[map]]
_maximum a posteriori probability estimate_ (or _MAP estimate_) _for model parameters_: θ̂^MAP^ = argmax~θ~(p(θ|𝓓)) = argmax~θ~(p(𝓓|θ)p(θ)): A estmator which sets the model parameters θ to the values that maximize the posterior probability.  MAP estimate is a generalization of MLE, see there.

_MAP estimate for predicton_: ŷ = ĥ(x) = argmax~y~[p(y|x,𝓓)]. I.e. the mode of the probabilistic prediction distribution. For binary classification, i.e. 𝓨 = {+1, -1} and K=2, this can be written as ĥ(x) = signʹ(log(p(y=+1|x)/p(y=-1|x))).

_Bayes' optimal predictor for squared loss_: Let X and Y denote the random variables denoting a predictions vector and a response respectively.  Assume data is generated iid.  Assume (unrealistically) we knew the joint distribution p(X,Y). The hypothesis h^∗^ minimizing the expected least square error E~X,Y~[(Y-h(X))²] is given by the conditional mean h^∗^(x) = E~X,Y~[Y|X=x] = ∫p(y|X=x)·y·dy and is called Bayes' optimal predictor for the squared loss.

_conjugate distributions_ / _conjugate priors_: A pair of prior distribution and likelihood distribution is called _conjugate_ if the associated posterior remains in the same family as the prior.  Conjugate priors can be used as regularizers.  Example: Flipping a coin.  Let θ denote the probability of heads.  We assume the prior to be Beta(a, b).  We observe h heads and t tails.  Calculating the posterior (posterior = likelyhood·prior/evidence) distribution gives Beta(a+h, b+t).

|======
| prior / posterior            | likelihood
| Beta                         | Bernoulli / Binomial
| Dirichlet                    | Categorical / Multinomial
| Gaussian (fixed covariance)  | Gaussian
| Gaussian-inverse Wishart     | Gaussian
| Gaussian process             | Gaussian
|======

_probabilistic predictions_ / _predictive distribution_ p(y|x,𝓓): denotes the probability distribution over possible responses, given the input vector x and training set 𝓓. I.e. it represents a probability for each possible value of the response y.

_supervised learning_: The given data is composed of the features X and the respective labels Y.

_unsupervised learning_: The given data has only the features X, but no labels.

_active learning_ (or _semi-supervised learning_ (_SSL_)): The labels are theoretically available, but expensive to acquire, so the learning algorithm must decide which labels it wants to see.  Semi-supervised learning can also be viewed at as the case of having some missing labels.

_frequentist vs bayesian_: In the probabilistic view, we also model the uncertainty. Before we didn't do that, or only in a relatively limited form via calculating variance, confidence interval, prediction interval.


*to-do* explain much better frequentist vs bayesian


=== Taxonomy of data

_object_: An object of interest, e.g. a person. _measurements_: One or more measurements per object or set of objects. E.g. when the data is some distance measure between two objects, then the measurement is a mapping from two objects to ℝ. This is also called _proximity data_. Sometimes, e.g. in psychology, it is better to measure a trait relative between two people, as opposed to trying to measure a trait per person.

_design space_: 𝓞 = 𝓞^(1)^ ⨯ 𝓞^(2)^ ⨯ ...

_measurement space_: 𝓧

_measurement_: A function 𝓞^(1)^ ⨯ 𝓞^(2)^ ⨯ ... → 𝓧

_monadic data_ 𝓞^(1)^: measurements are for one object

_dyadic data_ 𝓞^(1)^ ⨯ 𝓞^(2)^: mesurements are for multiple objects. E.g. measurements for each of \{users} ⨯ \{websites}, or for each of \{bag of words\} x \{websites}.

_pairwise data_: special case of diadic data where the two input object spaces are equal.

_nominal scale_ (or _categorial scale_): a set without any ordering. _ordinal scale_: an ordered set of values. The values are irrelevant, only their rank order matters.

_quantitative scales_: _interval scale_ the relation of numerical differences carries the information. Invariant w.r.t. translation and scaling. E.g. Fahrenheit; zero point and units were chosen arbitrarily.  _Ratio scale_ Invariant w.r.t. scaling. E.g. Kelvin scale; zero point is fixed, but unit is arbitrary.  _Absolut scale_: Absolut values carry information. E.g. swiss exam grades.

_data withening_: *to-do*


=== Classification

The goal of _classification_ is to take an input (vector) x and to assign it to one of K (or D or c) discrete classes C~k~, where k = 1, ..., K.  In the most common scenario the classes are disjoint.  The input space is devided into _decision regions_ whose boundaries are called _decision boundaries_ (or _decision surfaces_).

[[discrimant_function]]
A _discriminant function_ (or just _discriminant_) f~j~(x) is a function that takes a feature vector x as input and returns a high value if x is likely to belong class j. When making predictions, we can then return ŷ = argmax~j∈[K]~(f~j~(x)). In case of binary classification, it is enough to have one discriminant function, and the x for which f(x)=0 constitute the decision boundary.

The _margin_ is defined to be the smallest distance between the decision boundary an any of the observations.  Generally we prefer classifiers which result in a large margin.  The training sample is just a sample from the population, so a larger margin reduces the risk of missclassification for another (test) sample (imagine the 2D case, imagine a cloud / ellipse around each region of same-class-observations).  Also, the values of the x vector of each observation might be contaminated with measurement errors, i.e. is again a `cloud'.

References:

- Pattern Recognition and Machine Learning, chapter ``Linear Models for Classification''.


==== Class imbalance

When one class is much less frequent than the other class. By convention, the infrequent class has the `+' label, and the other the `-' label. A problem with imbalanced data is that the minority class instances contribute little to the empirical risk relative to the majority class.

Solutions:

- upsampling of minority class observations

- downsampling of majority class observations

- <<cost_sensitive_classification>>. Simulates upsampling/downsampling by scaling the loss.

- in case of a linear classifier, shift hyperplane by parameter τ: h(x) = signʹ(w^T^x-τ)


[[metrics_for_imbalanced_data]]
==== Metrics for imbalanced data

TP = true positive, TN = true negative, FP = false positive, FN = false negative, true_predictions = TP + TN.

|====
|                    | true label positive | true label negative
| predicted positive | TP                  | FP
| predicted negative | FN                  | TN
|====

- _accuracy_ = true_predictions / all: Bad metric for imbalanced data, since always returning the majority label still gives a good accuracy.

- _precision_ = TP / (TP+FP)

- _recall_ (or _true positive rate_ (_TPR_) or _sensitivity_) = TP / (TP+FN)

- _false positive rate_ (_FPR_) = FP + (TN+FP)

- _F1 score_ = 2TP / (2TP+FN+FP) = harmonic mean of precision and recall: We like to have both a high precision and a high recall. However the two are in contention, increasing one usually decreases the other.  The harmonic mean is sort of the average, but biased towards the smaller number. Thus to get a good F1 score, you need both a good precision and recall.

- _Reveiver operator characteristic (ROC) curve_: Plot false positive rate vs true positive rate. Often one curve per algorithm, giving a tool to compare the algorithms.

- _Precision recall curve_: As above, but precision vs recall.

- _AUC_: Area under the curve

- _Balanced Multi Class Accuracy_ (_BMAC_): 1/K∑~1≤c≤K~TPR~c~


[[cost_sensitive_classification]]
==== Cost sensitive classification

*to-do*

Examples: <<bayesian_decision_theory>>


==== Multi-class classification

- _one-vs-all_: Train K binary classifiers, one for each class. Per classififier, use positive label for observations of the respective class, and negative label for all other observations.  Thus we get a set of scores, one score for each class.  A high score meaning a high confidence that it's the respective class, a low score meaning high condifence that it's not the respective class.  The overall classification then for example chooses the class with the highest score.  One possible way to calculate score is w^T^x, i.e. distance to decision boundary.  In that case however weights must be normalized: w=w/|w|.

  * The K scores must be "on the same scale"

  * Individual binary classifiers see imbalanced data, even if the whole data set is unbalanced

  * One class might not be linearily separable from all others (e.g. consider three clusters clearly horizontally side by side, each cluster of a given class, then the middle cluster is not linearly separable from the one set of all others)

- _one-vs-one_: Have C(K 2) = K(K-1)/2 binary classes, one for each pair of classes. Then do majority voting, i.e. class with highest number of positive predictions wins. Note that no scores are needed, a binary decision per classifier is good enough.

- _explicit multi-class models_:

  * E.g. neuronal nets, having K output units, each having a sigmoid like activation function.

  * <<multi_class_SVM>>

  * Multi-class Perceptron

In practice, one-vs-all and one-vs-one work pretty well.


=== Generative model vs discrimantive model

_generative model_: Models the distribution of individual classes (i.e. the joint probability distribution p(x,y)), i.e. models how the data was generated.  However this is in general very hard. Typically less robust against outliers, because we have to make more assumptions, i.e. we can make more mistakes.  Since it knows the joint probability p(x,y), it can then easily derive the predictive distribution p(y|x) which also the discriminative model has.  The inverse is obviously not true, i.e. given p(y|x), one can in general not derive p(x,y).  The name `generative model' comes from that the p(x|y) which we estimate can be seen as generating a feature vector given a label.  That can be usefull to impute missing data.

p(y|x) = p(x|y)p(y)/p(x) [allows making predictions] +
p(y) [estimated; allows to generate labels] +
p(x|y) [estimated; allows to generate features given a label] +
p(x) = ∑~y~p(x|y)p(y) [allows to detect outliers] +
p(x,y) = p(x|y)p(y)

_discriminative model_: Uses the data to model/learn (discriminating) decision boundaries between classes.  Learns the the function h~θ~:𝓧→𝓨 or the conditional probability distribution p(y|x,θ).  Simply categorize, without caring about how the data was generated.  Does not attempt to model p(x).  Since it doesn't know p(x), it cannot detect outliers.  Similarly, consider logistic regression and a new point x, which is far away from the decision boundary and far away from the training data, for which we should make prediction.  Logistic regression would be very confident in its answer since x is far away from the decision boundary. However it may should not be that confident, since x is very different from any training observation.  The root of the problem is that the discriminative model doesn't care about p(x).  Typically more robust than the generative model, since modeling x may be very difficult.  For classificiation tasks generally outperforms generative model.

h~θ~:𝓧→𝓨 or p(y|x,θ) [allows making predictions] +
θ [parameters of the model to be trained, e.g. via MAP or MLE] +
θ̂^MLE^ = argmax~θ~(p(𝓓|θ)) +
θ̂^MAP^ = argmax~θ~(p(θ|𝓓)) = argmax~θ~(p(𝓓|θ)p(θ))

_recipe for generative model_: Typically the follwing recipe is used to estimate the joint distribution p(x,y) and the conditional distribution p(y|x).  The goal is to find the joint distribution p(x,y).  We don't do that directly, but note that by the chain rule, p(x,y) = p(x|y)p(y).  So we're left with estimating the factors p(x|y) and p(y).  We then can make predictions using the predictive distribution p(y|x) = p(x|y)p(y)/p(x), where p(x) = ∑~y~p(x|y)p(y).  More concretely, we can predict using MAP estimate.

_outlier detection_ with p(x) of generative model: We declare x an outier if p(x) is below some threshold.  If the threshold is chosen to high, we have too many false positives, if it's to low, we have too many false negatives.  If our (training) data is such that we know for each observation whether or not it is an outlier, we can find the threshold via cross-validation and some metric which balances false negatives and false positives, see <<metrics_for_imbalanced_data>>.  If our (training) data doesn't tell us wether a given observation is an outlier, finding a good threshold is challenging.

*to-do* anomaly/outlier (small p(x)) vs being unsure (p(y=c1|x) isn't clearly better than p(y=c2|x), where c1 is the most likely class and c2 is the runner up class)


=== Decision theory

_inference_ step/stage: Determination of the joint probability p(x,y) from training data. Is typically a very difficult problem.

_decision_ step/stage: Make an optimal decision regarding the problem at hand, based on the joint probability p(x,y) retreived from the inference step. Is generally very simple, even trivial, once we solved the inference problem.

A <<discrimant_function>> combines the inference stage and the subsequent decision stage into one step.

_minimizing misclassification rate_: Choose decision regions such that the probability for missclassification is minimal.

_minimizing the expected loss_: Choose decision regions such that the average total loss E[L], according to a loss function / matrix, is minimized. E[L] = ∑~k~∑~j~(L~k,j~∫~Rj~p(x, C~k~)dx).

_reject option_: In regions where we are relatively uncertain about the correct class, we dodge the problem and simply reject the input, i.e. answer "I don't know".  More formaly, we reject an input x if it lies within a region where the highest (with respect to k) posterior probability p(C~k~|x) is equal to or below a given threshold θ.


[[bayesian_decision_theory]]
==== Bayesian decision theory

A form of <<cost_sensitive_classification>>.  Given a conditional distribution over labels p(y|x), a set 𝓐 of actions, and a cost function C:𝓨⨯𝓐→ℝ. Bayesian decision theory recommends to pick the action that minimizes the expected cost. This decision is called the _Bayesian optimal decision_.

a^∗^ = f(x) = argmin~a∈𝓐~(E~y∈𝓨~[C(y,a)|x]) [f chooses best action a^∗^] +
p(y|x) is given

Actions can e.g. be:

- 𝓐 = 𝓨, i.e. the action is to return the classification / regression result.

- 𝓐 = {𝓨, "don't know" }. E.g. when we're really uncertain about the correct result, we admit it and return "don't know".

The cost function could e.g. be the 0-1 loss for classification.  The cost function of course can be asymmetric, i.e. in case of binary classification different costs for a false positive than a false negative.  For example diagnosing an healthy person as having a deadly illness is less severe than diagnosing an ill person as healthy.

Example: Bayesian optimal decision for logistic regression: p(y|x) is estimated by p̂(y|x) = Ber(y|sigm(w^T^x)). The set of actions is given by predicting a class, i.e. 𝓐 = {+1, -1}.  The cost function is the zero one loss, i.e. C(y,a) = l~0/1~(a, y).  Plugging into the formula and some math yields that the best action is the most likely class, i.e. a^∗^ = signʹ(w^T^x).


[[curse_of_dimensionality]]
=== Curse of dimensionality

In general, adding additional signal predictors that are highly associated with the response will improve the fitted model in terms of decreasing test MSE.  However even if they are associated, the increase in variance might outweight the reduction in bias.  Adding noise predictors that are not truly associated with the response will detoriate the fitted model, because they increase the dimensionality of the problem, exacerbating the risk of overfitting, since noise features may be assigned nonzero coefficients due to chance.

Models with no interaction suffer only little from curse of dimensionality, because they fit a predictor at a time, and when fitting a single predictor, they only `look in one direction' in the p-dimensional space.

KNN suffers more than linear regression or GAM with splines. KNN looks in all directions in the p-dimensional space at the same time.

See also respective paragraph in <<KNN>>.

References:

- Pattern Recognition and Machine Learning, chapter ``1.4 The curse of Dimensionality''.


=== Missing data

Reasons for missing data: Respectvie data might be difficult / expensive / impossible to acquire.  Problems in sensor, network, storage.



== Measuring model performance & Co

=== Formula table

|=====
| t~i~ > 3 | Rule of thumb for identifying outliners
| p~ii~ > 2p̄ or p~ii~ > 3p̄  | Rule of thumb for identifying high-leverage data points, where p~ii~ is a diagonal cell in the projection matrix P and p̄=p/n is the mean leverage value.
| D~i~ = 1/p · t²~i~ · (P~ii~/(1-P~ii~)) = 1/(pσ̂²) · ∑~j~(ŷ~j~-ŷ~j(i)~)² | Cook's distance of i-th observation.  P is the projection matrix.  ŷ~j(i)~ excludes the i-th row.
| D~i~ > 1 or D~i~ > 4/n | Rules of thumb for identifying influencial data points.
| TSS = ∑(y~i~ - ȳ)² | Total sum of squares. ȳ is the sample mean, see there.
| RSS = ∑e²~i~ | Residual sum of squares
| RSE = √(RSS/(n-p)) | Residual standard error
| Var̂[ε~i~] = RSE | Common estimator for Var[ε~i~] = σ
| (unadjusted) R² = (TSS - RSS) / TSS = 1 - RSS/TSS |
| adjusted R² = 1 - (RSS/(n-p)) / (TSS/(n-1)) |
| trainingMSE = RSS/n |
|=====


=== Comparison of model performance measures

See <<comparison_of_model_selection_methods>>


=== Assess model performance

The _performance_ of a model is a measure of how `good' a model models a given population, most often in respect to its predictive power, i.e. its prediction capability on test data. _test data_ is independent unseen data. More formaly, we compute a test error on basis of some loss function, the respective terms being defined in the following.

_loss function_ l(ŷ,y) (or l(θ, x, y)): Measures how different the prediction ŷ = h~θ~(x) is from the true response y.  I.e. quantifies the loss of a single observation.

Similarily a _loss matrix_ L~k,j~ is often used in a classification setting, where k denotes the true class and j denotes the estimated class.

Conversely, some authors consider instead a _utility function_, whose value they aim to maximize.  These are equivalent concepts if we take the utilitiy to be simply  the negative of the loss.

[[01_loss]]
_0/1 loss function_: Is 1 for a mistake, 0 otherwise. But is a nasty function to optimize. Is not continous, and is not convex.  Thus e.g. the gradient descent algorithm cannot be used to optimize.  If ŵ was argmin~w~[∑loss~0/1~(w,y~i~,x~i~)], the problem of finding the optimal solution would be NP hard.

loss~0/1~(w, y, x) = (y != signʹ(w^T^x)) = (yw^T^x < 0)

_0/1 loss function with no-decision extension_: For classification problems where the prediction can not only be one of the K classes, but also `no decision'. Is as the normal 0/1 loss, only that when the prediction is `no decision', then a d is returned, where d is a parameter.

_prediction error_: l(ŷ,y) = y-ŷ. References: The Elements of Statistical Learning p18 (prediction error, only indirectly defined), An Introduction to Statistical Learning p30 (prediction error, only indirectly defined)

_squared loss_: l(ŷ,y) = (ŷ - y)². I.e. squared prediction error.

_cost function_ Q (or L): Is a more general concept than the loss function. A loss function is w.r.t. one observation.  A cost function might be w.r.t. all the data, e.g. by summing a loss function over all the data.  A loss function is also a cost function.

_consistent_ model selection criterion: A model selection criterion is consistent, if when used for model selection, the probability of selecting the true model approaches asymptotically (in n) one.  Only applicable when the true model is member of the candidate models.

_expected risk_ (or _total expected risk_, or _true risk_, or just _risk_, or _expected error_, or _expected loss_) R(h) = E~y∈𝓨,x∈𝓧~[l(h(x),y)] = ∫~y∈𝓨~∫~x∈𝓧~l(h(x),y)p(x,y)dxdy: The risk associated with a given hypothesis h(·) is defined as the expected loss. The expected risk generally cannot be computed since in general p(x,y) is unknown. References: https://en.wikipedia.org/wiki/Empirical_risk_minimization (risk), aml2018 lecture slides 2 page 5/31 (total expected risk), https://en.wikipedia.org/wiki/Generalization_error (expected error), https://en.wikipedia.org/wiki/Statistical_learning_theory (expected risk), Machine Learning: a Probabilistic Perspective p 195,205 (expected loss, risk)

_empirical risk_ (or _empirical error_) R̂~𝓓~(h) = E~(y,x)∈𝓓~[l(h(x),y)] = 1/n ∑~(y,x)∈𝓓~l(h(x),y). Since p(x,y) is often unknown, the expected risk R(h) cannot be computed. We can estimate the risk by averaging the loss on a sample data set 𝓓. Assuming iid observations, R̂~𝓓~(h)→R(h) for |𝓓|→∞ (by law of large numbers). References: https://en.wikipedia.org/wiki/Generalization_error (empirical error), https://en.wikipedia.org/wiki/Statistical_learning_theory (empirical risk), Machine Learning: a Probabilistic Perspective p 205 (empirical risk)

_training error_ R̂~trainset~(ĥ): Empirical risk of a concrete trained model ĥ on basis of the training set. In general, E~train~[R̂~trainset~(ĥ)] < R(ĥ), i.e. the estimator R̂~trainset~(ĥ) is an overly optimistic estimator; it underestimates the true risk R(ĥ).  The reason is that the training data can be fitted better than unseen data.  Thus never evaluate a model by using the training error. References: aml2018 lecture slides 2 page 6/31 (training error), All of statistics p 219 (training error (however without the 1/n normalization))

_test error_ (or _generalization error_ (conflicting with another defintion)) R̂~testset~(ĥ): Empirical risk of a concrete trained model ĥ on basis of the test set (aka unseen data).  An unbiased estimator for the true risk, i.e. E~train~[R̂~testset~(ĥ)] = E[R(ĥ)], as long as the test data was not involved in any way in fitting ĥ. References: aml2018 lecture slides 2 page 7/31 (test error), An introduction to statistical learning p37 (test error), The Elements of Statistical Learning p220 (test error, generalization error; but unclear whether its over test sample or over population), Machine Learning: a Probabilistic Perspective p 23 (generalization error)

Apparently the test error is often taken to be synonymous to expected error (maybe implying that the (true) test error is based on an infitely large test set). E.g. https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/cv_boot.pdf slide 8

[[expected_test_error]]
The _expected test error_ (or _expected prediction error_) is the expected loss of a model `template' (i.e. not yet trained), when it is trained multiple times and each trained model is tested.  With respect to test error, now also the training sample is choosen at random from the population. References: An introduction to statistical learning p 34 (indirectly via expected test MSE), The Elements of Statistical Learning p 220 (expected test error, expected prediction error)

ExpectedTestError = E~train~[Err~test~]

The __expected test error at new x__ is analogous, but here we only look at a fixed, new x.  Recall that the corresponding true response Y is a random variable.

ExpectedTestError(x) = E~train~[E~Y~[l(Y,f̂(x))]]

_MSE_: see <<MSE_of_estimator>>

_training MSE_ RSS/n: I.e. training error on basis of squared loss.

_test MSE_: Test error on basis of squared loss.

_expected test MSE_: Expected test error on basis of squared loss.

_generalization gap_ (or _generalization error_ (arguably a non-standard and conflicting usage of the therm)) gen_gap~𝓓~(h) = R(h) - R̂~𝓓~(h). An algorithm is said to _generalize_ if lim~|𝓓|→∞~gen_gap~𝓓~(h) = 0.  In general the generalization gap cannot be computed since the risk R(h) in general cannot be computed. Instead, the aim is often to bound the generalization gap in probability: p(|gen_gap~𝓓~(h)|≤ε) ≥ 1-δ, where ε is called the error bound and is generally dependent on δ and n = |𝓓|. References: https://en.wikipedia.org/wiki/Generalization_error (generalization error), https://www.reddit.com/r/MachineLearning/comments/4zu08d/definition_of_generalization_error/ (generalization gap; argues that using `generalization error' is non-standard)

The _residual sum of squares_ (_RSS_) (or _error sum of squares_ (_SSE_)) is defined as RSS = |Y-Ŷ|² = |e|² = ∑~1≤i≤n~e²~i~.  Can be thought of as the amount of variability that is left unexplained after performing the regression.  Is a training error.  When LS fit was used, decreases monotonically as more predictors are added to the model (current model is optimal with respect to RSS; if an predictor is added that cannot reduce the RSS, then the respective coefficent is set to zero).  n-p degrees of freedem are associated with RSS.

RSS̃ means the same as RSS but with <<standardize_variables>>.

**to-do**(3) If the true model is linear, and the training set is infinitely large, then β̂ = β and e = ε.  I.e. altough e is non-zero, it is fully explained.  So we should say (RSS-|ε|²) is the amount of variability that is left unexplained after performing the regression?

The _explained sum of squares_ (_ESS_ or _SSE_) (or _model sum of squares_ or _sum of quares due to regression_ (_SSR_)) is defined as ESS = |Ŷ-Ȳ|².  ESS can be thought of as the amount of variability in the response that is explained by performing the regression.  Is a training error.  p-1 degree of freedeom are associated with ESS.

The _total sum of squares_ (or _TSS_ or _SST_) is defined as TSS = |Y-Ȳ|² = RSS + ESS = ∑~1≤i≤n~(y~i~-ȳ)².  Can be thought of as the amount of variability inherent in the response before the regression is performed.  Is a training error.  n-1 degrees of freedom are associated with TSS.

The _residual standard error_ (or _RSE_) is given by RSE = √(RSS/(n-p)).  The RSE is considered an absolute measure of the lack of fit of the model to the data.  Roughly speaking RSE is the average amount that the response will deviate from the true regression hyperplane.  Even if the model were absolutely correct and the parameters of the model were known exactly, any prediction Ŷ is still off by RSE.  RSE is often used as an estimator for the variance Var[ε~i~] ≈ σ̂² = RSE² of the error terms ε~i~.  Is a training error.

*to-do* derive given formula from √Var[e] or whatever is the basis, and then write RSE = √Var[e] = √(RSS/(n-p)).

The __R²__ (or _coefficient of determination_) statistic is defined as R² = (TSS - RSS) / TSS = 1 - RSS/TSS = ESS / TSS.  R² measures the proportion of variability in the response that can be explained by our model.  R² ∈ [0,1], 1 meaning good, 0 meaning bad.  Small values might occur becaus the used model (e.g. linear) is wrong or the inherent error σ² is high, or both.  The advantage of R² over RSE is that the former is relative (lies in [0,1]) and the later is absolute.  Is a training error.  Increases monotonically as more predictors are added, see also RSS.

The __adjusted R²__ statistic is defined as 1 - (RSS/(n-p)) / (TSS/(n-1)).  Adjusted R² is not as well motivated in statistical theory as AIC / BIC / C~p~, but is popular in practice.  Increases only when non-noise predictor is added.  Intuition:  Look at the nominator (RSS/(n-p)):  Adding noise predictors will lead to only a very small decrease in RSS, but the larger p will make the whole nominator smaller, ... *to-do*. i.e. adjus  Is an adjusted training error. *to-do* but an estimate for test error

**to-do**(3) I still don't get the difference between adjusted R² and R². What are pros / cons?

__Mallows's C~p~__ has multiple equivalent (in the sense that they select the same model) definitions, see below.  σ̂ is often estimated by the MSE of the full model.  Used to compare models for model selection.  The second summand can be viewed as penalty for model complexity, just as in regularization.  References: https://www.jstor.org/stable/2348411?seq=1#metadata_info_tab_contents[The interpretation of Mallows Cp statistic], All of statistics p 219, https://en.wikipedia.org/wiki/Mallows%27s_Cp, An introduction to statistical learning p 211

[1] C~p~ = RSS/σ̂² + (2p-n) +
[2] C~p~ = 1/n (RSS + 2·p·σ̂²) [σ̂² is unbiased ⇒ C~p~ unbiased w.r.t. test MSE] +
[3] C~p~ = trainingerror + 2pσ̂²

[[BIC]]
The _Bayesian Information Criterion_ (_BIC_) is BIC = -2ln(ℒ~𝓓~(θ̂^MLE^)) + k·ln(n), where ℒ~𝓓~(θ) = p(𝓓|θ) denotes the likelihood, θ̂^MLE^ denotes the maximum likelihood estimate of parameters, and k = |θ| the number of parameters.  The first summand can be viewed as cost function, and the second summand as penalty for model complexity, just as in regularization.  In case of linear regression: BIC = 1/n (RSS + log(n)·p·σ̂²).  Is an adjusted training error.  Used to compare models for model selection where each candidate model has an parameter space Θ; the model with the lowest BIC wins.  Is a consistent model selection criterion.  Tends to be overly pessimistic for small n due to the ln(n) in the model complexity penalty term.  Tends to select too simple models (i.e. underfit) for small n.

[[AIC]]
The _Akaike Information Criterion_ (_AIC_) is AIC = -2ln(ℒ~𝓓~(θ̂^MLE^)) + 2k, where ℒ~𝓓~(·), θ̂^MLE^ and k are as in BIC.  AIC is almost as the BIC, only that the model complexity penalty term is simply 2k opposed to k·ln(n). In the case of linear regression: AIC = 1/nσ̂² (RSS + 2·p·σ̂²).  Used to compare models for model selection where each candidate model has an parameter space Θ; the model with the lowest AIC wins.  Asymptotically equivalent to LOOCV.  AIC was designed such that when doing model selection on basis of AIC, the winning model is the one that miminimizes the KL divergence to the true model.  Tends to select too complex models (i.e. overfit) for small n. References: All of statistics p220 (AIC = ln(ℒ~𝓓~(θ̂^MLE^)) + k), An Introduction to statistical learning p 212 (case linear regression: AIC = 1/nσ̂² (RSS + 2·p·σ̂²)), https://en.wikipedia.org/wiki/Akaike_information_criterion, aml2018 lecture slides 6 slide 34

*to-do*: "Asymptotically equivalent to LOOCV". I assume here CV uses MSE as cost function? Also, asymptotically is w.r.t. sample size, right?

[[TIC]]
_Takeuchi Information Criterion_ (_TIC_) is TIC = -2ln(ℒ~𝓓~(θ̂^MLE^)) + 2trace[I·J^-1^], where I = Var[∇ℒ~𝓓~(θ̂^MLE^)], J = -E[∇²ℒ~𝓓~(θ̂^MLE^)], and where ℒ~𝓓~(·) and θ̂^MLE^ are as in BIC.  Tries to fix AIC's problem that AIC only works properly if the true model is in the model class.  Reduces to AIC if the true model is a member of the model class.

Recall: The notation E~train~[·], Var~train~[·], E~test~[·], Var~test~[·] etc. is defined by <<trainingsampe_testsample_notation>>.

The __bias of estimate f̂__ with respect to regression function f __at x~0~__ is defined analogous to the bias for an estimator θ̂, see there. f̂(x~0~) is a random variable since f̂, i.e. its parameters, depends on the random training data.

Bias~train~[f̂(x~0~)] = E~train~[f̂(x~0~)] - f(x~0~)

Bias~train,test~[f̂] = E~test~[Bias~train~[f̂(x~test~)]]

*to-do* how is variance of f̂ formally defined?


[[assess_model_coefficients]]
=== Assess model coefficients

The _bias of model parameter estimator β̂_ is defined the usual way the bias of an estimator is defined: Bias~train~[β̂] = E~train~[β̂] - β

_t-statstic_ for an estimator β̂ of unknown parameter β: see <<t_statistic>>.

In a linear regression model the coefficents β̂ found by OLS are <<BLUE>>.


=== Assess data points

An _outlier_ is a data point for which its response y~i~ is unusual by being far from the value predicted by the model. Alternatively: A data point with large studentized residual.  Observations whose studentized residuals are greater than 3 in absolute value are possible outliers [ISLR chapter 3.3.3 Potential Problems, Section 4. Outliers].  In linear regression, typically an outlier has only a small influence on the regression hyper-plane.  However it may have a big influence on RSE and R².  And since RSE is used as estimator for σ, also a big influence on confidence intervals and p-values, i.e. a big influence on the interpretation of such a fit (*to-do* are those statements restrictued to linear regression?).

A data point with high _leverage_ is one for which its predictor is unusual by being far away from the mean of the predictors.  Regarding linear regression, given projection matrix P, the leverages are defined as diag(P). Recall that P (also denoted H) only depends on the predictors X, and that Ŷ = PY, i.e. ŷ~i~ = p~i1~y~1~ + ... + p~ii~y~i~ + ... + p~in~y~n~.  You see from this formula that the leverage p~ii~ quantifies the influence the response y~i~ has on its predicted value ŷ~i~.  When having a high-leverage data point, the lack of neighboring predictors means that the fitted regression model will pass close to that particular observation.  As a rule of thumb, a leverage value greater than 2p̄ (other authors say 3p̄) is considered large, where p̄=p/n is the mean leverage value.

*to-do* I only understand that for simple linear regression, but not confidentaly for multiple linear regression. Is figuratively `far away' the Eucledian distance in ℝ^p^?. ISLR has an example: Figgure 3.13, middle plot, p.98. Has the red predictor higher leverage than the predictors at the right/left border of the ellipse? Its closer in Eucledian distance to the center/mean.

*to-do* The notion of outlier seems to be applicable not only to linear regression, but the notion of leverage seems only to be applicable to linear regression. Correct? Why this asymmetry? If not, what is the general definition / formula for leverage?

*to-do* There seem to be two intuitions for leverage: "a measure how far of a predictor is from the predictor mean" and "a measure of the influence of a response on its predicted value", and I cant bring them together in my head

**to-do**(4) I don't understand how high leverage by itself is a problem.  If I have high leverage but a tiny outlineingless, at least in linear rergession with OLS nothing bad at all happens, no?

[[influencial_data_point]]
An _influencial data point_ is one whose deletion would noticeably change the calculation. In particular, in regression analysis it has a large effect on the parameter estimates. In other words, a measure of how influencial a data point is, is a measure of the effect of deleting that data point.  One possible measure is the <<cooks_distance>>.  Note that outliers and high-leverage data points have the potential to be influencial, but they not necessarily are influencial. For models with two parameters, a possible way to visually identify influencial data points is to do n `experiments', each removing the i-th data point and then fit the model using the remaining data points, and then draw a scatter plot of the two optained parameters of each `experiment'  (e.g. β~0~ on the x axis and β~1~ on the y axis).  All points should be close together.  References: https://onlinecourses.science.psu.edu/stat501/node/337.

*to-do* If an data point both is an outlier and has high-leverage, is it guaranteed to be influencial or only very likely to be influencial? According to cooks distance, it is guaranteed to be influencial, no?

[[cooks_distance]]
The _Cook's distance_ D~i~ is a commonly used estimate of the <<influencial_data_point,influence>> of the i-th data point when performing least-squares regression analysis.  In an OLS analysis it can also be used to indicate regions of the design space where it would be good to obtain more data points.  The Cook's distance is defined as D~i~ = 1/p · t²~i~ · (P~ii~/(1-P~ii~)) = 1/(pσ̂²) · ∑~j~(ŷ~j~-ŷ~j(i)~)², where t~i~ is the i-th studentized residual, and ŷ~j(i)~ excludes the i-th row.  If the `outlineniness' (middle term t²~i~) is high and the leverage (last term) is large then the Cook's distance is large and thus the data point is deemed influencial.  Thresholds for identifying highly influential data points are controversal.  One is D~i~ > 1, another is D~i~ > 4/n. References: https://onlinecourses.science.psu.edu/stat501/node/340

*to-do* I still don't understand the summation definition. Why is the nominator not mostly zero?

*to-do* Has a given concrete value of the Cook's distance an interpretation, or is it just qualitative, large is bad, small is good? In the later case, why square studentized residual and why not simply use leverage P~ii~).


[[model_selection]]
== Model selection / feature selection

_Model selection_ (or _model tuning_) is the task of selecting a statistical model from a set of candiate models.  That may include determining the hyper-parameters of the choosen model and it may include determining the tuning parameters of the learning algorithm. See also <<cv_for_model_selection>>.

Whether or not model selection shall additionally also train the selected model is not clearly defined.

Model selection is typically done by computing the empirical risk of a candidate model on _validation data_.

[[occams_razor]]
_Occam's razor_: Choose the simplest model that explains the data.

_Feature selection_ (or _variable selection_, _attribute selection_, _variable subset selection_) is the task of selecting a subset of the available features (aka predictors).

Rational for feature selection

- Better generalization: Simpler models usually generalize better (i.e. overfit less)

- Interpretability: A model with fewer features is simpler and easier to understand

- Costs: Don't need to acquire respective data, less storage costs, less computational costs.

_Model validation_ (or _model assessment_ or _assessing performance (of a model)_) is the task of estimating the prediction error of a model.  For a meaningfull model validation we also need to calculate the bias and the variance of the estimate.  Recall that in general we can't compute the true prediction error, since that would require us to know the true joint distribution p(x,y) of the data.

The following solutions do not work in general:

Use previously unseen data for model validation or model selection::
It's not realistic to have further unseen data available.  Even so, in that case we could combine the original data and the newly available data into one data set, and be logically at the same point as in the beginning of the problem statement.

Use original data for model validation or model selection::
When we use the original data for model assessment, the retreived estimated prediction error will be heavily biased; it will be too good.  That's because we trained the model with exactly the same data as we measured the model's performance with.  It was the model trainings job to fit the model to the original data, so obviously the model will have a high performance on the original data.


[[inference_after_model_selection]]
=== Inference after model / feature selection

Say we do best subset selection, and then look at the p-values of the resulting coefficent estimates.  These p-values will be rather low, even if the data is random noise.  After all, the task of the algorithm was to select the best predictors, so obviously their p-values are among the best possible given the data.  The algorithm did data snooping. See also selection effect.

**to-do**(2) Isn't the real cause of the problem that we have not enough data per predictor, such that we can't distinguish between true signal (the true model/curve) and noise (variance of error terms)?  Then by chance, for one axis, all data points really are close to some slope, opposed to randomly around the plane at zero.  In Rcode11.R, if I increase n, the problem (finding significant coefficients) goes away. See also similar next question.

**to-do**(2) See exercise series 9, 2b. Under global null, the distribution of the p-value for a given coefficient, say for x~1~, is uniform. I am missing the intuitive explanation. Intuitively I would say if sample size n is much larger than p, it should be much much more unlikely that a p-value is significant. See above question.

Analogy: Take a sample from a normal distribution. Take the max of the sample. Compute the p-value of the max (with respect to the original normal distribution). Of course it will be rather small, probably less than 0.05, i.e. significant.  What happened was kind of hidden multiple testing: testing the max is like testing every observation, because we need to look at all obvservations to find the max.  When doing this whole thing multiple times, and then drawing the empirical distribution of all these maxes, it will be a bell-shaped curved clearly shifted to the right relative to the normal distribution it is based upon.

**to-do**(2) Is this also data snooping. if so, interlink or combine the two

_sample splitting_: One simple way to deal with the problem is to split the data.  With one part do the feature selection, and with the other do the fit and measurement of p-values for coefficient estimates.  These p-values will be `honest'.  Disadvantages are reproducability issues due to the random split, and loss of power since we only use half of the training data (see also validation set approach).


[[comparison_of_model_selection_methods]]
=== Comparison of model selection methods

Best/forward/back subset selection work for any prediction method, but are usually relatively slow.

L1-regularization is faster since training and feature selection happen simultaneously. But it only works for linear models.

Crossvalidation has small bias (the smaller the smaller the fold size, negectible for LOOCV), but non-small variance (the larger the smaller the fold size).

Bootstrap has small variance but non-small bias. Bias can e.g. be reduced via the Jacknife, which however then increases variance.

The advantage of doing model selection by selecting the one having the best criterion (e.g. AIC, BIC) opposed to selecting the one having the best cross validated cost is that the former is usually computationally cheaper and can use all the available data for training.  However certain adjusted training errors, such as AIC or BIC, rely on lekelihood optimization which can limit applicability.

All of Mallow's C~p~, AIC, BIC are asymptotically (for large n) good in the sense that the best model out of a set will have the highest Mallow's C~p~ from all models, the highest AIC from all models etc.

BIC vs AIC: As a rule of thumb, use BIC when you have a lot of data and AIC otherwise.

**to-do**(3) Adjusted training error vs estimated expected test error. When can I reasonably use adusted training error? E.g. in best subset, when can I use adjusted training error opposed to having to pay CV?


[[best_subset_selection]]
=== Best subset selection

Basic idea: Try all 2^p^ different models.

--------------------------------------------------
1: for k=0 to p
2:     M[k] = best model out of the C(p k) ways of choosing predictors
3: choose best model out of M
--------------------------------------------------

How to do all this exactly is left undefined. Also what `best' means on line 2 and what `best' means on line 3 is left undefined. For example in on line 2 we fit all C(p k) candidate models on the training set and pick the one having the smallest RSS (or equivalently largest R²). On line 3 we find the best model among the candidates M using some technique which estimates the prediction error. For example AIC, C~p~, AIC, BIC or adjusted R², computed on the training set. Or we use cross-validation to estimate the prediction error. Recall that the training error always decreases when predictors are added (more generally, with more flexible models), and thus training errors such as RSS or R² can't be used on line 3.

**to-do**(4) Why use RSS or R² in the innermost loop? why not exptectedTestMSE or so? I suspect due to computational complexity. But say I don't mind. Other reasons?

**to-do**(4) Why break problem of choosing among 2^p^ models into these two stages. Why not simply cross validation on 2^p^ models. _only_ because it would be computational expensive?

An alternative algorithm based on the idea that the prediction error is U shaped, see bias variance tradeoff.

--------------------------------------------------
1: do
2:     find best model out of the C(p k) ways of choosing predictors
3:     estimate prediction error of that best model
4: while estimated prediction error decreases
--------------------------------------------------

Pro: Simple

Con: Computational expensive. Not feasible for larger p (say 30-40).

Con: When p gets larger, the search space gets larger,  and thus there's the risk of overfitting and high variance of the coefficient estimates.

See also <<best_subset_selection_as_shrinkage>>

References:

- Book ``An introduction to statistical learning'', chapter ``6.1 Subset Selection''


=== Forward stepwise selection / greedy feature selection

A greedy approach.  Similar to best subset selection, but we iteratively improve the best model so far by greedely adding the best next predictor.  So in step 1 (see best subset selection), in iteration k, we have model M~k-1~ in our hand and try out p-k predictors.  The best predictor we add, resulting in model M~k~.

Instead 2^p^, we now only try out 1 + ∑~0≤k≤p-1~(p-k) = 1 + p(p+1)/2 models.

Forward stepwise selection tends to do well in practice.  However its not guaranteed to find the best model.  For instance, suppose that in a given data set with p = 3 predictors, the best possible one-variable model contains X~1~, and the best possible two-variable model instead contains X~2~ and X~3~.  Then forward stepwise selection fails because in the first iteration it chooses X~1~, and can't undo that decision in the later iterations.

Pro: Usually faster than backward stepwise selection (if few relevant features)


=== Backward stepwise selection

As forward stepwise selection, only that we start out with the full model, and iteratively remove the least useful predictor.

Con: Can't be used when n < p, because we need to start with the full model

Pro: Can handle dependent features

*to-do* but we still can start with the biggest possible model.  I would still think that one would call this approach backward stepwise selection, no?


=== L1-regression to encourage sparsity

Best subset selection is computationally expensive. <<lasso_regression>> can be viewed as an computationally less expensive approximation to best subset selection with loss function being the quadratic loss.  See also <<comparison_of_regularization_methods>>.  Using L1-regularization, as the Lasso does, can also be used in other methods, e.g. SVM.


=== Hybrid feature selection approaches

First greedely add predictors with the forward stepwise selection method, then remove predictors that no longer provide and improvement with the backward stepwise selection method.


=== Bayesian model selection

Method for model selection.  Given a set 𝓜 of candidate models. First compute posterior over models:

p(m|𝓓) = p(𝓓|m)p(m)/p(𝓓) +
p(𝓓) = ∑~m∈𝓜~p(m,𝓓)

From this we can compute the MAP model

m̂ = argmax~m~(p(m|𝓓))


[[validation_set_approach]]
=== Validation set approach

The _validation set approach_ is a technique for model assessment or model selection.

Partition data randomly in two equaly sized partitions, one constituting the training data and the other constituting the validation data.  Train an estimate f̂ using the training data, and then calculate an estimate of the prediction error using the validation data.  Note that the validation set approach does _not_ correspond to 2-fold cross validation; in 2-fold cross validation, each split would be once test set and once validation set.

Pro: Simple

Pro: Fair estimate of prediction error, i.e. closer to prediction error than to (adjusted) training error.

Contra: Loss of power: Fewer training data always always means a worse fit of the model.  In particular it typically means more bias.  In other words, it's too pessimistic: we get a biased (too high) estimate of the prediction error.

Contra: Large variance of the validation estimate (e.g. estimated expected test MSE) because the validation estimate might depend a lot on how the partition turned out to be.

References:

- Book ``An introduction to statistical learning'', chapter ``5.1.1 The Validation Set Approach''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4.3.1 Leave-one-out cross-validation''


[[cross_validation]]
=== Cross Validation

!!!!! clean up usage of terms `test set/sample/...' vs `validation set/sample/...'.

_Cross-validation_ (or _rotation estimation_) is a coarse technique for model assessment _or_ model selection.  It's a coarse technique in the sense that it has multiple more concrete instanciations, such as <<LOOCV>> and <<k_fold_CV>>.  It tries to mitigate the problem of only having one data set (aka sample of the population) of finite size, altough we actually would like infinitely many training sets and test sets, each set of infinite size.

_CV for model validation_: Repeatedly partition the original data set into a training set and a validation set, each time doing the partition in a different way.  In each iteration, calculate an estimate of the performance of the model using the training set and the validation set of that iteration. At the end, average the results, delivering the (final) estimate of the model's performance.

*to-do* but then we didn't really measure a concrete trained model. It's more a measure of the way we train, not a measure of a concrete trained model.

mind that we estimate expected test mse, for the true one we need the population

we are a bit sloppy and take the number of CV for both the model and the estimate f

!!! we also need to compute mean and bias

--------------------------------------------------
estimated_prediction_error_by_CV(model, data)
    errors = emptycollection()
    for trainset, validationset in partition(data)
       model.train(trainset)
       errors.append(loss(model,validationset))
    return mean(errors)
--------------------------------------------------

[[cv_for_model_selection]]
_CV for model selection_: See also <<model_selection>>. Delivers best model (in sense of template, not trained), but you don't know yet how good it is on unseen data.  Analogous to CV for model validation, but do it for each candidate model.  I.e. there are two nested loops, an outer doing different partitions, and and an inner trying all candidate models (that is three nested loops in case of double cross validation).  At the end, we select the candidate model with the best averaged estimate of the performance.  As noted in chapter model selection, whether or not the selected model shall also be trained is not clearly defined.  If we want to train, we can train it on the complete original data set, or just take the already trained model which was trained on a subset.  The former is computationally more expensive, but larger training sets are generally better.  Note that we cannot use the optained performance estimates also for model validation.  For that use <<double_cross_validation>>. Alternatively, put aside a _test set_ which is used to report the final performance of the concrete trained model.  However you must never use the test set to derive any decision, more specifically it must not be used during trainining a model or within CV to decide which model to pick.  Also after doing CV and then after evaluating the winning concrete fitted model's performance with the test set, don't change your mind and choose another model.

--------------------------------------------------
best_model_by_CV(candidatemodels, data)
    best_error = infinity, best_model = NIL
    for model in candidatemodels
        error = estimated_prediction_error_by_CV(model, data)
        best_model = model if error < best_error
    return best_model
    # note that we don't know estimated prediction error of returned model
--------------------------------------------------

*to-do* Chapter/paragraph about choosing a statistic for model performance. e.g. why exactly is training MSE not good.

*to-do* compare and contrasts these terms: (statistical) learning method, model, estimate f̂ of f, systematic information f. More usually used terms, especially for f̂ and f?

*to-do* Is all of the this chapter correct, inclusive following subchapters? Be picky!

See also <<oob_error_estimation>>, which computes an estimate for the test error for bagged trees.

pro: The estimate is nearly unbiased w.r.t the true risk.  It is slightly too pessimistic since it doesn't use all the data for training. It slightly underfits, since we don't use all the data we have for training.  As always, the less training data we have, the larger the tendency of underfitting.  So we want want to make the training data large by making one fold small, in the extreme leading to LOOCV (which then has minimal bias but more variance).

con: The variance of the finaly resulting empirical risk (i.e. the average validation of each candidate model) is quite large, especially for small folds.  Variance typically increases in an inverse exponential manner as fold sizes get smaller.  Variance is large due to highly correlated training sets.

*to-do* better understand why exactly variance is large. https://stats.stackexchange.com/questions/61783/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation

*to-do* both variance and bias have an exponential shape. I.e. yes it gets worse/better, but after a while only insignificanly

Contra: The smaller the fold size, the more computationally expensive, especially for large n.  However for some models, e.g. for linear regression, there are short cuts for LOOCV.

References:

- Book ``An introduction to statistical learning'', chapter ``5.1 Cross-Validation''

- Book ``All of statistics'', chapter ``13.6 Model Selection''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4 Cross-Validation''


[[double_cross_validation]]
=== Double cross validation

_Double cross validation_ (or _nested cross validation_) delivers you expected estimated test error of the following procedure (i.e. how good the model resulting from the procedure performs on unseen data). The procedure is to use cross validation to select the best model.

Note that we cannot do model selection with cross validation and then use the same data set for model assessment, because the assessment would test a model with data that the model already has seen.  We would thus get a biased, i.e. too optimistic, i.e. too small estimated expected test error.

Outer cross validation: `Model' assessment: Repeatedly partition the original data set into a test set and a training-valiation set.  In each of these outer iterations, pass the training-valiation set to the the inner CV which returns a trained model.  Calculate the estimated test error of that trained model using the test set.  At the end, averaging all those delivers the estimated expected test error of the precedure.  Recall that each trained model the outer layer sees was trained with slightly different training data than every other, thus overall we get this E~train,test~[...] the estimated expected test error demands.

Inner cross validation: Model selection: Do normal model selection via cross validation on the training-validation set received from the outer CV.  Return the trained selected model received from model selection.

Note that different inner cross validations may select different models.  That's ok, since as said at the beginning of this chapter, we asses the performance of the procedure, not of some model.


[[LOOCV]]
=== Leave one out cross validation (LOOCV)

Equals n-fold cross validation.  Do n iterations. In each iteration, make the i-th observation the test set, and the rest the training set. Let f̂^(-i)^ denote the estimate of the model fitted with training data being the original data without the i-th observation.

expectedTestMSÊ = 1/n ∑(y~i~ - f̂^(-i)^(x~i~))²

In linear regression, that simplifies to *to-do*

expectedTestMSÊ = 1/n ∑(e~i~/(1-diag~i~(P)))², e = residuals, P = projection matrix

Pro: No randomization

References:

- Book ``An introduction to statistical learning'', chapter ``5.1.2 Leave-One-Out Cross-Validation''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4.3.1 Leave-one-out cross-validation''


[[k_fold_CV]]
=== k-fold cross validation

A specialization of cross validation.  Split observations randomly in k equaly sized partitions. k = n results in <<LOOCV>>.

How to choose k? If too small, we don't use enough training data. Risk of underfitting to training set. Risk of overfitting to test set. If too large, there is not really a problem, but it is computationally expensive.  In practice, k=5 or k=10 is often used.

Rule of thumb: k = min(√n,10)

References:

- Book ``An introduction to statistical learning'', chapter ``5.1.3 k-Fold Cross-Validation''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4.3.2 K-fold Cross-Validation''


=== Monte Carlo cross validation

A specialization of cross validation. In each iteration, define training set by uniformily at random assign observations.  The validation set is given by the complement.  The size of the training set is a parameter of the algorithm.


[[bootstrap_model_validation]]
=== Bootstrap for model selection / model validation

_naive bootstrap_: Train a model for each bootstrap sample. Use all the original data to compute the empirical risk corresponding of a model / bootstrap sample. Final emprical risk is the average over all models / bootstrap samples. Contra: Empirical risk is too optimistic since training data was also used for validation.  Contra: Not at all available data is used for training.

*to-do* move to some good place: when all/part of the training data data is also used for validation, then the empirical risk is too optimistic, since an overfitting model gets not `punsihed' by true unseen data.

_leave-one-out boostrap_: Train a model for each bootstrap sample. For each observation in the original training data, we compute an intermediate averaged empirical risk by using that one observation as validation data and averaging the empirical risk over all models which do not have that observation in their training data.  The final empirical risk is the average of these intermediate averaged empirical risks. Contra: Not at all available data is used for training. Thus empirical risk it is biased (too pessimistic).

*to-do* for many (most?) training methods, having a training observation multiple times does not change anything relative to having it only once, no? I think I am wrong in assuming that it doesn't make a difference. E.g. in OLS linear regression, when one observation is repeated tons of times, it has a huge weight relative to the other obvservations. Isn't it the case that for most training methods, repeating the same observation multiple times is approximately equal to having the same amount of very closeby observations.

_.632 bootsrap_: 0.368·err~train~ + 0.632·err~loo boostrap~, where err~train~ denotes the training error on the full original data set.  Tries to mitigate the problem of leave-one-out bootstrap that it doesn't use all the available observations for training.  Tends to work badly in overfitting situations.  Pro: Lower bias than leave-one-out bootstrap.

_bootstrap vs cross-validation_: cross-validation has less bias, bootstsrap has less variance

One can consider using the jacknife to reduce the bias of the boostrap method.


=== Misc model selection techniques

Usually one doesn't use the training error (which is based on some cost function) for model selection, since it generally is an too optimistic estimate of the true risk, since the training's goal was to minimize the training error.  A common method for model selection is cross validation, which however doesn't use all the available data for training, leading to bias which gets larger the larger one fold is.  Another method for model selection is using cost functions for the training error which try to adress the problem that a training error based on a naive cost function tends to overfit the more complex the model is, which in turn leads to an overly optimistic (i.e. biased) training error.  See <<AIC>> or <<BIC>>.

Use a <<likelihood_ratio_test>> to decide between two models.


[[model_training]]
== Model training

_Estimating f by paramtetric methods_: We assume f to be a parameterized function.  The parameters β (or w) of f are called _coefficients_ (or _unknown parameters_).  The problem of estimating f reducues now to computing estimators β̂ for the coefficients β.  For example the linear model has f̂(X) = Xβ, where β is a p ⨯ 1 vector constituting the unknown parameters.  We use the training data to _fit_ (or _train_ or _estimate_) our model, i.e. to compute estimates β̂ of the unknown parameters β.  Phrased as noun, this is called _model fitting_ (or _model training_ or _model estimation_).  There are multiple approaches for fitting the model, the most common approach being (ordinary) <<least_squares>>.

The _training set_ 𝓓 = {(y~i~,x~i~)}~1≤i≤N~ is the set of observations which is used to train the model.

_Generalization_ means the ability to well predict a predicted value ŷ given new input data, despite that we trained with only a finite training set.

_Overfitting_ occurs when the model has too many degrees of freedom. The model is too flexible. The model fits the training data very well, in the extrem case goes through every training data point, but then doesnt generalize well.  The fitted model starts to fit the noise (see also the high variance statements), instead of the underlying true function.  Also the variance of Var~train~[f̂(x~0~)] will be high, i.e. if we train on multiple training sets, yielding multiple f̂, and given some fixed x~0~, the set of values f̂(x~0~) will have a high variance. Bias~train~[f̂(x~0~)] will be low.

_Underfitting_ occurs when the model has too few degrees of freedom. The model is not flexible enough. In an extrem case for example the true function is a polynomial of a higher order degree, but the model is only linear, i.e. we approximate a higher order polynomial with a line. Bias~train~[f̂(x~0~)] will be high, Var~train~[f̂(x~0~)] will be low (i.e. quite insensitive to noise).


=== Gradient descent

An iterative approximation algorithm to find a minimum of some objective function.  If its a convex function, the algorithm will find the global minimum.  In machine learning, the objective function often is the empirical risk R̂(w). Recall that w is in general a vector.

--------------------------------------------------
w = some feasible value
until convergence:
  w -= η~t~∇~w~R̂(w)
--------------------------------------------------

t denotes the current iteration and η~t~ denotes the learning rate of the t-th iteration.

Say we use the same learning rate η for all iterations. If η is too small, it takes a long time until we find the minimum. If η is too large, after some time we overshoot the minimum in each iteration and thus oscillate. A possible class of solutions to this problem is adaptive learning rate. Those are topics in optimization and out of the scope of this document.

Note that iterative approximation approach such as gradient descent might overall still be the method of choice even if a closed form solution exists.  E.g. if the closed form solution is computationally too expensive for the problem at hand.  See respective paragraph in <<least_squares>>.  Also the exact optimum is often not required, we only need a solution that is good enough.  We will have a prediction error anyway, so we only have to ensure that the training error is roughly in the same order.

Convergence criterions:

- gradient is small enough

- the objective function difference between two iterations is small enough


=== Stochastic gradient descent (SGD)

_Stochastic gradient descent_ (or _incremental gradient descent_ or _sequential gradient descent_) is a method of optimization. It is a stochastic approximation of the gradient descent optimization. The objective function to be minimized must have the form of a sum (typically over all observations) Q(w) = ∑Q~i~(w).

*to-do* more concrete algorithm. Variants. The stochastic gradient descent algorithms has variants. The inner loop of iterating over all observations can be replaced by uniformily at random pick only one observation

_mini-batch SGD_ with batch size B is a generalization of _batch SGD_ (B=n) and _online SGD_ (B=1).

Convergence criterions:

- Fix the number of iterations. E.g. 10 epochs, where one epoch is one pass through the whole data set.

- Occassionally compute full objective function / gradient and apply gradient descent a convergence criterion. E.g. every n iterations.

- Monitor objective value on a separate validation set.


Applications:

- Empirical risk minimization. Recall that empirical risk is sum of losses over all observations.

- Computing a MLE. Also here a sum over all observations appears.


=== EM algorithm

An algorithm to solve the optimization problem often arising in unsupervised learning of a generative model, where we have to find p(x|θ), i.e. estimate θ̂ by MLE.

E.g. for gaussian mixture models, where θ = (w,μ,Σ) and p(x|w,μ,Σ) = ∑~1≤j≤K~w~j~𝓝~d~(x;μ~j~,Σ~j~), the non-convex optimization problem is argmax~w,μ,Σ~(∑~1≤i≤n~log∑~1≤j≤K~w~j~𝓝~d~(x|μ~j~,Σ~j~)), subject to further constraints.  However if we would know w, i.e. if we would know the labels, it has a closed form solution, see GBC.

_Hard EM_ algorithm. The E-step makes a hard assignment y~i~ to x~i~.

--------------------------------------------------
Initialize θ[0]
for t = 1, 2, ...:
    # E-step: predict labels
    for i = 1 to n:
        y[i][t] = argmax~y~p(y|x[i],θ[t-1])
                  = argmax~y~p(y|θ[t-1])p(x[i]|y,θ[t-1])

    # M-step: compute *M*LE using closed form formula
    θ[t] = argmax~θ~p({x,y[t]}|θ)
--------------------------------------------------

Disadvantages of the hard EM-algorithm: It makes a hard label assignment. However in regions where clusters `collide', that is a too strong decision, we should really estimate a probability for each cluster.


_Soft EM_ (or just _EM_) algorithm. We no longer make an hard assignment of a feature vector x to only one label y, but now try to compute a probability p(y=c|x,θ) for each class c.  Suppose we know p(y|θ) and p(x|y,θ) and θ.  Then γ~c~(x) = p(y=c|x,θ) = p(y=c|θ) · p(x|θ) / (∑~1≤j≤K~p(y=j|θ)p(x|θ)).

*to-do* more details

The EM algorithm can be shown to be equivalent to the following:

E-Step: Compute the expected complete data log likelihood

Q(θ;θ^(t-1)^) = E~z1:n~[cdll|x~1:n~,θ^(t-1)^] = +
∑~1≤i≤n~∑~1≤j≤K~γ~j~(x~i~)log(p(x~i~,y=j|θ)) +
cdll = log p(x~1:n~,z~1:n~|θ) +
γ~j~(x) = p(y=j|x,θ^(t-1)^) [expected sufficient statistics] +
θ^(t-1)^ = best known estimation so far

M-Step:

θ^(t)^ = argmax~θ~(Q(θ,θ^(t-1)^))


In case of semi-supervised learning, γ~c~(x) is known for the observations where we have labels, namely it's 1 for the respective class and 0 for all other classes.  Typically with only very few given labels, the EM algorithm correctly knows which cluster / Gaussian is to be associated with which class.

In genral, the soft EM will typically result in higher likelihood values relative to the hard EM.

One can see Lloyd's heuristic as special case of Hard-EM for GMM. Uniform weights over clusters and assuming identical, spherical covariance matrices.

Number of clusters K can be found by cross-validation.  For GMM that works typically fairly well (which is in contrast to k-means).


[[bagging]]
=== Bootstrap aggregation (bagging)

_Bagging_ is a general method for reducing the variance of a statisticial learning method. It's particularly useful and frequently used in the context of <<decision_trees>>.

Recall that averaging a set of observations reduces variance.  The central limit theorem says that given independent X~1~, ..., X~n~, each with variance σ², the variance of the mean X̄ is σ²/n. The idea is to train B models, each model on its own training set of size n.  The final model is just a wrapper around these B models: it delegates the call to the B submodels, and returns the averaged result.  Thus we need B training sets of size n each.  Since we only have access to one training set, we generate B bootstrap training sets based on the original training set.

_bagging and decision trees_: We train B decision trees.  The trees are grown deep, and are not pruned, thus they have high variance but low bias.  Averaging the results of the trees reduces the variance.  Note that the final model is not a tree anymore, i.e. interpretition drops.  However the trees might be correlated. Averaging correlated quantities does not lead to an as large decrease in variance as we would like. <<random_forests>> try to tackle this problem.

_bagging for classification via majority voting_:  Each of the B models/trees `votes' for one of the K categories.  The categegorie with the most votes wins, i.e. is what is returned by the overall model.  A hint why this works, without giving a formal proof:  If the trees/models, that is classifiers in general, were independent, and each classifier has a missprediction rate of less than 1/2, K=2, the majority vote approaches a perfect classifier (i.e. classifies always correctly) for B→∞.  In reality however the trees are not independent (but see also <<random_forests>>) because all bootstrap samples are derived from one original sample.  Also, if each classifier has a missprediction rate of more than 1/2, the majority vote approaches a perfecty bad qualifier.  Take away: Bagging a good qualifier can improve performance, bagging a bad classifier can degrate performance.

_bagging does not do anything for linear predictors_: Thus bagging should be used for non-linear estimators, e.g. trees.

**to-do**(2) Bagging does nothing for linear predictors: Why does averaging not reduce variance of the mean anymore? The CLT is universal; where is the catch?

[[oob_error_estimation]]
_Out-of_bag (OOB) error estimation_ / _OOB MSE_:  Is an alternative to cross validation to estimate the test error, which is computationally less demanding.  It can be shown that each bootstrap sample on average uses about 2/3 of the observations of the originial sample. The non-used observations are called _out-of-bag observations_.  Given the i-th observation, for every tree in which the i-th observation is OOB, i.e. the observation did not particpate in training the tree, we make a prediction using that tree. This yields about B/3 predictions.  For regression, we average, for classification, we a take majority vote, yielding a single OOB prediction for the i-th observation.  Over all n observations this yields n OOB predictions.  We use them to compute the OOB error, which is an estimate of the test error.  It can be shown that for large B, OOB error is virtually equivalent to LOOCV error.

*to-do* CV estimates the expected test error, but OOB error estimation estimates the test error, so they are not really compareable?

Bagging of decision trees reduces variance, at the cost of lower interpretability. For more pros & cons, see <<comparison_of_tree_based_methods>>.

References:

- ISLR, chapter ``8.2.1 Bagging''


=== Ensemble learning

Do predictions by making a weighted average of the predictions made by candidate models.

When taking a simple (unweighted) average, bias is also averaged, and variance is as follows

Var[f̂(x)] = 1/B²∑~i~Var[f̂~i~(x)] + 1/B²∑∑~i≠j~Cov[f̂~i~(x),f̂~j~(x)]

Thus if the coverainces are very small, the resulting variance of the final model is approximately the averaged variance divided by B.  Consequently, we should strive for putting non-similar models into the candidate set.

Machine learning competitions are often won by ensembles.

Catch phrase: wisdom of crowds.  By averaging, we get rid of oddities.


=== Bayesian model averaging (BMA)

Bayesian model selection tries to selected the best model and then use that to make predictions.  BMA makes predictions by making a weigthed average of the predictions made by each model. The predictive disribution thus is

p(y|x,𝓓) = ∑~m∈𝓜~p(y|x,m,𝓓)p(m|𝓓)

*to-do* be more conrete how to compute the required terms


=== Bayesian learning

The parameter θ is considered a random variable with unknown posterior probability distribution p(θ|𝓓).  The model evidence p(𝓓) is unknown.  The likelihood p(𝓓|θ) and the prior p(θ) are known / guessed.  The goal is to find the probability distribution p(x̃|𝓓) of a new data poiny x̃; note that θ doesn't occur directly in p(x̃|𝓓).

p(x̃|𝓓) = +
∫p(x̃,θ|𝓓)dθ = +
∫p(x̃|θ,𝓓)p(θ|𝓓)dθ =~①~ +
∫p(x̃|θ)p(θ|𝓓)dθ ≈~if ② holds~ +
p(x̃|θ̂)

① p(x̃|θ,𝓓) = p(x̃|θ) since x̃ and any x~i~ ∈ 𝓓 are iid.
② p(θ|𝓓) ~ δ(θ-θ̂)

A prior kind of represents an additional set of observations.  The more narrow a prior distribution is, i.e. the closer it is to a dirac, the more additional observations it represents.  If your prior is very narrow, then it better be correct.  Otherwise you need a huge amount of observations to clearly overrule the bogous `observations' comming from the prior.


[[MLE_vs_bayes]]
MLE vs Bayes estimation

- The results are equal in the asymptotic limit (for reasonable prior distributions)

- Pro MLE: MLE only need differential calculus and can use gradient descent techniques.

- Contra Bayes: Bayes needs a computationally expensive integration.


== Models

=== Comparison of models

Ridge and Lasso are modifications of the linear model.  They make the linear model less flexible, in orer to reduce variance, at the cost of hopefully a small bias.

Nonlinear models like polynomial regression, step functions, regression splines, smoothing splines, local regression, generalized additive models (GAM) increase the flexibility to reduce bias, at the cost of hopefully only a small incease in variance.  Recall however, that when the true model is linar, then there's no bias already.

When the underlying function is smooth in some regions and spiky in other regions, typically ANNs perform better than Gaussian kernels.


[[linear_regression_models]]
=== Linear regression models

In the linear multiple regression model the relation between the response Y and the predictors, i.e. the design matrix X, is linear, i.e.

f(X) = Xβ [stastic notation] +
h(x) = w^T^x [machine learning notation]

Thus the estimated model is f̂(X) = Xβ̂ and thus Ŷ = f̂(X) + ε. Often the first column of X is all ones, which then makes β~0~ the _intercept_: the hyperplane described by f̂ intercepts the y-axis at β~0~.  _Simple linear regression_ is linear regression with only one predictor variable. f(X) = Xβ defines the _population regression hyperplane_, which is the best linear approximation to the true relationship between X and Y (recall that linearity was just an assumption). f̂(X) = Xβ̂ defines the _least square hyperplane_, which is an estimate based on the training data.

The method to fit the model is commonly ordinary <<least_squares>>.  Other methods include <<ridge>> and <<lasso>>.

Assumptions of the linear model:

- E[ε] = 0, i.e. no systemtatic error, i.e. the linear model is the true model. Recall that we have Y = f(X) + ε and that the error terms ε are a catch-all for all we miss with our model, here the linear model f(X) = Xβ. If the true model is not linear, then when using f(X) = Xβ, E[ε] won't be 0.

  * As a consequence, we have E[β̂] = β (i.e. unbiased estimator β̂ of coefficients).

  * If this assumption does not hold, we need other models than the linar model.

  * Check with Tukey-Anscombe Plot

- Predictors X are exact, i.e. observed without erors. **to-do**(3) Isn't that per definition catched by the error terms ε?

  * If this assumption does not hold, we may can use Errors-in-variables models.

- Cov[ε] = σ²I~n×n~, i.e. the errors are uncorrelated and have constant variance. That implies homoscedasticity of the error terms ε.

  * As a consequence we have Cov[β̂] = σ²(X^T^X)^-1^, and from that Var[β̂] = diag(Cov̂[β̂])

  * If the error terms ε are not homoscedastic we may can use weighted least squares.

  * If the error terms ε are correlated we may can use generalized least squares.

  * Check with Tukey-Anscombe Plot

- Error terms ε are jointly normally distributed. I.e. together with the above: {ε~i~|∀i} iid \~ 𝓝(0, σ²).  (*to-do* Since here we say they are independent, doesn't that imply they are also uncorrelated. I.e. the previous point is actually implied by this point? Ok, they are only uncorrelated given they are independent if second moments are finite (sais wikipedia in `uncorrelated random variables'), but is that really the point here?)

  * As a consequence, we have
    ** β̂ \~ 𝓝~p~(E[β̂], Var[β̂])  [which we require for individual t-tests / p-values]
    ** Ŷ \~ 𝓝~n~(E[Ŷ], Var[Ŷ])
    ** e \~ 𝓝~n~(0, Var[e])
    ** σ̂² = RSE² ~ σ²/(n-p) 𝜒²(n-p)

  * If this assumption does not hold, we can rely on the CLT which implies that for large sample size n, the above distributions are still approximately true.  However we may should use robust methods (*to-do* which ones?) instead least squares.

- The matrix has full rank p < n.

  * If that is not fulfilled, then we simply cannot solve the equations delivering β.

*to-do* Is it in the context of this lecture good enough to know these assumptions and their concequences as a whole and also only for linear regression fitted with least squares? If only some asumptions are fulfilled but not others, I find it pretty hairy to properly understand which consequences / formulas still hold wand which doesn't.  Also I never really know which assumptions are relative to linear regression model and which are relative to the least squares fitting method.

_Projection matrix_ (or _influence matrix_ or _hat matrix_), denoted P (or H), is given by P = X(X^T^X)^-1^X^T^. It is decuded from Ŷ = Xβ̂ = X(X^T^X)^-1^X^T^Y = PY.  It is an orthogonal projection on the space spanned by the columns of X.

_categorial (or qualitative) predictors_: Given a categorial predictor X~j~ representing l classes {class0, ..., class(l-1)). Make (l-1) dummy variables X~j|0~, ..., X~j|l-1~. X~j|k~ is 1 if X~j~ equals classk, and 0 otherwise. Instead the value pair (0, 1) we could also use other non-equal values like e.g. (-1, 1).  The class with no dummy variable, here denoted class0, is called the baseline.

_Pros/cons of adding more predictors_: pro) Better fit to the training data. con) overfitting. pro) lower bias. con) higher variance. con) lower interpretability. Rational for higher variance: the individual variabilities for each coefficient sum up and the variability of the estimated hyper-plane increases the more predictors are entered into the model, whether they are relevant or not. See also bias-variance trade-off. See also <<curse_of_dimensionality>>

[[notes_on_correlation]]
_Notes on predictor correlation_: If predictors are correlated in some way, but especially when they they are linearly correlated (e.g. measureable with <<pearsons_correlation_coefficient>>), then the variance and the p-value of the respective coefficient estimators β̂~i~ increase.  Imagine a linear model with two predictors, say there's the strong correlation X~1~ = 2X~2~, and say the true relation is Y = X~1~.  With some training data, LS migh deliver β̂~1~ = 1 and β̂~2~ = 0, with other training data β̂~1~ = 0 and β̂~2~ = 1/2, and with yet other training data anything inbetween.  Thus the variance and thus the p-value of β̂~1~ and β̂~2~ will be high.  You can also imagine a 3D plot, in the X~1~-X~2~ plane the observations are on a line, and the points in the 3D space are also almost on a line, and there's no stable way to lay a plane through a line.  When the predictors are correlated but not linearly correlated, there may or may not be a negative effect, but only when they are uncorrelated we are sure that we don't suffer from the mentioned negative effects.  See also <<interpretation_of_t_test>>.

[[notes_on_orthogonal_X_matrix]]
_Notes on orthogonal X matrix_:  An orthogonal X matrix directly implies uncorrelated predictors.  It can be shown that an orthogonal X implies that Cov[β̂] is diagonal matrix, i.e. the coefficient estimators β̂ are uncorrelated.  This in turn implies that β̂~j~ is identical to a β̃~j~ of a model using X~j~ as sole predictor, i.e. it can be shown that both have the same formula.  Followingly the t-test / p-value for the two are identical.

[[interpretation_of_t_test]]
_Interpretation of a t-test / p-value_ of a coeficient estimate β̂~j~:  Recall that from the assumptions it follows that the coefficient estimates β̂ are normally distributed.  Thus as always, the t-statistic for β̂~j~ is (β̂~j~ - β~j~)/sê[β̂~j~] \~ t~n-p~.  Note that sê[β̂~j~] depends on β̂, i.e. on the full model, because sê[β̂~j~] = ... = √diag(RSE²(X^T^X)^-1^),  where RSE depends on Ŷ and Ŷ in turn depends on β̂.  In a t-test for β̂~j~, the null hypothesis H~0~ is in general β~j~ = 0  (or in other words H~0~: y \~ -X~j~, H~a~: y \~ .).  A t-test for β̂~j~ quantifies the effect of the predictor X~j~ after having subtracted the linear effect of all other predictor variables: Y - ∑~0≤i≤p;i≠j~β̂~i~X~i-th col~ = β̂~j~X~j~.  Note that when looking at these individual t-tests, besides the multiple testing problem in general, it can happen that all individual t-tests show insignificant p-values, altough it is true that some predictor variables have a significant effect.  This occures if predictors are correlated, see <<notes_on_correlation>> paragraph.  That at least some predictors have an effect can be quantified with a F-test. If p-value of F-test is not significant, then you can't trust p-values of coefficients.   Uncorrelated predictors implies independent tests, and independent tests are nice to interpret.

_F-test / ANOVA_: The null hypothesis is that all β~i~ are zero (aka global null).  In other words, full model is compared against empty model.  Recall that ESS has p-1 d.o.f. and RSS has n-p d.o.f.  If p-value of F-test is not significant, then you can't trust the p-values of the coefficient estimates. In R, one way to calculate the p-value of the global F-test would be anova(fit.empty, fit.all), where fit.empty = lm(response \~ 1, ...) and fit.all = lm(response ~ predictor1 + ...).

H~0~: β~0~ = ... = β~p~ = 0 +
H~0~: F = (ESS/(p-1)) / (RSS/(n-p)) \~ F~p-1,n-p~

A _partial F-test_ compares a reduced model where q predictors are left out (i.e. their coefficents are zero) to the full model.  WLOG we assume their indicies are (p-q+1, ..., p).  The partial F-test can be interpreted as the partial effect of adding these q predictors to the model.  The p-value of a partial F-test leaving out one predictor equals the p-value of that predictor.

H~0~: β~p-q+1~ = ... = β~p~ = 0 +
H~0~: F = \((RSSʹ-RSS)/(pʹ-1)) / (RSS/(n-p)) \~ F~pʹ-1,n-p~

**to-do**(1) I found both statements "The F-statisic of leaving out a given predictor equals the squared t-statistic of that predictor" and "The p-value of a partial F-test leaving out one predictor equals the p-value of that predictor". Don't they contradict?

*to-do* Relation to <<multicollinearity>>

Var̂[β̂] = σ̂² / (Var[X]·(1-R²)). Thus Var̂[β̂] increases when variance σ̂² of error terms increases, and it decreases when n or Var[X] increase.

_choosing which predictors to add_ / _pros/cons of adding more predictors_: see <<model_selection,feature selection>>

_one multiple linear regression vs multiple single linear regressions_: Linear regression on a single predictor variable yields the same respective coefficient estimate β̂~i~ as the multiple linear regression only if the predictor variables are orthogonal, see also <<notes_on_correlation>>.

_Tukey-Anscombe plot_ (_residuals vs fitted_): Scatterplot of residuals e (on y-axis) versus fitted values Ŷ (on the x-axis).  Ideally points should be randomly around horizontal line through zero (due to assumpution E[ε] = 0), and their vertical spread should be constant (due to assumption of homoscedasticity). If the trendline shows a trend, there is some evidence that the linear model assumption is not correct.  If the standard deviation grows linearly with the fitted values, the log-transform Y → log(Y) stabilizes the variance *to-do* what is the overall picture of this log-transformation?

_Q-Q (quantile-quantile) plot_: Scatterplot of empirical quantiles of the residuals (on the y-axis) versus the theoretical quantiles of a N(0, 1) distribution (on the x-axis).  Ideally the points should be on a 45 deg line.  Deviations from that mean the residuals are not normally distributed. Q-Q plot is nice to verify that a distribution is normal distributed. See also https://data.library.virginia.edu/understanding-q-q-plots/

Reference for these diagnostic plots: https://data.library.virginia.edu/diagnostic-plots/

*to-do* I don't understand that plot. What does it mean to have something theoretical (i.e. not directly related to our actual data) on an axis?

**to-do**(2) What do I see on the Q-Q plot that I don't see (as easily) on the Turey-Anscombe plot?

|=====
| P = X(X^T^X)^-1^X^T^; p~ij~ = Cov[ŷ~i~,y~j~]/Var[y~j~] | Projection matrix *to-do* formula for p~ij~ using only x's. That would help to see what it mathematically means for a predictor to be far away from the mean of predictors.
| P^T^ = P | I.e. P is symmetric
| P² = P | I.e. P is idempotent
| PX = X | X is invariant under P
| diag(P) | Leverages of observations
| ∑p~ii~ = p |
| p~ii~ ∈ [1/n,1] |
| Y = f(X) = Xβ + ε | β and ε unknown, see however assumptions on ε.
| Ŷ = Xβ̂ = PY  |
| E[Ŷ] = Xβ |
| Cov[Ŷ] = σ²P |
| x~0~^T^β̂ ± σ̂√(x~0~^T^(X^T^X)^-1^x~0~)·qt~n-p;1-α/2~ | (1-α)100% confidence interval for E[y~0~], given new observation x~0~.
| x~0~^T^β̂ ± σ̂√(1+x~0~^T^(X^T^X)^-1^x~0~)·qt~n-p;1-α/2~ | (1-α)100% prediction interval for y~0~
| E[ε] = 0 | By assumption
| Var[ε] = σ²  | By assumption
| Var̂[ε] = σ̂² = RSE² ~ σ²/(n-p) 𝜒²(n-p) | *to-do* 𝜒² is the distribution of RSE², as opposed to σ̂², since σ̂² is an estimator and could also be estimated using other statistics, right?
| e = Y - Ŷ = (I~n~ - P)Y | Residuals
| E[e] = 0 |
| Cov[e] = σ²(I~n~ - P) | Residuals e are correlated (error terms ε are not)
| Var[e] = diag(Cov[e]) = σ²(1-diag(P)) | Variance of residuals is _not_ constant (unlike variance of error terms ε).
| sê[e] = se[e] = √Var[e] | (*to-do* is this absolutely correct? why/proofs?)
| e / sê[e] = e / (σ√(1-diag(P))) | Studentized (or standardized) residual
| β̂^OLS^ = argmin~β~(RSS(β)) = (X^T^X)^-1^X^T^Y | Coefficient estimates, computed using OLS. Note that this forumula is handy for theoretical purposes.  For numerical computatioin QR decomposition is mutch more stable. https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Least_squares_estimator_for_.CE.B2[Proof]
| E[β̂] = β |
| Cov[β̂] = σ²(X^T^X)^-1^ | Note that in some notations Var instead of Cov is used; since it's a matrix, it's implicit that the covariance matrix is meant. https://stats.stackexchange.com/questions/68151/how-to-derive-variance-covariance-matrix-of-coefficients-in-linear-r
| Cov̂[β̂] = σ̂²(X^T^X)^-1^ |
| Var[β̂] = diag(Cov[β̂]) | Usual definition
| Var̂[β̂] = diag(Cov̂[β̂]) = σ̂² / (Var[X]·(1-R²)) |
| sê[β̂] = √Var̂[β̂] | Usual definition
| β̂ \~ 𝓝~p~(E[β̂], Var[β̂]) |
| (β̂~j~ - β~j~)/se[β̂~j~] ~ 𝓝(0, 1) | See distribution of β̂.
| t-value of β̂~j~: (β̂~j~ - β~j~)/sê[β̂~j~] \~ t~n-p~ | Commonly we set β~j~ = 0. See also distribution of β̂, t-statistic & t-distribution.
| p-value of β̂~j~: pseudo R-code: 2*pt(abs(t.value.beta), df=n-p, lower=FALSE) |
| β̂~j~ ± sê[β̂~j~]·qt~n-p;1-α/2~ | (1-α)100% confidence interval for β~j~. qt~n-p;1-α/2~ denotes the 1-α/2 quantile of a t~n-p~ distribution.
| F = [(TSS-RSS)/(p+1)] / [RSS/(n-p)]  | F-statistic / ANOVA. The null hypothesis is β~i~ = 0 for all i except 0.  F-statistic is 1 if there is no relationship between X and Y, larger otherwise. Under H~0~ F~p-1,n-p~ distributed.
|=====

*to-do* How to *correctly* interpret what the p-value of β̂~j~ really means. Not in the general sense, but the concrete context of linear regression. See script, box at bottom of page 10.

*to-do* write out diag of Cov[β̂] and P explicitely

References:

- https://www.stat.berkeley.edu/~aditya/resources/LectureSEVEN.pdf

- http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf


=== Linear regression, probabilistic view

Linear regression is a model of the following form, where θ = (w,σ²) are the parameters of the model.

p(y|x,θ) = 𝓝(y|w^T^x,σ²) +
p(y|x,θ) = 𝓝(y|w^T^Φ(x),σ²) [if basis functions are used]

A common estimator θ̂ of the parameter θ is the MLE.  It turns out that here it is equivalent to the least squares estimator, which is based on directly minimizing the empirical risk of the training data, with MSE as loss function.

References:

- Book ``Machine Learning, a Probabilistic Perspective'', chapter ``7 Linear regression''.


[[kernelized_linear_regression]]
=== Kernelized linear regression (inc. regularization)

Turns a linear regression model, here Ridge regression, into a non-linear model using the <<kernel_trick>>.

Recall ŵ = argmin~w~(∑~1≤i≤n~(w^T^x~i~-y~i~)² + λ‖w‖~l~^l^)

Similar to kernelized perceptron, it can be shown that the optimal estimate ŵ lies in the span o the data, i.e. is of the form ŵ = ∑~1≤i≤n~α~i~x~i~.

h(x) = ∑~1≤i≤n~α~i~k(x~i~,x)

Learning (for L2-norm regularization):

α̂ = argmin~α~(‖α^T^K-y‖~2~² + λα^T^Kα) = (K+λI)^-1^y


[[least_squares]]
=== Least squares regression

_Least squares_ (or _LS_) is a method of fitting a model which tries to minimize the RSS.  In other words, it is a method for computing the estimators β̂ of the model coefficients β.  Let β̂^LS^ (or β̂, since LS is kind of the standard) denote the estimator, as defined by LS, of the coefficients β of the model:

β̂^LS^ = argmin~β~(RSS(β)) = +
(X^T^X)^-1^X^T^Y [closed form solution for linear regression]

There are two categories: _Ordinary least squares_ (or _OLS_ or _linear least squares_) and _non-linear least squares_.  Linear least squares has a closed-form solution, see <<linear_regression_models>>.  The nonlinear problem is usually solved by iterativie refinement.

[[BLUE]]
_Gauss-Markov theorem_: In a linear regression model with uncorreleated error terms ε, constant finite variance Var[ε~i~] = σ² and E[ε] = 0, the coefficents β̂ found by OLS are _BLUE_ (_best linear unbiased estimator_).  Here, `best' means Var~train~[β̂] is minimized compared to other unbiased estimators, and unbiased means Bias~train~[β̂] = 0.

_computational cost_ in case of linear regression: Note that X^T^X is a p⨯p matrix, and solving (X^T^X)^-1^X^T^Y is then in O(p³), which is practically quite high for large p, which occurs in certain practical problems. Thus non-closed form solutions such as (stochastic) gradient descent might be a feasible alternative.

*to-do* But that only works when the population regression function f (i.e. the true model) is linear, right? YES. Else also the linear function based on the coefficient β has bias. What is the appropriate terminology to phrase all this?

*to-do* From BLUE follows that Bias~train~[f̂] = 0? NO. That is, only if I choose all the predictors I should pick. I.e. expectedTestMSE = (Bias~train~[f̂])² + Var~train~[f̂] + Var[ε]?

Contra: Var[β̂^LS^] is high in case of <<multicollinearity>>. <<shrinkage_method>>s try to solve this problem.


[[shrinkage_method]]
=== Shrinkage Methods / Regularization Methods

_Shrinkage methods_ (or _regularaization methods_) is a family of techniques which try to solve the problem of <<multicollinearity>>.  Altough ordenary least squares coefficient estimates are <<BLUE>> (i.e. unbiased and low variance), in case of correlated predictors the variance of a coefficent estimator might still be unacceptably high.  Shrinkage methods now trade lower variance for (hopefully only) a small increase in bias.  Also, least squares has problems wit large p or even p>n.  Shrinkage methods don't suffer from that as much.

*to-do* other sources say shrinkage methods are about fighting overfitting (which in turn means high variance). Applies <<occams_razor>>, since fewer features means a simpler model, and making the coefficients smaller goes towards fewer features. Controll complexity of the model. Balance goodness of fit vs overfitting.

Note that putting a penality on the weights w as done by Lasso or Ridge is just one form of regularization.

[[standardize_variables]]
_standardize variables_: Ridge and Lasso are not scale invariant, which least squares is. So we should standardize variables to have variance 1 and mean 0: x̃~ij~ = (x~ij~-μ~j~)/σ~j~, where μ~j~ = 1/n∑~i~x~ij~ and σ~j~²=1/n∑~i~(x~ij~-μ~j~)².  Also standardize y~i~ likewise.  As a consequence, the intercept will be guaranteed to be 0, thus X̃ is not required to have the artificial first column consisting of ones.  Note that if only X is standardized but not Y, i.e. there might be an non-zero intercept β~0~, then the β in the penalty term, e.g. λ‖β‖~2~² in Ridge, should be replaced by β^(0)^, where β^(0)^ means β with the intercept β~0~ removed, i.e. one element less. This is because we only want to regularize the coefficients of the features, we don't want to regularize the intercept.

Standardization is done by default in R's glmnet.

[[multicollinearity]]
_Multicollinearity_ describes the situation that a predictor can be linearly predicted from one or more other predictors.  In this situation, when using OLS, Var~train~[β̂] is hight, i.e. there's a higher chance of the estimator β̂ being far away from the real β.  Recall that in OLS, the estimators β̂ are unbiased.  Consider a model with two predictors X~1~ ≈ X~2~.  Then f̂(x) = β̂~0~ + β̂~1~X~1~ + β̂~2~X~2~ ≈ β̂~0~ + β̂~3~X~1~, where β̂~3~ = β̂~1~ + β̂~2~. If there was only one predictor as in the rhs of ≈, we could calculate β̂~3~ well. But in the model with two predictors, OLS doesn't know how to distribute β̂~3~ among β̂~1~ and β̂~2~ such that β̂~1~ + β̂~2~ = β̂~3~.  E.g in one training data sample, β̂~1~ will be large and β̂~2~ will be small, in the other training data sample the other way round, in yet another training data sample it's evenly distributed etc.  <<shrinkage_method>>s try to solve this problem.

[[choosing_regularization_parameter]]
Choose regularization parameter λ with cross validation. Typically you would choose the candidate λ logarithmically spaced, e.g. {10^-4^, 10^-3^, ..., 10^4^}, since the order of magnitude matters, not the exact value.

References:

- Book ``An introduction to statistical learning'', chapter ``6.2 Shrinkage Methods''


[[bayesian_interpretation_of_regularization]]
==== Bayesian interpretation of regression and regularization

Regularization can often be understood as MAP inference.

optimization view: loss-function + regularization +
probabilistic view: likelihood * prior

For example Ridgre regression expressed in the Bayesian view:

ŵ^ridge^ = argmax~w~(p(w|𝓓)) = argmax~w~(p(w)∏~1≤i≤n~p(y~i~|x~i~,w)) +
p(y|w,x) \~iid 𝓝(w^T^x,σ²) [likelihood / noise] +
p(w) ~ 𝓝(0,σ²/λ) [prior]

More formally and generally, we can write the regression view as follows, where l is some loss function, and Reg(·) is some regularization:

ŵ = argmin~w~(R̂(w) + Reg(w)) +
R̂(·) = ∑~1≤i≤n~l(h(x~i~;w),y~i~)

The probabilistic / Bayesian view in general can be written as follows

ŵ = argmax~w~(p(w|𝓓)) = argmax~w~(p(w)∏~1≤i≤n~p(y~i~|x~i~,w))

The relation between these two views is:

Reg(w) = - log p(w) +
l(h(x~i~;w),y~i~) = - log p(y~i~|x~i~,w)

|=====
| name               | loss function | regularization | likelihood  | prior
| least squares      | squared loss  | none           | Gaussian    | Uniform
| best subset        | squared loss  | L0-norm        | Gaussian    | ?
| Lasso              | squared loss  | L1-norm        | Gaussian    | Laplace
| Ridge              | squared loss  | L2-norm        | Gaussian    | Gaussian
| Robust regression  | ?             | ?              | Laplace     | Uniform
| Robust regression  | (non convex)  | ?              | Student     | Uniform
|=====

Large λ in regularization means that we believe the prior has a small variance, i.e. that w is close to zero.  If I believe the noise (in likelihood) is large, then I should regularize more.

If you have outliers, you should consider another likelihood distribution than Gaussian, for example Student's t.  That's because with Gaussian, everything that is a few standard deviations away from the mean has a probability of practically 0, so outliers strongly disagree with the model and thus mess things up.

The more your prior is away from the truth, the more observations you need to make up for it.


[[comparison_of_regularization_methods]]
==== Comparison of regularization methods

The criterion are convex in β for Ridge and Lasso (see geometrical interpretation explained in Ridge). That is computationally convenient. Given a set of λ (opposed to one λ), we still can compute the optimal solution β̂ in one go.

Lasso is good for feature selection.  Best subsetset selection is computationally expensive. It is non-convex.  Lasso is in a sense the best convex relaxation of best subset selection.  The found solution, since there are less constraints, won't be the same, i.e. is an approximation, but we hope it's close enough.


[[ridge_regression]]
==== Ridge regression / L2-regularized regression

_Ridge regression_ (or _Tikhonov regularization_ or _weight decay_) is a <<shrinkage_method>>. λ≥0 and s≥0 are tuning parameters, where there is a bijection between them.

Recall to <<standardize_variables>>.  For simplicity, here X and Y refer to the standardized variables, as opposed to the original ones. ‖β‖~2~ denotes the L2-Norm (aka Eucledian distance) of β, i.e. ‖β‖~2~² = ∑~1≤i≤p~β²~i~.  Three equivalent forms of formulating the optimization problem are:

β̂^ridge^ = argmin~β~(RSS(β) + λ‖β‖~2~²) +
β̂^ridge^ = argmin~β~(RSS(β)), subject to ‖β‖~2~²≤s +
β̂^ridge^ = see <<bayesian_interpretation_of_regularization>>

The analytical closed form solution is:

β̂^ridge^ = (X^T^X+λI)^-1^X^T^Y

_Geometric interpretation_: For the case of 2 predictors, i.e. also two coefficients β~1~ and β~2~: Imagine a 2D coordinate system, the x axis represents possible β~1~ values, the y axis possible β~2~ values.  Each point in the plane represents a possible β, or a possible estimate β̂.  The point (β̂^LS^~1~, β̂^LS^~2~) represents the LS estimate β̂^LS^, which minimizes RSS.  Around that point we can draw contour lines, each contour line representing another, higher, RSS.  Also, we draw a circle around center (0,0) with a diameter of √s, where s can be derived from λ and vice versa.  The geometrical interpretation of the shrinkage penalty term is that the estimate β̂ must lie within the circle.  Assuming β̂^LS^ lies outside the circle, the point where the first contour line touches the circle is the estimate β̂ found by Ridge.  β = E[β̂^LS^] is the point representing the true coefficient β. Around it we can draw ellipses with semi-axes n·Var[β̂^LS^], n={1,2,...}, represinting the area where {68%, 95%, ...} of the β̂^LS^ will lie.  Assuming β is outside the shrinkage penalty circle, the β̂^Ridge^ will only be on a comperatively small part on the shrinkage penalty circle, i.e. Var[β̂^Ridge^] is lower than Var[β̂^LS^]. However we also see that E[β̂^Ridge^] will no be away from β.

The elements of β̂^Ridge^ generally won't be zero, which is in contrast to β̂^Lasso^.  That's because the first contour line touching the shrinkage penalty circle probably doesn't do that at a point which exactly lies on one or more axes.

The second term, λ|β|², is also called _shrinkage penalty_.  Its effect is that of _shrinking_ the coefficient estimates towards zero.  The tuning parameter λ serves to control the amount of shrinkage.  λ=0 means no shrinkage, resulting in β̂^ridge^ = β̂^LS^.  λ→∞ means infinite shrinkage, resulting in β̂^ridge^ = 0.

Unlike least squares, Ridge has a solution β̂^ridge^ even if n < p.

See also <<choosing_regularization_parameter>> λ.

References:

- Book ``An introduction to statistical learning'', chapter ``6.2.1 Ridge Regression''


[[lasso_regression]]
==== Lasso regression / L1-regularized regression

A <<shrinkage_method>>.  See also subchapter <<ridge_regression>>, where common topics are discussed.  λ≥0 and s≥0 are tuning parameters, where there is a bijection between them.

Recall to <<standardize_variables>>.  For simplicity, here X and Y refer to the standardized variables, as opposed to the original ones.  ‖β‖~1~ denotes the L1-Norm of β, i.e. ‖β‖~1~ = ∑~1≤i≤p~|β~i~|.  Three equivalent forms of formulating the optimization problem are:

β̂^Lasso^ = argmin~β~(RSS(β) + λ‖β‖~1~) +
β̂^Lasso^ = argmin~β~(RSS(β)), subject to ‖β‖~1~≤s +
β̂^Lasso^ = see <<bayesian_interpretation_of_regularization>>

Solution to the optimization problem:  Note that the L1 norm is not differentiable, which makes it harder to optimize.  The objective function however is still convex, which is good for optimization.  One possible optimization technique is LARS.

The elements of β̂^Lasso^ often are exactly 0, especially for high λ.  Look at the respective paragraph of <<ridge_regression>>.  Now in Lasso, the geometrical interpretation of the shrinkage penalty is a square rotated 45 deg, the corners on the axes.  Now it is more likely that the first contour line touches the square at an corner than at an corner.  Since the corner lies on an axis, β̂^ridge^ is 0 with respect to the other axis.  Imagine four strips, each strip extending from one side of the square, each strip being diagonal with respect to the coordinate system.  Only if the true β lies within such a strip, the first contour line would hit the side of the square.  The area of those strips relative to the rest of the plane is small, thus it is more likely that the first contour line hits a corner.

Lasso can be seen as using a tractable surrogate loss function (using L1 norm) with respect to the intractable loss function (using L0 norm) of the best subset selection. See also <<comparison_of_regularization_methods>>.

See also <<choosing_regularization_parameter>> λ.

References:

- Book ``An introduction to statistical learning'', chapter ``6.2.2 The Lasso''


[[best_subset_selection_as_shrinkage]]
==== Best subset selection viewed as shrinkage

<<best_subset_selection>> can also be viewed as shrinkage method.  Especially in the 2nd variant of the definition we see that it selects not more than s predictors. λ≥0 and s≥0 are tuning parameters, where there is a bijection between them.

Recall to <<standardize_variables>>.  For simplicity, here X and Y refer to the standardized variables, as opposed to the original ones.  ‖β‖~0~ denotes the L0-Norm of β, i.e. ‖β‖~0~ = ∑~1≤i≤p~1~βi≠0~.  Three equivalent forms of formulating the optimization problem are:

β̂^subset^ = argmin~β~(RSS(β) + λ‖β‖~0~) +
β̂^subset^ = argmin~β~(RSS(β)), subject to ‖β‖~0~≤s +
β̂^subset^ = see <<bayesian_interpretation_of_regularization>>

Solution to the optimization problem: The above optimization problem is difficult since the L0 norm regularization term makes it an non-convex objetive function. To make it a tractable problem, we can use the L1 norm as surrogate for the L0 norm, resulting in the Lasso, see also <<comparison_of_regularization_methods>>

The elements of β̂^subset^ often are exactly 0, especially for high λ.  Look at the respective paragraph of <<ridge_regression>>.  Now in best subset selection, the geometrical interpretation of the shrinkage penalty is a line on each axis from 0 to s. *to-do*

*to-do* In the geometrical interpretation, the contour lines will almost surely either touch the tip of one line (making this β̂~i~ equal to s), or less likely some line (making this β̂~i~ somewhere between 0 and s). I don't see how it can be (apart from a tiny probability) that multiple predictors are selected.  I don't see at all how a β̂~i~ can be larger than s -- but im sure in best subset selelection when s predictors are choosen, the β̂~i~ can be larger than s.

See also <<choosing_regularization_parameter>> λ.


[[basic_functions]]
=== Basic functions / basis functions / basis expansion

We only look at univariate models, i.e. one predictor X.  We have a family of k fixed known functions b~j~(X), j ∈ [k].  This transformation delivers the new linear model

f(x~i~) = ∑~0≤j≤k~β~j~b~j~(x~i~) + ε~i~

It is a regular linear model with k predictors b~j~(X), j ∈ [k].  Hence the same rules apply, we can use the same fitting methods such as least squares, and the same inference tools are available.

More generally, including multivariate models, we can replace the measured observation (p+1)-dimensional vector x~i~ of the i-th iteration with the (d+1)-dimensional feature vector Φ(x). Let Φ~j~(x~i~) denote the j-th feature computed from the i-th observation. Φ~j~(·) can be a non-linear function. Note that Φ~0~(·) must be constant 1, such that w~0~Φ~j~(·) = w~0~ represents the intercept.

h(x) = ∑~0≤j≤d~w~j~Φ~j~(x)

Visualization: Image a binary classification problem with two measurements per observation. Observations for class 0 are disk shaped centered at (0, 0), and obsevations for class 1 are ring shaped, the ring more or less separated from the circle.  In the 2D measurement space (a plane), there is no linear decision boundary.  The decision boundary is a circle between the disk and the ring. However when we add a third feature x^(3)^ = x^(1)^² + x^(2)^², the x^(1)^x^(2)^ plane of the 2D measurement space is transformed into a paraboloid in the 3D feature space.  Class 0 is at the bottom of the paraboloid, class 1 is higher up on the paraboloid.  Now in that 3D feature space we can linearly seperate the two classes by a plane parallel to the x^(1)^x^(2)^ plane.  The intersection between that decision boundary plane and the paraboloid yields a decision boundary that is circular in the 2D measurement space.

Examples:

- <<polynomial_regression>>

- <<step_functions>>

- <<regression_splines>>

References:

- ISLR, chapter ``7.3 Basic functions''


[[polynomial_regression]]
=== Polynomial regression

Special case of the <<basic_functions>> approach.  We only look at univariate models, i.e. one predictor X.  We replace the standard linear model Y = Xβ + ε with a polynomial in X of degree d resulting in a new linear model

Y = P~d~(X) + ε = β~0~ + β~1~X + β~2~X^2^ + ... β~d~X^d^ + ε

In practice, it is unuasal to use d greater than 3 or 4, because of overfitting, i.e. the polynomial can take strange shapes, especially at the left and right boundary.

As always, its beneficial if predictors are uncorrelated, i.e. if the matrix X is orthogonal, see also <<notes_on_orthogonal_X_matrix>>.  In R, poly(...) returns an orthogonal matrix.

Pro: easy to fit via LS

Pro: all the properties of linear regression still hold (since technically it still is linear regression)

contra: instable at boundaries

References:

- ISLR, chapter ``7.1 Polynomial Regression''


[[step_functions]]
=== Step functions

Special case of the <<basic_functions>> approach.  Can also be seen as special case of piecewise polynomials in that the polynomials have degree 0.  We only look at univariate models, i.e. one predictor X.  We cut the X axis at k cut points c~1~, ..., c~k~ into k+1 bins / regions. In the j-th bin (starting at 0), we use the constant functions β~j~.  This amounts to conoverting the continous variable X into an ordered categorial variable.  The model then is as follows. Note that we don't use C~0~(x), since that is redundant to intercept β~0~.  i denotes the data point index.

y~i~ = β~0~ + ∑~1≤j≤k~β~j~C~j~(x~i~) + ε~i~

Where C~j~(·), sometimes called dummy variables, are defined as follows. I(·) denotes the indicator function.

C~j~(x) = I(c~j~ ≤ x < c~j+1~) ∀ j ∈ [1,k-1] +
C~k~(x) = I(c~k~ ≤ x)

β~0~ can be interpreted as the mean Y value for X < c~1~.

References:

- ISLR, chapter ``7.2 Step Functions''


[[regression_splines]]
=== Regression splines

==== Piecewise polynomials / Splines

We only look at univariate models, i.e. one predictor X.  A spline is a piecewise polynomimal with further boundary conditions which are explained later. The idea is to combine step functions and polynomial regression.  Or in other words, instead of having one global polynomial, we have multiple rather low degree polynomials, one for each of multiple mutually exclusive consecutive regions of X.

_Spline of degree d_: We have k+1 regions.  The k boundaries are called _knots_.  Each region as a polynomial of degree d.  Additionally we have the constraints that the global function derivatives must be continous up to degree d-1.  A _cubic spline_ is a spline of degree 3.

E.g degree 0: Step functions, i.e. non-continuous at knots. degree 1: piecwise linear, i.e. continuous at knots. degree 3: piecwise cubic, with continuous 0th, 1st, 2nd derivatives.

*to-do* but degre 0, i.e. stepfunctions, are not continuos in 0th derivative degree, so they are _not_ splines?!

The _truncated power basis function_ h(x, ξ) of degree d will be used shortly and is defined as follows.  ξ denotes the position of a knot.  Note that at ξ, h, h', h'' etc up to degree d-1 are continous, so the spline's constraints are fulfilled.

h(x, ξ) = { (x-ξ)^d^ if x>ξ; 0 otherwise

There many ways to represent splines of degree d.  We can e.g. base on the <<basic_functions>> approach, in which case there are still many ways of choosing a basis function.  One way is the following, where γs are further model coefficients additional to the βs, ξ~l~ is the position of the l-th knot, and the truncated power function h is defined above.  As always with the basis functions approach, the model can then be fit using linear regression.

y~i~ = ∑~0≤j≤d~β~j~x~i~^j^ + ∑~1≤l≤k~γ~l~h(x~i~, ξ~l~) + ε~i~

There are d+k+1 degrees of freedom.  Analysis:  There are k knots, i.e. k+1 regions, thus k+1 polynomials of degree d.  Recall that a polynomial of degree d has d+1 parameters.  At each knot there are d constraints.  (k+1)(d+1) - kd = k+d+1.

We still need to decide on the number of knots and where to place them.  In practice we often choose only the number of knots, and then distribute them uniformily regarding percentiles, i.e. each region has the same amount of data points.  This altough it seems natural to choose more knots where we feel the model should be able to change more rapidly.  This 2nd variant can work well, still in pratice often the knots are distributed uniformily.

Contra: high variance of f̂ at boundaries, since there polynials still can go wild.  Natural splines try to mitigate this problem.

A _natural spline_ is a spline with the additional boundary conditions that the global function must be linear in the two boundary regions.  Confidence intervals for f̂ in the two boundary regions are now much smaller compared to regular splines.   Note however since the two f̂ are different, parts of the confidence interval of natural splines might be outside of the confidence interval of regular splines.  k degrees of freedom; 2·2 less than regular spline because each of the two border knots has two additional constraints.

*to-do* formula analogous to the above y~i~ = ... for natural splines

*to-do* formula for basis functions of splines and for natural splines

References:

- ISLR, chapter ``7.4 Regression Splines''


==== Smoothing splines

We only look at univariate models, i.e. one predictor X.  As in ridge and lasso, the model takes a ``loss + penalty'' formulation and a tuning paramter λ.  In the definition below which also introduced g, RSS(g) is a loss function that encourages g to fit the training data well, and the term λ∫gʺ(x)²dx is a penalty term that penaltizes a highly ``wiggly'' g.

Let G be a class of functions where [a, b] ⊆ ℝ dontes the data range

G = {g: [a,b] → ℝ, gʺ exits and ∫~a,b~gʺ(x)²dx < ∞}

The estimate f̂ of the regression function is then defined as follows.  It can be shown that f̂ is a natural cubic splines with n knots, each training data point having one knot on it.  There multiple possible such cubic splines, and we want the one satisfying the given `argmin constraint'.

f̂ = argmin~g∈G~[RSS(g) + λ∫~a,b~gʺ(x)²dx]

λ = 0: f̂ perfectly fits the _training_ data (RSS=0), i.e. f̂ interpolates the training data, i.e. f̂ goes through every training data point.  It's is a flexible model (n degrees of freedom), thus has low bias and high variance.

λ → ∞: gʺ(x) is forced to be 0 for all x, i.e. g must be linear.  f̂ is equivalent to the linear regression least squares solution.  It's an inflexible model (2 degrees of freedom), thus more bias and less variance; recall that least squares only has zero bias if the true model is linear.

The nominal degree of freedom is n, see also natural splines.  Nominal degree of freedom usually refers to number of free parameters.  However here, these n parameters are constrained or shrunk down.  The effective degrees of freedom df~λ~ is a measure of the flexibility of smoothing splines.  It is between n (for λ = 0) and 2 (for λ → ∞), and in general its defined to be df~λ~ = trace(S~λ~), see definition of S~λ~ below.

*to-do* I don't properly understand the difference between degrees of freedom and effective degrees of freedom. Are equivalent degrees (script 3.4.3) of freedom and effective degrees of freedom synonyms?

The coefficients estimates β̂ (n⨯1 vector) can be computed using fast linear algebra.

As in least squares linear regression where we had Ŷ = Xβ̂ = PY, we can write Ŷ = S~λ~Y where the matrix S~λ~ can be computed using fast linear algrebra.

A way to find a good λ is <<cross_validation>>.  In particular leave one out cross validation (LOOCV), because it turns out that we can do LOOCV in essentially one single fit.

*to-do* why exactly 2nd derivative is chosen

*to-do* what exactly is the difference to cubic natural spline? As far as I understand it, the model f is identical, but the model parameters are fitted with different methods, wheras natural cubic spline is fitted with LS.  Also, smoothing spline has one knot at each data point (what if there are multipl data points per x? average?).  Is smoothing spline equivalent to natural cubic splines where knots are at the data points.

*to-do* how does fitting work? To a level of details important for this lecture.

References:

- ISLR, chapter ``7.5 Smoothing splines''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``3.4 Smoothing splines and penalized regression''


=== Generalized Addidtive Models (GAMs)

y~i~ = β~0~ + ∑~1≤j≤p~f~j~(x~ij~) + ε~i~

The f~j~ are unspecified univariate (smooth) functions (aka smoothers).  E.g. in linear regression, f~j~(x~ij~) = β~j~x~ij~. GAMs are called additive, because there's a separate f~j~ for each predictor X~j~, and then all results are added together.  Followingly there is also no interaction, which would require functions taking multiple predictors as arguments.  That keeps GAMs simple.  From no interaction also follows only little curse of dimensionality, see there.

f~j~ are constrained as follows in order to get an identifiable model. Without these constraint, one could add constant a to f~j~ and substract it from f~jʹ~.  Constraining the f~j~ s that way results in β~0~ = mean(Y).

E[f~j~(X~j~)] = ∑~1≤i≤n~f~j~(x~ij~) = 0

If all f~j~ are univariate models which allow to be fitted with LS, then the overall model can obviously be fitted with LS.  Interpetability is also good, since we can draw an x~j~-y plot for each f̂~j~ (recall that there's no interaction).

References:

- ISLR, chapter ``7.7 Generalized Additive Models''


==== Backfitting

If the f~j~ cannot be fitted using LS (e.g. smoothing splines), one possible fitting approach is backfitting.  It iteratively optimizes one coordinate (corresponding to a predictor), while keeping all others fixed.

--------------------------------------------------
backfitting(Y:n⨯1 vector, X:n⨯p matrix)
  beta0.hat.asvector = mean(Y) * "n⨯1 vector all 1"
  for each predictor j = 1...p:  
    g.j = n⨯1 vector all 0
  do:
    for each predictor j = 1...p:
      Yʹ = Y - beta0.hat.asvector - ∑~1≤k≤p;k≠j~g.k
      f.hat.j = fit.j(Yʹ ~ X.j)
      gnew.j = f.hat.j(X.j)
      gnew.j -= mean(gnew.j) (i.e. normalize)
    convergence = max~over all 1≤j≤p~( (|gnew.j-g.j|)/|g.j| ) < tolerance
    for each predictor j = 1...p:
      g.j = gnew.j
  until convergence
  Result: The f.hat.j s (i.e. their coefficent estimates)
--------------------------------------------------

Normalizing gnew.j is not strictly required, but gives better control of numeric errors.  The convergence criterion could also be choosen differently.  The one used here means to stop when f̂~j~(X~j~) doesn't substantially change anymore.  Tolerance might be 10^-6^.

Note: In most situations, backfitting is equivalent to the Gauss-Seidel method for solving a certain liniear system of equations.

pro) very general, works for all smoothers f~j~.

contra) slow


[[decision_trees]]
=== Decision trees

p denotes the number of predictors, y∈ℝ, x∈ℝ^p^. P = {R~1~, ..., R~M~} is a partition of ℝ^p^ into M regions / partitions.  β~1~, ..., β~M~ are coefficients of the model.  I(·) denotes the indicator function.

The underlying model: y = g~tree~(x) + ε, g~tree~(x) = ∑~1≤r≤M~β~r~I(x∈R~r~).

Note that g~tree~ is piecewise constant.

β~j~ equals the average of the y values in region R~j~. Thus, computing/estimating the coefficients β is easy once we know the partition P.

The hard part is finding a good partition, see <<estimation_of_partition>>.

Note that interactions are allowed.

*to-do* somewhere, probably close to correlation and or (in)depence, write what interaction is:  Say we have two predictors.  With no interaction, the curve Y(X1) remains the same independent of X2.

If the number of data points per region is too small (e.g. when tree is too deep), we are suffering from overfitting.

References:

- ISLR, chapter ``8 Tree-Based Methods''


[[estimation_of_partition]]
==== Estimation of partition

We restrict ourselves to partitions that can be represented as trees of axis parallel regions, i.e. p-dimensional boxes.  The idealized goal from training perspective is to find a partition which minimizes the RSS (which on the downside would result in a strong overfit).

Recursively split ℝ^p^ into regions.  At each split, greedely choose an axis / predictor and a split point such that the log likelihood is increased the most.  In other words, from all the possible trees resulting from the possible splits, choose the the tree/split with the highest log likelyhood.  In certain cases this is the same as reducing the RSS the most.  A possible stopping criterion might be that all regions contain fewer than a given number of observations.

*to-do* concrete example of log likelihood in this context

_prunning back_: The resulting full tree T~0~ is likely to overfit by beeing to deep / complex.  A smaller subtree T ⊂ T~0~ with fever splits is likely to have smaller variance, smaller test error and better interpretation at the cost of higher bias.  Intuitively we want to find the subtree that leads to the lowest test error. Note that in case of <<bagging>>, we don't prune.

Note that the alternative algorithm of stop growing the tree when a split doesn't seem worth it is not the same as the above algorithm of growing deep and then prune afterwards.  This is because a seemingly worthless split might be followed somewhere deeper down by a very worthwhile split, that is a split that leads to a large increase in the log likelyhood.

_cost complexity pruninning_: A concrete prunning back method.  We find a subtree T⊆T~0~ according to the following argmin formula, where |T| denotes the number of leaves in tree T, and α is a tuning parameter.  Note that it's a similar pattern as seen in <<lasso>> or <<ridge>>. α=0 means no shrinkage penalty resulting in T=T~0~, and α→∞ results in |T|=0, i.e. T has no split.  It turns out that as we increase α from zero, branches get pruned from the tree in a nested and predictable fashion, so obtaining the whole sequence of subtrees as a function of α is easy.  Note that in case of bagging, we don't prune.

argmin~T⊆T0~{RSS(T) + α|T|}

We can do cross validation to find the best α, and then do the recursive splitting and the cost complexity pruning using the optained α on the original data set.

See also <<bagging>> to further reduce variance.


[[random_forests]]
==== Random forests

Is a variant of bagging which aims to decorrelate the trees by the way how splitting works during building a tree.  At each split, only a random set of size m of the p predictors are allowed to be considered for the split.  Often m = √p for classification and m = p/3 for regression.  m = p amounts simply to bagging.

Recall that the idea behind bagging is that averaging quantities decreases variance.  However the variance is not as much reduced when the quantities are correlated.  In the context of bagging that might happen when there's a strong predictor.  Most of the bagged trees will have that predictor in the top split, and consequently most of the bagged trees will look quite similar, and thus the predictions from these bagged trees will be highly correlated.

References:

- ISLR, chapter ``8.2.2 Random Forests''


[[comparison_of_tree_based_methods]]
==== Comparison of tree-based methods

Pros/cons of trees:

pro) interpretation, visualization

pro) allow interactions

pro) can easily handle qualitative predictors (no need for dummy variables)

neutral) classification performance ok

con) prediction performance of functions rather poor (much improved by <<bagging>> or random forests)

con) high variance (much improved by <<bagging>>)

con) piecewise constant underlying function is `unnatural'

con) unstable splitting error at top of tree propagates down

Comparison:

|=====
|                     | trees   | bagged trees | random forests
| interpretation      | +       | -            | -
| computation         | +       | \--          | -
| performance         | -       | +            | \++
| OOB error estimates | -^1)^   | +            | \+
| overfitting         | yes^1)^ | ok           | ok
|=====

1) Use crossvalidation to mitigate


[[KNN]]
=== K-nearest-neighbor regression (KNN)

f̂(x~0~) = 1/k ∑~xi∈N0~y~i~, where N~0~ are the k closest observations to x~0~.

If the underlying model is linear, KNN is worse.

Curse of dimensionality: With more predictors KNN gets worse. Intuitively: When p is gettig larger, then in p-dimensional space the density of data point usually decreases.  Thus the number of neighbors usually decreases.  Linear regression also suffers from larger p's, but not as mutch as KNN. See also <<curse_of_dimensionality>>.

Note: When plotting test error (e.g. expected test MSE), then often on the x-axis we plot 1/k (opposed to k).

*to-do* is a special case of local regression, right? Weihthing function K is constant 1, and the underlying model is a constant function.


=== Local regression

Computes a fitted value ŷ~0~ given an target point x~0~, as opposed to computing coefficents for a global function f̂.  Select the k = s·n nearest training points x~i~ of x~0~, and assign a weight K~i~ = K(x~i~, x~0~) to each of them.  The fraction s ∈ (0,1] (also called span) of training points to select and the weighting function K are parameters of the algorithm.  The weighting function K should be such that the x~i~ closest to x~0~ is assigned the highest weight, and the x~i~ furthest away is assigned the lowest weight.  Do weighted least squares regression on these k points using K~i~ as weight, delivering f̂~x0~.  Whether the underlying model is constant, linear or a polynomial of some higher degree is a further parameter of the algorithm.  The fitted value ŷ~0~ at x~0~ is then ŷ~0~ = f̂~x0~(x~0~).

The most important choice of all these parameters is the span s. It controls the flexibility of the fit: small values results in very local and wiggly fit, large values result in global fit.  We can for example use cross-validation to choose s.

References:

- ISLR, chapter ``7.6 Local Regression''


[[LOESS]]
=== LOESS regression curve

LOESS is locally weighted polynomial regression.  Loosely speaking, LOESS can also be seen as a generalization of KNN which is smoother.  KNN can be thought of as a rectangle function.  The k neigherst neighbors get wheight 1/k, all others get weight 0.  LOESS has a smoother weighting function, parameterized by α.  Larger α means more moothing, which results in less variance and more bias.  Larger n (more samples, while keeping x-range constant) makes it more precise.

**to-do**(3) Comparison LOESS vs local regression

**to-do**(3) What exactly is meant by its getting more precise with larger n? Ssee Exerscise Set 3, exercise 3b. It seems to me that the variance drops, but the bias stays constant. Isn't that against the bias-variance tradeoff? Or is modifying n not covered by bias-variance trade off?


=== Generative models for discrete data

==== Bayesian concept learning

C: A concept, e.g. "prime numbers"
𝓓: (example) data set, drawn from C
x̃: test case / qeuery point
The task is to classify x̃, i.e. to decide whether x̃ ∈ C

The _hypothesis space_ 𝓗 is a set of known concepts.  The subset of 𝓗 that is consistent with the data 𝓓 is called the _version space_.  The _extension_ of a concept is the complete set of examples.

_likelihood_ p(𝓓|h) = 1/|h|^N^: The model favors the smallest concept consistent with the data.  p(𝓓|h) is the probability of getting the data 𝓓 given the data is created by independently sampling N items (with replacement) from concept h.  It get's smaller the larger the concept h is.

_prior (probability)_ p(h) = subjective_definition: Assign each concept a probability, such that probabilities sum up to 1. E.g. in the numbers game, "primes except 7" is less likely (less natural) than "primes".  However this assignment probabilities is often subjective.

_model evidence_ (or _marginal likelyhood_) p(𝓓) = ∑~hʹ∈𝓗~p(𝓓,hʹ)

_posterior (probability)_ p(h|𝓓) = p(𝓓|h)p(h) / p(𝓓): Is the likelihood time the prior, normalized.

_MAP estimate_: ĥ^MAP^(𝓓) = argmax~h~[p(h|𝓓)]

_Maximum likelihood estimate (MLE)_: h^MLE^(𝓓) = argmax~h~[p(𝓓|h)]

_Data overhelms the prior_: For N→∞, ĥ^MAP^ converges to ĥ^MLE^.

Proof: ĥ^MAP^(𝓓) = argmax~h~[p(𝓓|h)p(h)] = argmax~h~[log(p(𝓓|h)) + log(p(h))] →① argmax~h~[log p(𝓓|h)] = argmax~h~[p(𝓓|h)] = ĥ^MLE^(𝓓). ①: For N→∞. The likelyhood term depends exponentially on N, while the prior term stays constant.

_posterior predictive distribution_ p(x̃|𝓓) = ∑~h~p(x̃|h)p(h|𝓓): _predictive inference_, i.e. predict the distribution of a new data point.

*to-do* extract general info / topics to more general place, also the wiki link in references.

References:

- Book ``Machine Learning, a Probabilistic Perspective'', chapter ``3.2 Bayesian concept learning''.

- https://en.wikipedia.org/wiki/Bayesian_inference


==== Beta-binomial model

Let X ∈ {0,1} \~iid Ber(θ) denote the outcome of a coin flip, θ ∈ [0,1]. After N flips, the data is 𝓓 = {x~1~, ..., n~N~}. Let N~1~ \~ Bin(N,θ) denote the number of heads and N~2~ = N - N1 the number of tails.

_likelihood_ p(𝓓|θ) = θ^N1^(1-θ)^N0^

_prior_ p(θ) ∝ Beta(a,b)

_posterior_ p(θ|𝓓) ∝ p(𝓓|θ)p(θ) = ...

References:

- Book ``Machine Learning, a Probabilistic Perspective'', chapter ``3.3 The beta-binomial model''.


=== Logistic regression

Binary classification, despite having `regression' in the name.  It's named the way it is due to the similarity to linear regresssion.

p(y|x,w) is modeled as p(y|x,w) = Ber(y|sigm(w^T^x)), where sigm(·) denotes the sigmoid function, and Ber denotes the Bernoulli distribution.  Interpretation: Our noise model is that far away from the decision boundary, the probability p(y|x) should be close to 1 and 0 respectively, depending on the side.  Close to the boundary, we don't really know which class a point belongs to, so the probability p(y|x) should be close to 0.5. So logistic regression, relative to linear regression, replaces the assumption of Gaussian noise by Bernoulli noise.

ŵ^MLE^ = argmin~w~(∑loss~logistic~(w, y~i~, x~i~)) +
loss~logistic~(w, y, x) = log(1+exp(-yw^T^x))

The logistic loss is a convex function, thus we can use convex optimization techniques, e.g. SGD.  Logistic loss can be viewed as a smooth approximation to the hinge loss.

∇~w~loss~logistic~(w) = p̂(Y=-y|w,x)(-yx) +
p̂(Y=-y|w,x) = 1/(1+exp(yw^T^x)) [prob. of missclassifaction with w]

Thus the gradient step in SGD is w += ηyxp̂(Y=-y|w,x)

As with ridge / lasso for linear regression, we might add a regularization term, to fight overfitting.

L1(Laplace prior): ŵ = argmin~w~(R̂(w) + λ‖w‖~1~) +
L2(Gaussian prior): ŵ = argmin~w~(R̂(w) + λ|w|²)

E.g. for L2 regularization, the gradient step in SGD is w = w(1-2λη) + ηyxp̂(Y=-y|w,x)

probabilistic classification: use conditional distribution

p̂(y|x,ŵ) = 1/(1+exp(-yŵ^T^x))

Estimated Bayesian optimal decision under 0-1 loss: ŷ = a^∗^ = argmax~y~(p̂(y|x,ŵ)) = sign(ŵ^T^x). I.e. the action a^∗^ that minimizes the expected cost is the most likely the class, where set of actions 𝓐 is {+1, -1} (i.e. the action is returning the guessed class) and cost function is the 0-1 loss.

Logistic regression makes the same regression as a special case of Gaussian naive Bayes classifier, see there.

As seen with bayesion optimal decision, having the distribution p̂(y|x,ŵ) is valuable when wanting to make decisions later on. Note that we also could have different cost functions, e.g. asymmetric (different costs for false positive and false negative), or we also have a dontknow action (i.e. answer). With p̂(y|x,ŵ) we not only can do predictions, we also can quantify our uncertainty.  *to-do* move to a more general place

kernelized logistic regression: *to-do* 2.5.18 00:32:29

multi-class logistic regression: *to-do*

References:

- Book ``Machine Learning, a Probabilistic Perspective'', chapter ``1.4.6 Logistic regression''.


=== Perceptron

A linear two-class discriminant model. The input vectcor x is first transformed using a nonlinear transformation yielding a feature vector Φ(x).  The first column of Φ(x) is typically all ones, aka the bias component.  The model is then signʹ(w^T^Φ(x)), where sign is similar to the sign function, only that signʹ(x) = 1 for x=0.  Output +1 means class C~1~, -1 means class C~2~.

Interpretation: Consider the case of 2 predictors, an intercept of 0 and Φ(x) = x.  Make a scatter plot of the training data in the x~1~-x~2~ plane, each observation represented by one of two symbols/colors.  The decision boundary is a line (being 1D = (2-1)D).  The line is determined by its normal rooted at the origin, the vector w.  Region R~1~ (class C~1~) is determined by signʹ(w^T^x) = 1, i.e. w^T^x ≥ 0, i.e. one side of the line plus the line itself, and region R~2~ is the complement.

Learning: The parameter vector w is learned as follows, based on the stochastic gradient descent algorithm: Start with a best guess for w, e.g. zero.  Iterate over all observations and update w each time: w = w + η~i~Φ(x)~i~t~i~, where η~i~ denotes the learning rate, Φ(x)~i~ denotes the feature corresponding to the i-th observation, and t~i~ is the class of the i-th observation.  Repeat this until some convergence criterion is met.

Intuition: Given an training observation. If the current w results in a correct classification, leave w as is. Otherwise, i.e. in case of misclassification, add the distance vector from the decision boundary to the observation to w.

We actually would like to minimize the number of missclassifications, i.e. use the <<01_loss>> function, but the resulting minimization optimization problem would be NP hard.  So instead we use a surrogate loss function, here the _perceptron loss function_ (or just _perceptron_), which is convex.  However when evaluationg the model, e.g. via cross-validation, use the original loss function, here the 0/1 loss function.

*to-do* the perceptron loss is a convex surrogate function for the 0-1 loss, we actually care about 0-1 loss, but optimize something else, the surrogate, here the perceptron loss.

here we work directly with x opposed to Φ(x) (*to-do* integrate more nicely)

ŵ = argmin~w~[∑loss~perceptron~(w,y~i~,x~i~)] +
loss~perceptron~(w, y, x) = max(0, -yw^T^x)

Theorem: If data is linearly seperable, the Perceptron will find a linear seperator.

There migh be no unique solution, because the perceptron does not care about margin. From the perceptron's point of view, all linear seperators are equially good.  This is because for all correct classifications, the perceptron loss is 0, regardles of how `good' the classification was.  The SVM is a variant of the perceptron that cares about the margin, see there.

There is not really a probabilistic interpretation, i.e. there's no likelihood function corresponding to the loss function.  See also <<bayesian_interpretation_of_regularization>>.

References:

- Pattern Recognition and Machine Learning, chapter ``4.1.7 The perceptron algorithm''.


=== Kernelized perceptron

 Turns the perceptron, a linear model, into a non-linear model using the <<kernel_trick>>.

*to-do*


[[SVM]]
=== Linear support vector machine (SVM)

A linear two-class discriminant model.  A slight variation of the perceptron, which uses a different loss function, namely the hinge loss, and L2-regularization.  As a result, unlike the perceptron, the SVM is a _maximal margin classifier_, meaning that it maximizes the margin.

ŵ = argmin~w~(∑loss~hinge~(w, y~i~, x~i~) + λ‖w‖~2~^2^) +
loss~hinge~(w, y, x) = max(0, 1-yw^T^x)

Gradient based algorithms can be used for this optimization problem (i.e. to find argmin~w~).

The regularization parameter can be viewed as a measure on how much we insist on the maximal margin classifier constraint. A small λ means we strongly insist.

See also <<choosing_regularization_parameter>> λ.

*to-do* what are SVMs in general. It seems that the above is just one example for classification, which choosed to choose loss~hinge~ as loss function.

Despide that the difference between the SVM and the Perceptron is quite small, that difference has a big influence. In practice, the SVM often performs much better.  In practice, we almost always prefer the SVM over the Perceptron.  For educational purposes, the Perceptron is easier to understand and serves as an good introduction.

There is not really a probabilistic interpretation, i.e. a likelihood function corresponding to the loss function, see also <<bayesian_interpretation_of_regularization>>.


[[kernelized_svm]]
=== Kernelized SVM

Turns the linear SVM into a non-linear model using the <<kernel_trick>>.  As in kernelized Perceptron, start with w = ∑~1≤i≤n~α~i~y~i~x~i~. Instead of w we now learn / optimize α. Note that ‖w‖~2~^2^ = w^T^w. Note that as in normal SVM, the regularization parameter λ remains.

α̂ = argmin~α~(∑~1≤i≤n~max(0, 1-y~i~α^T^k~i~ + λα^T^D~y~KD~y~α) +
k~i~ = (y~1~k(x~i~,x~1~), ..., y~n~k(x~i~,x~n~))

*to-do* define D~y~ and K

The resulting model is:

h(x) = signʹ(∑~1≤i≤n~α~i~y~i~k(x~i~,x))

The perceptron had (*to-do* move to right place):

α̂ = argmin~α~(∑~1≤i≤n~max(0, y~i~α^T^k~i~)

In general kernelized SVM are a very good choice for classification problems up to a few tens of thousands observations, i.e. should at leasts be tried.


[[multi_class_SVM]]
=== Multi-class SVM

An explicit multi-class classifier, i.e. it directly outputs a class {1, ..., K}. Similar to SVM (a binary classifier), but internally with K decision boundaries, thus K weights w^(1)^, ..., w^(K)^.  Instead the hinge loss, the multi-class hinge loss is used:

loss~multiclasshinge~(w^(1)^, ..., w^(K)^, x, y) = max(0, 1 + max~1≤j≤K,j≠y~((w^(j)^)^T^x - (w^(y)^)^T^x))

_Optimization_: E.g. SGD.

_Prediction_: h(x) = argmax~j~((w^(j)^)^T^x)


==== Rational for multi-class hinge loss

For each data point (x~i~, y~i~), we would like to find weight vectors (w^(1)^, ... w^(K)^) such that

(w^(y~i~)^)^T^x~i~ > max~1≤j≤K,j≠yi~((w^(j)^)^T^x~i~) + 1

The max~...~(...) can also be understood as the score of the runner-up classifier.  We want that our best classifier is better than the runner-up classifer by some margin, hence the ``+1'' term.  The constant 1 is flexible good enough due to regularization.  This leads us to the multi-class hinge loss, which is 0 only if the above condition is met, and otherwise is a measure of how much the above condition is violated.


=== Clustering

Unsupervised classification, i.e. can be seen as the unsupervised analog to (supervised) classification.

Standard approches:

- Hierarchical clustering

- Partitional approaches

- Model-based approaches

  * <<gaussian_mixture_model>>

  * <<k_means>>


Applications:

- Documents based on the words they contain

- Images based on image features

- Customers based on their purchase history

- ...


[[k_means]]
=== k-means clustering

Example of clustering.  Assumes points x = {x~1~, ..., x~n~} are in d-dimensional Euclidean space. Represent each cluster by the position of its center.  Each point in the hyperplane is assigned to the closest center, i.e. to the cluster associated with that center.  This induces a Voronoi partition.

Define the k centers μ = (μ~1~, ..., μ~k~) by minimizing average squared distance from each observation to each center.

μ̂ = argmin~μ~(loss~av_squared_dist~(x, μ)) +
loss~av_squared_dist~(x, μ) = ∑~1≤i≤n~min~1≤j≤k~(‖x~i~-μ~j~‖²)

Is a non-convex optimization, NP-hard in general.  Thus instead of trying to find the optimal solution, we use approximations, e.g. <<lloyds_heuristic>>.

Strategies for model selection (i.e. determining number of clusters k).

- _Heuristic quality measure_ (or _elbow heuristic_). Try k = {1, 2, ...}, each time run the optimization algorithm, e.g. the k-means algorithm. Stop when the cost was reduced by a neglectible amount. I.e. when loss(x, μ̂^(i-1)^)-loss(x, μ̂^(i)^) < threshold, where μ̂^(i)^ denotes the μ̂ of the i-th iteration, i.e. k=i.  Note that loss(x, μ̂^(i)^) monotonically decreases with i.

- _Regularization_: *to-do*

*to-do* move to some other place: In unsupervised learning, model selection is a much harder problem than in supervised learning, since there is no clear objective.  There are heuristics, but there is no really good answer (yet).

Pros / cons:

- Determining the number of clusters k is difficult. This is the major (unsolved) practical problem.

- In general converges to a local optimum, which might not be the global optimum.

- Number of iterations can be exponiential (even in plane), but in practice often converges quickly.

- The above two are practically not a big issue if we get the initialization right

- Cannot well model clusters of arbitrary shape (however see Kernel-k-Means)

- Sensitive to outliers. E.g. imagine two clearly separated clouds, and one outlier far away from both clouds. With k=2, one group would be the two clouds, and another group would be the single outlier.

- Similarly, indirectly assumes clusters are circular (because points are assigned to nearest center, and `nearest' implies circular around center)


[[lloyds_heuristic]]
=== k-means algorithm / Lloyd's heuristic

Is a heuristic to solve the optimization problem posed by k-means clustering.

Algorithm: Initialize each of the k centers with an initial best guess.  While not converged: Assign each data point to closest center. Update/recalculate each center by averaging its assigned data points.

Guaranteed to monotonically decrease loss~av_squared_dist~(x, μ) in each iteration.

Strategies for initializing the k centers:

- _Random seeding_: Uniformily at random pick data points as centers.  But then chances are that initially, large clusters have more than one center and small clusters have no center. This initial situation might make it difficult for the algorithm to find the clusters initially not having a center.

- _K-Means++_: Let D(x) denote the shortest distance from a data point x to the closest center we have already chosen.  Uniformily at random choose a data point as first center. Then k-1 times: Choose data point x~i~ ∈ x as center with probability D(x~i~)²/∑~1≤j≤n~D(x~j~)².


=== Dimension reduction

Unsupervised regression, i.e. can be seen as the unsupervised analog to (supervised) regression.

Assume 𝓓 = {x~1~, ..., x~n~} ⊆ ℝ^d^. The goal is to find a mapping _f_:ℝ^d^→ℝ^k^ where _k_≪d is a parameter.  I.e. f projects vectors x ∈ ℝ^d^ into a k-dimensional subspace.


==== Linear dimension reduction

We assume centered data 𝓓, i.e. zero mean.

Educated guessing leads us to believe that the points {x~1~, ..., x~n~} ⊆ ℝ^d^ actually all approximately lie near a k-dimensional hyperplane.  E.g. k=1 means all lie near a line, k=2 means all lie near a 2D plane.  Thus want to find a k-dim hyperplane given by matrix _W_ ∈ ℝ^d⨯k^ going through the origin and approximately through all points.  The point __x̄~i~__ on hyperplane closest to x~i~ is x~i~'s dimension reduced version.  We constrain the matrix W to be orthogonal and having columns of length 1 in Eucledian norm.  Now it's easy to see, especially in the line case k=1, that x̄~i~ = z~i~W, for some scalar __z~i~__ ∈ ℝ.  We want to minimize the sum of squared distances.  Or in other words, we want to minimize the reconstruction error of the projection, measured in Eucledian norm.  The matrix W is given by the following optimization problem.  Here it's deliberately written down in a verbose form to express the logical background:

Ŵ = argmin~W~(∑~1≤i≤n~‖x~i~-x̄~i~‖~2~²) +
x̄~i~ = z~i~W +
z~i~ = W^T^x~i~  [z~i~∈ ℝ is just a scalar]

having the closed form optimal solution

Ŵ = (v~1~|...|v~k~) +
v~i~ = Eigenvectors of empirical covariance ∑ = 1/n ∑~1≤i≤n~x~i~x~i~^T^

*to-do* really? I need k vectors

The mapping can then be stated as

f(x) = W^T^x

Choosing k:

- for visualizatoin: by inspection :-)

- for feature induction: by cross validation

- otherwise: so that most of the variance is explained, see also k-means


===== Deduction of optimization problem

Let's start with the simple k=1 case. We want to find a line given by w∈ℝ^d^ which goes through the origin and approximately through all points x~i~.  That line is then a ℝ^1^ subspace of ℝ^d^.  The point x̄~i~ on the line being closest to x~i~ is the reduced version of x~i~.  The goal is to minimize the sum of squared perpendicular distances between x~i~ and the line, i.e. between x~i~ and x̄~i~.  We don't know yet what the value of x̄~i~ is, but we know it's on the line given by w, so we parameterize it by a scalar c~i~∈ℝ yielding x̄~i~ = c~i~w.  In order that the different c~i~w are comparable, we must normalize w, i.e. we demand |w|=1. The formal optimization problem then is

ŵ,ĉ = argmin~|w|=1,c~(∑~1≤i≤n~‖x~i~-x̄~i~‖~2~²) +
x̄~i~ = c~i~w

Assuming we know the optimal w, then the scalars are determined by c~i~=w^T^x~i~

ŵ = argmin~|w|=1~(∑~1≤i≤n~‖x~i~-ww^T^x~i~‖~2~²) +

Which can be shown (just do the squaring and a bit of math, and using that the data is normalized to have zero mean) to be equivalent to the following, where ∑ = 1/n ∑~1≤i≤n~x~i~x~i~^T^ is the empirical covariance.

ŵ = argmax~|w|=1~(w^T^∑w)

This optimzation problem has a closed form optimal solution:

ŵ = principal eigenvector of ∑

For arbitrary k, we want to find a k-dimensional hyperplane, given by matrix W ∈ ℝ^d⨯k^, which goes through the origin and approximately through all points x~i~.  In order that W defines a d-dimensional hyperplane it must be orthogonal.  In order that the ``Wc~i~'' terms in the following formula below are comparable, each column vector of W must have unit length.  The goal is to minimize the sum of squared perpendicular distances from each point to the hyperplane, leading to the following optimization problem, known as _principal component analysis_ (_PCA_).

Ŵ,ĉ = argmin~W,c~(∑~1≤i≤n~‖x~i~-Wc~i~‖~2~²)

The PCA optimization problem has a closed form solution, nameley the principal Eigenvectors v~i~, i.e.

Ŵ = (v~1~|...|v~k~)


=== Gaussian based Bayes classifiers

==== Gaussian Bayes Classifier (GBC)

The most general form of the GBC based classifiers.  As in Gaussian naive Bayes classifier (GNB), but we no longer assume that features are conditionally independent given a label.  GNB has a nicer introduction than this chapter.

p̂(y) = count(i s.t. y~i~=y)/n [K-element empirical distribution] +
p̂(x|y) = 𝓝~d~(x; μ~y~, ∑~y~)

Estimating the parameters with MLE, the closed form solution is:

μ̂~y~ = 1/count(i s.t. y~i~=y) ∑~i s.t. yi=y~x~i~  [a d-element vector] +
∑̂~y~ = 1/count(i s.t. y~i~=y) ∑~i s.t. yi=y~(x~i~-μ̂~y~)(x~i~-μ̂~y~)^T^ [dxd covariance matrix]

The predictive distribution then is, using Bayes' rule:

p̂(y|x) = p̂(x|y)p̂(y)/p̂(x) +
p̂(x) = ∑~1≤i≤n~p̂(x|y~i~)p̂(y~i~)

In case of binary classification, the discriminant function f(x) fulfilling f(x) = log(p(y=+1|x)/(p(y=-1|x))) is the following:

f(x) = ... really complicated ... [discriminant function]

Pro: Captures correlations among features

Pro: Avoids overconfidence

Contra: Computational complexity O(K+d²)

Contra: Can not handle discrete features. But see <<categorial_naive_bayes_classifier>>.

*to-do* how robust is it in general if the `data is Gaussian' - assumption is not met?


==== Gaussian naive Bayes classifier (GNB)

Special case of GBC: features are conditionally independent given a label, i.e. ∑~y~ = diag(σ²~y,1~, ..., σ²~y,d~).

We follow the general recipe of estimating p(x,y) = p(x|y)p(y) by estimating p(x|y) and p(y).  p̂(y) is estimated by the emprical distribution of class labels {y~1~, ..., y~n~}.  We model the given features as conditionally independent given a label, yielding p̂(x|y) = ∏~1≤j≤d~p(x^(j)^,y). I.e. given a class label y, each feature x^(j)^ of a feature vector x is `generated' independently of the other features.  That the features are conditionally independent given a label is an unrealistic assumption, but it generally works well in practice. We model features by conditionally independent Gaussians: p̂(x^(j)^,y) = 𝓝(μ~y,j~, σ²~y,j~).  Summary:

p̂(y) = count(i s.t. y~i~=y)/n [K-element empirical distribution] +
p̂(x|y) = ∏~1≤j≤d~p(x^(j)^,y) [assumes features are mutually independent] +
p̂(x^(j)^,y) = 𝓝(μ~y,j~, σ²~y,j~) [assumes each feature is normally distributed]

Estimating the parameters with MLE means estimating the mean and variance of a population given a sample, which is done by computing the mean and variance of the sample:

μ̂~y,j~ = mean(all x~i~^(j)^ where y~i~=y) +
σ̂²~y,j~ = var(analogous)

The predictive distribution then is, using Bayes' rule:

p̂(y|x) = p̂(x|y)p̂(y)/p̂(x)  +
p̂(x) = ∑~1≤i≤n~p̂(x|y~i~)p̂(y~i~)

pro: rather simple, closed form solution

contra: can be overconfident (probability close to 0 or 1), if the assumption that the features are conditionally independent given a label doesn't hold, which almost always is the case.  If we only care about he most likely class, e.g. in the case of MAP prediction, that might be fine.


==== GNB, binary classification and class independent variance

The following discusses the special case of GNB restricted to binary classification and class independent variance σ²~j~, where j∈[d] designates the feature.

Starting with choosing the common discriminant function f(x) = log(p(y=+1|x)/(p(y=-1|x))), yields the following closed form solution for f(x):

f(x) = w^T^x + w~0~ [discriminant function] +
w~0~ = ... complicated ... +
w~j~ = (μ~+,j~ - μ~-,j~) / σ²~j~

Prediction then is:

p(y=1|x) = sigm(f(x)) = sigm(w^T^x + w~0~)

If the GNB's model assumptions are met, i.e. if p(y,x) comes from a GNB model with class independent variances, p(y=1|x) is a logistic regression and hence GNB and logistic regression will make the same predictions.


==== Quadratic discriminant analysis

Special case of GBC: Restricted to binary classification.

The discriminant function f(x) fulfilling f(x) = log(p(y=+1|x)/(p(y=-1|x))) is:

f(x) = ... really complicated ...

Predictions can be made by

h(x) = signʹ(f(x))


==== Fisher's linear discriminant analysis (LDA)

Special case of GBC / quadratic discriminant analyis: Restricted to binary classification, an uninformative prior p̂(y) = 0.5, and equal covariance matrices ∑ = ∑~p~ = ∑~n~.

The discriminant function f(x) fulfilling f(x) = log(p(y=+1|x)/(p(y=-1|x))) is:

f(x) = w^T^x + w~0~ +
w = ∑^-1^(μ~p~-μ~n~) +
w~0~ = 1/2(μ~n~^T^∑^-1^μ~n~ - μ~p~^T^∑^-1^μ~p~)

Prediction then is:

h(x) = sign(f(x)) = sign(w^T^x + w~0~)

Predictive distribution (note that it has the same form as logistic regression):

p(y=+1|x) = 1/(1+exp(-f(x))) = sigm(w^T^x + w~0~)

If model assumptions are met, LDA makes the same predictions as Logistic Regression.

Contra: Not very robust against violations of assumptions.

*to-do* Isn't that statement applicable to all GBC based methods?


==== Gaussian-Mixture Bayes classifier

Each class is modeled by a set of Gaussian humps, i.e. by a Gaussian mixture.  Can be seen as bringing Gaussian mixture model (GMM) and Gaussian Bayes Classifier (GBC) together.  This allows to better model arbitrary p̂(x|y); GBC is quite restrictive by assuming that p̂(x|y) is a single Gaussian hump.

GBC had

p̂(x|y) = 𝓝~d~(x; μ~y~, ∑~y~)

Now we have

p̂(x|y) = ∑~1≤j≤humpcount(y)~w~y,j~𝓝~d~(x; μ~y,j~, ∑~y,j~) +
w~y,j~ ≥ 0, ∑~1≤j≤humpcount(y)~w~y,j~=1


[[categorial_naive_bayes_classifier]]
==== Categorial naive Bayes classifier

As Gaussian naive Bayes classifier, but for discrete features.  No assumptions on the form of p̂(x|y). Instead, based on empirical distributions retreived from the given data.

There is no variant which drops the assumption that features are conditionally independent given a label.  This is because we don't assume any form of p̂(x|y), i.e. each possible empirical distribution would be a candidate.  E.g. if the features are binary, then there are 2^d^ different feature vectors, and for each of those we would need a probability.  In the Gaussian case, there are only d (means) + d² (covariance matrix) parameters.

*to-do* 05:15 00:10:00


==== Mixed conditional distributions

Let each feature have its own individual conditional distribution p̂(x^(j)^,y). A group of features can be assumed as conditionally independent given a label, another group can be assumed to be conditionally dependent given a label.


[[gaussian_mixture_model]]
=== Gaussian mixture model (GMM)

Example of clustering.  As a Gaussian Bayes Classifier, but unsupervised, i.e. labels are missing.  The number of clusters K must be guessed.  For each cluster, there's a Gaussian hump, parameterized by mean μ~j~ (d-dimensional vector) and covariance matrix Σ~j~ (d⨯d matrix), in 𝓧 space.  Each hump is weighted by w~j~, where Σ~1≤j≤K~w~j~ = 1 and w~j~≥0.  I.e. w~j~ plays the role of p(y=j) in GBC.

θ = (w,μ,Σ) +
p(𝓓|w,μ,Σ) =~(iid xi)~ ∏~1≤i≤n~p(x~i~|w,μ,Σ) +
p(x|w,μ,Σ) = ∑~1≤j≤K~w~j~𝓝~d~(x;μ~j~,Σ~j~) +
w = (w~1~, ..., w~K~) +
w~j~ represents p(y=j) +
μ = (μ~1~, ..., μ~K~) +
μ~j~ is a d-dimensional vector, μ~j,l~ being mean of j-th cluster w.r.t. l-th feature +
Σ = (Σ~1~, ..., Σ~K~) +
Σ~j~ is the d⨯d covariance matrix of the d-dim normal distribution of the j-th cluster

Finding the MLE with respect to p(𝓓|w,μ,Σ) yields the following nonconvex optimization problem.  We use the trick that taking the log of p(x|w,μ,Σ) doesn't affect the result of argmin, but allows us to compute a sum instead of a product, which in turn makes it an easier optimization problem.

(ŵ,μ̂,Σ̂) = argmax~w,μ,Σ~(p(𝓓|w,μ,Σ)) = argmax~w,μ,Σ~(∑~1≤i≤n~ log p(x|w,μ,Σ)) +
subject to: Σ~j~ s must be symmetric positive definite, ∑~1≤j≤K~w~j~=1, w~j~≥0

The optimization problem doesn't has a closed form solution, partyly due to the sum inside the log.  Also it is non-convex and has constraints.  If it was only non-convex, we could e.g. use SGD.  To solve this optimization problem, for example the EM algorithm can be used.  Recall that solving the optimization problem for GBC was relatively easy.  I.e. if we had the labels, solving the above optimization problem is relatively easy.  Thus the EM algorithm is adequate, since it alternates between estimating labels and estimating the parameters of interest using the previously estimated labels.


=== Gaussian processes

Informally introducing Gaussian processes with an example.  Say I measure my heart rate at 08:00 and at 08:01 on many days.  The measurements at 08:00 are represented by a random Gaussian variable, and the measurements at 08:01 by another random Gaussian variable.  Together that yields a multivariate (2 dimensions) Gaussian distribution.  The covariance matrix is likely showing a correlation, since the heart rate 1 min appart is correlated.  Now say I measure not only at 08:00 and 08:01, but also on 08:02 and 08:03 etc.  I still get a multivariat Gaussian, but this time with more dimensions.  In a time - heartrate plot, I could plot one picewise linear curve per day by connecting the measurements of that day.  Now imagine we make countably infinitely many measurements per day.  The joint distribution is a multivariate Gaussian with countably infinitly many dimensions.  Each piecewise curve of a day in the time - heartrate plot can now in the limit, as we have countably infinitely many measurements per day making the piecewise steps infinitely small, be seen as a function.  We say f(x) ~ GP.

f \~ GP(m(·),k(·,·)) denotes that f is a Gaussian process.  A Gaussian process is fully defined by mean function m(·) and kernel function k(·,·).  f is a Gaussian process if (f(a~1~), ..., f(a~n~)) \~ 𝓝~n~(m(a⃗),K(a⃗,a⃗ʹ)), where a⃗ = (a~1~, ..., a~n~) is a vector of values.  The elements a~i~ are often themselves vectors, namely the feature vectors.

Very often the mean function is set to zero, i.e. m(x) = 0. The really interesting part is the covariance matrix.  Also in the following, we set m(x) = 0.

The prior for y is only dependent on the Gaussian noise:

p(y|σ²,k) = 𝓝~1~(y|0,σ²+k(0,0))

Model of joint distribution of the y s which correspond to the given n feature vectors. As always y~i~ corresponds to feature vector x~i~. The feature vectors are given by design matrix X = matrix[x⃗~1~, ..., x⃗~n~], where x⃗~i~ denotes the feature vector of the i-th observation.  Note that the labels y⃗ = (y~1~, ..., y~n~) of the data are _not_ involved.

p(y⃗|X,k,σ²) = 𝓝~n~(y⃗|0⃗,C~n~) [y⃗ denotes generic variables, _not_ the labels] +
C~n~ = K + σ²I~n~ [n⨯n covariance matrix] +
K = (k(x⃗~1~,x⃗~1~), ..., k(x⃗~n~,x⃗~n~)) [n⨯n _Kernel matrix_ (or _Gram matrix_)] +
k(x⃗,x⃗ʹ) [_kernel function_ (or _covariance function_)]

Example with two feature vectors x~1~ and x~2~, x~1~ and x~2~ being non-similar, i.e. far apart from each other.  Because they are non-similar, k(x~1~, x~2~) will be very small, and thus the Gram matrix C~2~ will be nearly diagonal.  𝓝~2~(y⃗|0⃗,C~2~) with a nearly diagonal C~2~ means one Gaussion hump centered at (0,0) which is nearly rotation symmetric.  y~1~ and y~2~ are nearly independent.  I.e. knowing y~1~ doesn't tell much about y~2~ and vice versa.  Note that how many features there are per feature vector is irrelevant.  Only the value of k(x~1~, x~2~) is of interest, i.e. the value the resulting Gram matrix C~2~.

Example as before, but with x~1~ and x~2~ being similar, i.e. close. The Gram matrix C~2~ will be dense, far from being diagonal.  The Gaussian hump will be a narrow elongated hill centered at (0,0), the long ridge being in some direction.  Knowing y~1~ tells you pretty much about y~2~ and vice versa.

*to-do* Is this also regarded as a prior? We take the design matrix into account, but not yet the labels. Kind of a semi-prior.

*to-do* Explain what that means, i.e. why it's usefull despite not having parameters (beside the hyperparameters of the kernel).

*to-do* make clear that the above is the superposition of two gaussian processes, the function I wish to learn and the Gaussian noise

*to-do* distinguish between when we have no data, an highlight this as being the prior p(y), and the case when we have data X and y⃗. I think above y⃗ is meant in a generic sense, not the actual labels of the observations y⃗.

As a first step towards prediction, let's look at the joint distribution of the y s which correspond to the given (n+1) feature vectors when there's one more observation x̃⃗.  The labels of the data are _not_ used.  y⃗ and y denote generic variables.

p(vector(y⃗,y)|x̃⃗,X,k,σ²) = 𝓝~n+1~(vector(y⃗,y)|0⃗,C~n+1~) +
C~n+1~ = matrix(row(C~n~, k⃗), row(k⃗^T^, c)) [(n+1)⨯(n+1) covariance matrix] +
k⃗ = (k(x⃗~1~,x̃⃗), ..., k(x⃗~n~,x̃⃗)) [vector length n+1, similarity x̃ to X] +
c = k(x̃⃗,x̃⃗) + σ² [scalar]

Predictive distribution of y corresponding to a new observation x̃⃗.  Now here y⃗ does denote the labels of the data.  As always, a predictive distribution communicates the uncertainty about the prediction, here pretty directly directly via the variance σ̃².  Note that each prediction of an y given a x̃⃗ involves all the data (X,y⃗), as e.g. in KNN.  The formula follows from the above joint distribution of adding a new observation and the rules for conditional Gaussian distributions (2.81 and 2.82 in Pattern Recognition and Machine Learning).

p(y|x̃⃗,𝓓=(X,y⃗),θ=(k,σ²)) = 𝓝~1~(y|μ̃,σ̃²) +
μ̃ = k⃗^T^C~n~^-1^y⃗ [scalar] +
σ̃² = c-k⃗^T^C~n~^-1^k⃗ [scalar]

*to-do* Those plots which already have some data points, and plot the mean and an standard deviation envelop, how do they do it? Do they for each point they want to plot take the mean and the upper/lower standard deviation from p(ỹ|x̃⃗,X,y⃗), where x̃⃗ is the point on the x axis and p(ỹ|x̃⃗,X,y⃗) then delivers mean and upper/lower standard deviation?

*to-do* these drawings where with no data we just see the prior as a linear band, and as we add data pionts we narrow the band near the data points. Depending on the kernel/covariance, the einschnitt around a data point is wide or narrow.

*to-do* drawing with no observations, i.e. just the prior. Theres the mean, the upper/lower `bound', and a few examples of f.  The kernel defines how `wiggly' the f s will be.

*to-do* drawings representing overfitting and underfitting. e.g. at 42:50

*to-do* model selection (i.e. finding kernel hyperparameters) via marginal likelihood

f(x) ~ GP(m(x),k(x,xʹ)):

- f(x) is a random variable having a gaussian distribution with mean m(x) and variance given by k(x,xʹ), where xʹ is just some xʹ∈𝓧

- f(·) is not really a function. It can be kind of viewed as function because f is a vector of jointly Gaussian variables of uncountably infinite length.

- f(x) is not necessarily continous

- gaussian process: think distribution over function values (_not_ functions)

GP is an non-parametric model.  Non-parametric models have a finite but unbounded number of parameters that grows with the data.  This is informal definition is refinment refinment of "non-parametric models have an infinite number of parameters", which many authors use.  We see that looking at the predictive distribution.  All our data is involved in calculating p(ỹ|x̃⃗,X,y⃗).  The more data / observiations we have, the more parameters the model has, i.e. the more flexible it is.

*to-do* I don't really understand why the model should be more flexible when we have more data. Having these plots in mind where the more observetions we add, the more the bands are narrowed down, implies to me that there are less functions, i.e. the model is more restricted.

Predictive uncertainty (i.e. the thickness of the `bounding bands' in a plot) is data independent (the labels; the features we have). This is both a good and a bad thing.  Imagine the extreme case where we have many observations spread in the x space, but the respective y s are pretty random. Then the σ̃² at every x is pretty small, i.e. narrow band in the plot, which doesn't seem right considering that the observed y s are rather random.

Note that we can just do linear algebra, though it involves inverting a two dimensional n⨯n matrix.  Despide the Baysian philosophy, we don't have to integrate, which we usually have to do all over in the Baysian world.

Referennces:

- MLSS 2012: J. Cunningham - Gaussian Processes for Machine Learning: https://www.youtube.com/watch?v=BS4Wd5rwNwE, https://www.youtube.com/watch?v=KcB8c3a4LYU

- http://katbailey.github.io/post/gaussian-processes-for-dummies/

- Book: Gaussian Processes for Machine Learning, Rasmussen & Williams


==== Kernels & Kernel matrix

See also <<kernels>> *to-do* how do the kernels of GP and the kernels of kernelized perceptron etc relate?

k:𝓧⨯𝓧→ℝ

A kernel _models similarity_, i.e. k(x,xʹ) returns a measure of similarity between x and xʹ.  A kernel encodes our _prior_ assumption about the function which we wish to learn.  More precisely, the similarity is actually measured in a implicitely defined Eucledian space.  x and xʹ are converted to that Eucledian space, the similarity is measured in the Euclidian space and returned.  That the similarity is actually measured in an Eucledian space is the reason for the required properties of the kernel such as symmetry and positive semi-definitness.  The notion of similarity depends on the application.

k must be a Mercer kernel.  It must be symmetric (k(x,xʹ)=k(xʹ,x)).  For the continuous case, it must be positive semi-definite.  In the discrete case, the corresponding Gram matrix must be positive semi-definite.  The kernel functions often have hyper parameters.

*to-do* is "mercer kernel" equivalent to "symmetric and positive semi-definit"?

Different kernels have different _invariance properties_. For example, invariance to rotation or translation. E.g. in recognition of handwritten digits, translatinig an image should not affect similarity.

Kernel matrix K must be positive semi-definite (i.e. x⃗^T^Kx⃗≥0 ∀x⃗), which it is if k is a Mercer kernel.

The more diagonal the kernel matrix is, the more wiggly the function will be, since from knowing one x, one doesn't learn much about a close by xʹ.  If the Gram matrix is diagonal, then the y s are independent.

For some concrete kernels, see <<concrete_kernels>>


==== Application example: controller

We look at the controller of a drone.  The goal is to find the set of controller parameters which maximize some performance measure.  We want to find that set in a learning by doing fashion.  We try one set, measure the performance, try another set, and so on.  However we cannot just try out any set, because some controller sets are really bad and would crash the drone. A crash makes prototyping too expensive, and we want to avoid that case.  For simplicity lets assume only there's only one controller parameter.  We make an educated guess what a first safe parameter would be and denote that x~1~.  We make a measurement of the resulting performance y~1~.  Imagine a performance vs controllerparameter plot showing the predictive distribution p(y|x⃗,𝓓).  The Gaussian process is now pretty certain about the performance near x~1~, and gets less certain the further away from x~1~ we are. When the lower two standard deviation line is below some threshold, we deem the respective parameter x as too unsafe to try out.  Or the other way round, we only try out new controller paremeters x where the lower two standard deviation line is above the threshold.  There are two reasons to try out a new parameter x.  One is trying out new parameter x at the border of the safe region, to make the save region wider, which will happen because the Gaussian process than has more data.  The other is to try out more parameters x in regions where the current mean curve of the predictive distribution indicates a maximum, to be more sure we found the precise global maximum.

References:

- https://www.youtube.com/watch?v=7ZkZlxXHgTY


=== Uncertainty sampling

Form of active learning.  Based on a probabilistic model yielding predictive distribution p(y|x,𝓓).  We incrementally train the model, by always quering the label y~i~ for the x~i~ we're most uncertain about.  Say we have a two-class classification problem.

--------------------------------------------------
D' = ∅
while not satisfied
  Train model using D'
  i = argmin~i ∉ D'~(score(xi))
  Query label yi, add i-th observation to D'
--------------------------------------------------

Possible score function to be maximized: score(x) = -|p(y="+"|x)-0.5|


=== Artificial Neural Neutwork (ANN)

The term _Artificial neural network_ refers to nonlinear functions which are nestsed compositions of (variable) linear functions composed with (fixed) nonlinearities.  I.e. the linearities are learned, and the non-linearities are choosen apriory and then are fixed.

Recall basis functions:

h(x) = ∑~0≤j≤d~w~j~Φ~j~(x) +
ŵ = argmin~w~(∑~1≤i≤n~l(y~i~,h(x)))

The key idea is to paramaterize feature maps Φ~j~(x) (later called activation function, denoted φ), thus becoming Φ(x,θ~j~), and to optimize not only on w, but also θ. I.e. we try to learn good features from data.

h(x) = ∑~0≤j≤d~w~j~Φ(x,θ~j~) +
ŵ = argmin~w,θ~(∑~1≤i≤n~l(y~i~,h(x)))

In neuronal nets, we nest, i.e. x is given by the previous layer.

_Deep learning_ generally refers to models with nested, layered non-linearities. A classical example is an ANN with many hidden layers, trained via SGD or variant thereof.

Pro: Overall training its largely matrix / vector operations, which can be efficiently and in parallel be computed in (dedicated) HW, e.g. GPUs.

Pro/Contra: Good for clean data, bad for noisy data. If data is noisy, the neuronal net is going to learn that mistakes and then repeat them.

Contra: User has to configure many parameters (like e.g. architecture of the network, parameters of the learning algorithm).

Contra: Not roboust. Training the same model multiple times gives different results.


==== Feedforward neural network

In case of zero hidden layers also called _singlelayer perceptron_, in case of one or more hidden layers also called _multilayer perceptron_.

Say there are L hidden layers.  Imagine L+2 columns (or _layers_) of _units_ (or _neurons_, or _nodes_) {u^(0)^, u^(1)^, ... u^(L+1)^}. The first column corresponds to the input x, i.e. u^(0)^ = x, the last column corresponds to the output y, i.e. y = u^(L+1)^.  The layers between the input and the output layer are called _hidden layers_. The units in the hidden layers are called _hidden units_.  As in linear regression / classification, the intercept / bias is absorbed into w, meaning that the true u^(l)^ is extended by prepending an element which is always 1, resulting in the actually used u^(l)^.  The units of a given column depend on the units of the previous column the following way, where φ(·) is a fixed (i.e. without parameters that we would need to learn), possibly nonlinear, _activation function_, and w~j~^(l)^ is the parameter / weight vector of the j-th unit in the l-th (transformation-)layer.

l-th layer, j-th unit: +
z~j~^(l)^ = (w~j~^(l)^)^T^u^(l-1)^ +
u~j~^(l)^ = φ(z~j~^(l)^)  +
w~j~^(l)^ = weight vector for j-th unit in l-th layer, |w~j~^(l)^| = |u^(l-1)^| +
u~j~^(0)^ = x +
y = u~j~^(L+1)^ +
output layer: in regression ommits φ(·), in classification φ = signʹ

Or in shorter notation, l-th layer:

u^(l)^ = φ(u^(l-1)^) [here φ returns a vector; is applyied component wise] +
z^(l)^ = W^(l)^u^(l-1)^ +
W^(l)^ = |u^(l)^| ⨯ |u^(l-1)^| weight matrix for l-th layer

In case of a single hidden layer network, having k units in the hidden layer and a single output:

h(x) = ∑~1≤j≤k~wʹ~j~φ(w~j~^T^x) +
|wʹ| = k, |w~j~| = |x| = n

In case of no hidden layers, and a single output, we just get linear regression:

h(x) = w^T^x

In the spirit of the introduction to the neuronal networks chapter, the parameter vectors w~i,j~ of the hidden layers correspond to the parameter set θ parameterizing the feature maps. That the output layer ommits applying φ(·), can be see from the h(x) = ∑~0≤j≤d~w~j~Φ(x,θ~j~) of the introduction.

In principle, each layer, or even each neuron can have its individual non-linearity φ.

_prediction_: ŷ = h(x) = u^(L+1)^, and u^(L+1)^ is recursively given above. Starting at input x and computing each unit is called _forward propagation_.

_Design decisions_: Number of layers, number of units per layer, non-linearity function(s) used, whether there's one global non-linearity function or one per layer or even one per unit.

There is no consensus what the n in a n-layer neuronal network counts. 1) The number of transformations (the last being only linear) 2) the number of layers of hidden units 3) the number of layers of units (input, output and hidden).

_Universal approximation theorem_: Neuronal networks are _universal adaptors_. A neuronal net with one hidden layer and a continuous sigmoidal activation function can approximate any continuous function on a compact input domain to arbitrary accuracy, provided sufficiently many hidden units.


==== Avoiding overfitting

Neuronaly networks are highly flexible models due to having many parameters |W|, which might be even larger than number n of observations, thus there's considerable risk of overfitting.  If there are many hidden units, the universal approximation theorem basically guarantees overfitting.

_Early stopping_ (a form of regularization): Don't run SGD until it converges, i.e. finds a local minima. For example, monitor empirical risk on the valdation set, and stop once validation error starts to increase.  Due to the universal approximation theorem, the global optimum tends to overfitt, i.e. fit the noise,  and we want to stop at the piont overfitting starts to happen. 

_Dropout regularization_: During training, in each iteration, ignore some hidden units (`remove' them from the net, but only for this iteration).  Each hidden unit has a probability of p (e.g. 1/2) of being ignored.  At the end, multiply all weights with p.

_Regularization_: One way to mitigate the problem is to add regulariztion the usual way by adding a regularization term: Ŵ = argmin~W~(∑~1≤i≤n~l(W,x~i~,y~i~) + λ‖W‖~2~²).  However then we need to choose the regularization parameter λ, which we usually do via cross validation.  However even one training of an ANN is quite expensive, so in general CV is often not feasible.


[[ann_invariances]]
==== Invariances

E.g. in handwritten digit recognizition, when the written digit is translated or scaled, that should not lead to a worse recognition.

Solutions:

- Augmentation of the trainin set. E.g. for each handwritten digit, add a few images containing translated & scaled versions.  But it's not an efficient solution, since it increases the number of training observations.

- Structure of ANN shall reflect invariances, see <<convolutional_nn>>

In certain fields, there are feature classes which already have nice properties. E.g. in computer vision, there's _SIFT_ (scale invariant feature transosformation), in speech recognition there's _Ceptrum_.


==== Learning a feedforward neural network

We need to learn the weight matrices W = (W^(1)^, ..., W^(L+1)^).  Other parameters of a neuronal network are fixed with respect to learning: number of hidden layers, number of units for each hidden layer, the activation function to be used by each unit, the activation functions themselves have no parameters which need to be learned.  Choose some loss function (e.g. percetpron loss, multi-class hinge loss, square loss etc.).  If there are multiple output units, typically the overall loss is the som of per-output losses.  Do empirical risk minimization Ŵ = argmin~W~(∑~1≤i≤n~l(W,x~i~,y~i~)). Is in general a non-convex optimization problem, even with a single hidden layer.  Typical choices are variants of gradient descent, despite the risk of only finding a local minima. In this solutions, initialization (where to start the search) really matters. Nevertheless, for reasons not well understood, gradient based algorithms tend to find good models.  Also, somehow we're much more likely to find the global optimum when we overspecify, i.e. have more hidden units than we actually minimaly need in theory for a good model to exist.

_Backpropagation_ is a method of computing the gradient ∇~W~l(W) for one iteration in SGD (or a variant), to adjuts the weights W -= η~t~∇~W~l(W).  Backpropagation works by computing the loss of one data point at each unit in the output layer (were the true label y is known from the training data and the current prediction ŷ is known based on the current weights W), and then work backward layer by layer.

*to-do* Is the following specific to ANN, or is it simply `SGD (and variants) and non-convex objective functions'?

_Initialization_: Since the optimzation problem is non-convex, i.e. there might be multiple local minimas, initialisation of the weights W matters a lot.  It turns out that random initialization works well in practice. E.g. w~ij~ \~ 𝓝(0, 0.01) or w~ij~ ~ 𝓝(0, 1/√|layer|).  You might want to repeat training multiple times to avoid taking weights resulting from a local minima.

_Choosing the learning rate η_ parameter of the SGD algorithm: One strategy which proved to be practical is to start with a small (say 0.1) learning rate, and after some iterations decrease slowly. So for example η~t~ = min(0.1, 100/t).  Additionally, you may want to ensure that the weight change resulting from the learning rate is within some constant factor (smaller 1) of the absolute weight.

_Learning with momentum_: A common extension to SGD. Idea: move not only in direction of gradient, but also in direction of the last weight update(s). E.g. if only the last weight update is taken into account: ∆W = m∆W + η~t~∇~W~l(W), W -= ∆W, where m is a further parameter.  In a slightly more complex variant, we take the average of the last N updates.

_Preventing oscillation_ around minima: Learning with momentum helps.


[[convolutional_nn]]
==== Convolutional neuronal network (CNN)

A _convolutional neuronal network_ are is an ANN for specialized applications, in particular image recognition.  It is designed to attack problem of <<ann_invariances>>.   They bake a certain degree of shift and scale invariance into the model. 

At a high level, this is done by sharing weights.

Imagine recognition of handwritten digits. A hidden unit in the first hidden layer is connected only to the input corresponding to a patch of closeby pixels (i.e. all other weights are set to zero). The weights of each patch are constrained to be similar.

CNN layer architecture: input -> (convolution -> pooling)+ -> fullyconnected+ -> output, where + means "one or more". I.e. relative to ANN, "(convolution -> pooling)+" is inserted.

The convolution layer does kind of filtering. The pooling layer does kind of subsampling, having the benefit of fewer overall units, i.e. fewer parameters to learn.


==== Common activation functions

_rectified linear units_ (_ReLU_) φ(z) = max(z,0): Similar to the perceptron loss, just flipped at the y axis. These days kind of the standard. Pro: fast to compute, gradients do not `vanish' (important for deep networks), Contra: not differentiable.

_sigmoid activation function_ φ(z) = 1/(1+exp(-z)): φ(-∞)=0, φ(0)=1/2, φ(∞)=1, increases monotonically, close to zero it's approximately linear.

_tanh activation function_ φ(z) = tanh(z) = (exp(z)-exp(-z))/(exp(z)+exp(-z)): φ(-∞)=-1, φ(0)=0, φ(∞)=1, increases monotonically, close to zero it's approximately linear.


[[kernels]]
== Kernels

Allows to work implicitly (i.e. without actually calculating Φ(x)) in high-dimensional feature spaces (see <<basic_functions>>), as long as we can compute inner products k(x,k') = Φ(x)^T^Φ(x) efficiently. Indeed often k(x,k') can be computed more efficently than computing Φ(x)^T^Φ(x) explicitely.

[[kernel_trick]]
_Kernel trick_ (or _kernelization_): Express current learning algorithm s.t. it only depends on inner products. Replace those inner products by kernels.

_Required properties of kernel_: Is a function k:𝓧⨯𝓧→ℝ, where 𝓧 is the data space, satisfying symmetry (k(x,xʹ)=k(xʹ,x) ∀ x,xʹ∈𝓧) and positive semi-definitess (kernel matrix (or gram matrix K = (k(x~i~,x~j~))~1≤i≤n,1≤j≤n~ must be positive semi-definit for any sample {x~1~, ..., x~n~} of 𝓧, for any n).

Verifying wether a kernel is positive semi-definit is generally not that easy, so one practical way of getting new kernels are the kernel composition rules and valid known kernels.

[[kernel_engineering]]
_Kernel composition rules_ (or _kernel engineering_):

- k = k~1~ + k~2~
- k = k~1~ · k~2~
- k = c·k~1~ for c > 0
- k = p(k~1~), where p is a polynomial with positive coefficients
- k = exp(k~1~)
- preprocess data: k(x,xʹ) = k~1~(foo(x),foo(xʹ))

_Prediction_: Prediction is often roughly of the form h(x) = ∑~1≤i≤n~α~i~k(x~i~,x). That intuitively means that at there's a kernel function centered at each data point x~i~, weighted with α~i~, and that all these kernel functions are summed up.

*to-do* is the above really a good descriptions for all kernels? Isn't it only for Gaussian / Laplace?

Kernels provide a principled way of deriving non-parametric models from parametric ones.

*to-do* but some kernels such as Gaussian have themselves parameters

Pro of kernelized perceptron: h(x) = signʹ(∑~i~α~i~y~i~k(x~i~,x)) shows that computation can be cheap if relatively to n only a few α~i~ are non-zero.

_How to choose a suitable kernel and kernel parameters_: See <<model_selection>>, <<concrete_kernels>> and recall the kernel composition rules.  _Avoid overfitting_: Kernels map to (very) high-demensional feature spaces, which indicates that we might suffer from overfitting. The problem of overfitting however is not as bad as it first sounds, because the number of parameters equals the number of observations, i.e. |α| =  n, which usually is much less than the number of dimensions in the feature space. Still we can apply regularization, as for example in <<kernelized_linear_regression>> or <<kernelized_svm>>.


=== Kernel application examples

Example perceptron: It can proven (representer theorem) that the optimal hyperplane ŵ is of the form ŵ = ∑~1≤i≤n~α~i~y~i~x~i~.  Based on that idea, the learning of ŵ is simple (looking at stochastic gradient descent where an observation is choosen uniformily at random), even if Φ(x) has many more dimensions than x. Let t denote the index of the outer iteration, and j denote the index of the choosen observation.  Instead w = w + η~t~Φ(x)~j~t~j~, we do now α~j~ = α~j~ + η~t~.

--------------------------------------------------
 w = 0                               α = 0
 iterate (t) until convergence       iterate (t) until convergence
     pick observation (j) u.a.r.         pick observation (j) u.a.r.
                                         w = ∑~i~α[i]y[i]x[i]
     yhat = signʹ(w^T^x[j])              yhat = signʹ(w^T^x[j])
                                         [yhat = signʹ(∑~i~α[i]y[i]k(x[i],x[j]))]
     if y[j] != yhat                     if y[j] != yhat
         w += η[t]x[j]y[j]                   α[j] += η[t]
--------------------------------------------------

loss~perceptron~(w, y~i~, x~i~) = max(0, -y~i~w^T^x~i~) = +
max(0, -∑~j~α~j~y~j~y~i~x~i~^T^x~j~)

Example: Anova kernel *to-do*

Example: Using Gaussian kernel in the context of the perceptron. Looking at the right algorithm above, each time a (randomly choosen) data point x~j~ is missclassified, we add a gaussian bump to f(x) = ∑~i~α~i~y~i~k(x~i~,x)) at x = x~j~, due to the "α~j~ += η~t~" statement and the fact that k(x~i~,x) is a gaussian bump centered at x. Intuitively, the model ŷ = h(x) = signʹ(f(x)) is approximated by signʹ(∑~i s.t. ‖xi-x‖≤ch~α~i~y~i~) for some small c.  The α~i~ are optimized during model training.  Each data point (y~i~, x~i~) has its associated α~i~.

Example: Two class k nearest neighbors classifier. We can express two class k-NN with ŷ = h(x) = signʹ(∑~1≤i≤n~y~i~k(x~i~,x)) with k(x,xʹ) = I(x is among k nearest neighbors of xʹ), where I is the indicator function.  Note here there are no parameters to be trained.


[[concrete_kernels]]
=== Some concrete kernels

See also <<kernel_engineering>> for rules how to create new kernels by composing existing kernels.

- _linear kernel_: k(x,xʹ) = x^T^xʹ

- _polynomial kernel_: k(x,xʹ;d) = (x^T^xʹ+1)^d^. The feature space contains all monomials up to degree d.

- _Gaussian kernel_ (or _Radial basis function_ (_RBF_), or _squared exponential kernel_): k(x,xʹ;h) = exp(-‖x-xʹ‖~2~²/h²). h is the _bandwith_ (or _length scale_) parameter. Small h means a more flexible model, i.a. a more wiggly resulting function ∑~i~α~i~k(x~i~,x~j~) which is more prone to overfitting.  A large h means an inflexible model prone to bias.  At least in case of SVM or perceptron, with a large h you almost get a linear decision boundary.  Or from another viewpoint, a small h means narrow Gaussian bumps and a large h means wide Gaussian bumps.  The decision boundary is very smooth; it's differentiable infintely many times.  Gaussian kernel is named that way only because it looks like a Guassian curve. The feature space has an infinite number of dimensions.  Infinitely differentiable.  Canonical kernel for Gaussian processes for smooth functions.

- _Laplacian kernel_ (or _exponential kernel_): k(x,xʹ;h) = exp(-‖x-xʹ‖~1~/h). h works as in Gaussian kernel.  The decision boundaries are approximately piecewise linear.  I.e. the decision boundary is quite ragged.

- _Periodic kernel_: k(x,xʹ;p,h) = exp(-‖sin(px)-sin(pxʹ)‖~2~²/h²). Note the similarity to Gaussian kernel, x is replaced by sin(px) and xʹ is replaced likewise by sin(pxʹ).  p>0 is a further parameter.  An alternate way to define it is k(x,xʹ) = exp(-a∑~1≤i≤|x|~sin(p·(x~i~-xʹ~i~))²), where a>0 and p>0 are scaling parameters for amplitude and periodicity, respectively.

- _sigmoid_ (_tanh_) k(x,xʹ;a,b) = tanh(ax^T^xʹ-b)

- _rational quadratic_ k(x,xʹ;σ²,l,α) = σ²(1+(x-x)²/(2αl²))^-α^.

- _periodic squared exponential_ k(x,xʹ;σ²,l,p) = σ²exp(-2sin²(π|x-xʹ|/p)/l²).  As squared exponential, but additionally periodic.

References:

- https://www.cs.toronto.edu/~duvenaud/cookbook/


== Cheat sheet

Shrinkage/Regularization: Fight overfitting (high variance) by applying <<occams_razor>>.

|====
| l(ŷ,y) | loss function (for one observation)
| l~quadratic~(ŷ, y) = (y - ŷ)² |
| l~0/1~(ŷ, y) = (y != ŷ) = (yŷ < 0) |
| l~perceptron~(ŷ, y) = max(0, -yŷ) |
| l~hinge~(ŷ, y) = max(0, 1-yŷ) |
| l~logistic~(ŷ, y) = log(1+exp(-yŷ)) |
| Q | cost function. More general than the loss function, e.g. sum of loss functions over all observations
| R(h) = E~y∈𝓨,x∈𝓧~[l(h(x),y)] = ∫~𝓨~∫~𝓧~l(h(x),y)p(x,y)dxdy | expected risk
| R̂~𝓓~(h) = E~(y,x)∈𝓓~[l(h(x),y)] = 1/n ∑~(y,x)∈𝓓~l(h(x),y) | empirical risk
| R̂~trainset~(ĥ) | training error of given model ĥ
| R̂~testset~(ĥ) | test error of given model ĥ
| R(h) - R̂~𝓓~(h) | generalization error
| ĥ = argmin~h~(R̂~trainset~(h)) | empirical risk minimizer (ERM)
| p(𝓓\|θ) or p(y~i~\|x~i~,θ) | likelihood
| p(θ) | prior (probability)
| p(𝓓) = ∑~θ∈Θ~p(𝓓,θ) | model evidence (marginal likelihood)
| p(θ\|𝓓) = p(𝓓\|θ)p(θ) / p(𝓓) ∝ p(𝓓\|θ)p(θ) | posterior (probability)
| θ̂^MLE^ = argmax~θ~(p(𝓓\|θ)) | maximum likelihood estimator (MLE)
| θ̂^MAP^ = argmax~θ~(p(θ\|𝓓)) | maximum a posteriori probability estimate (MAP estimate)
| p(y\|x,𝓓) | probabilistic prediction

| ŵ = argmin~w~(R̂(w) [+ regterm]) | general pattern. R̂(w), i.e. loss function within, better be convex
| ŵ = argmin~w~(RSS̃(w)) | Least squares linear regression
| ŵ = argmin~w~(RSS̃(w) + λ‖w‖~1~) | Lasso regression
| ŵ = argmin~w~(RSS̃(w) + λ‖w‖~2~²) | Ridge regression
| ŵ = argmin~w~(∑l~perceptron~(w^T^x̃~i~,ỹ~i~)) | Perceptron (classification)
| ŵ = argmin~w~(∑l~hinge~(w^T^x̃~i~,ỹ~i~) + λ‖w‖~2~²) | Support vector machine (SVM) (classification)
| ŵ = argmin~w~(∑l~logistic~(w^T^x̃~i~,ỹ~i~) [+ regterm]) | Logistic regression (classification)

| α̂ = argmin~α~(∑~1≤i≤n~max(0, y~i~α^T^k~i~) | Kernelized perceptron (classification)
| α̂ = argmin~α~(∑~1≤i≤n~max(0, 1-y~i~α^T^k~i~ + λα^T^D~y~KD~y~α) | Kernelized SVM (classification)

| h(x) = w^T^x | Linear regression, Lasso regression, ridge regression
| h(x) = signʹ(w^T^x) | Perceptron, SVM

| p(y\|x,w) = Ber(y\|sigm(w^T^x)) | Logistic regression

| h(x) = ∑~1≤i≤n~α~i~k(x~i~,x) | Kernelized linear regression, L2-norm regularization
| h(x) = signʹ(∑~1≤i≤n~α~i~y~i~k(x~i~,x)) | Kernelized perceptron, Kernelized SVM (classification)

| k(x,xʹ) = x^T^xʹ | linear kernel
| k(x,xʹ) = (x^T^xʹ+1)^d^ | polynomial kernel
| k(x,xʹ) = exp(-‖x-xʹ‖~2~²/h²) | Gaussian kernel
| k(x,xʹ) = exp(-‖x-xʹ‖~1~/h) | Laplacian kernel
| k(x,xʹ) = exp(-‖sin(px)-sin(pxʹ)‖~2~²/h²) | Periodic kernel
|====
