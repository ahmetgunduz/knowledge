// The markup language of this document is AsciiDoc
:encoding: UTF-8
:toc:
:toclevels: 4


= Mathematics

Mainly mathematics tailored towards the nees of computer science.


== Misc basics

A _set_ is an unordered collection of distinct _elements_ (or _objects_, or _components_).
The notiation {a, b} defines a set consisting of elements a and b.
The _size_ (or _cardinality_) of a set S is the number of its elements, denoted as |S|.
A _sequence_ is an ordered collection of possibly duplicate elements,
The notation (b, a, a) defines a sequence.

The _powerset_ (or _power set_) of a set S is the set of all subsets of S, including the empty set and S itself.

[[permutation]]
A _permutation_ of a set S is a sequence containing every element of S exactly once.
The number of permutations of a n-element set is n!.

A (total) _function_ f : X â†’ Y maps all elements of its input set X to elements of its output set Y.
Note a function is allowed to map multiple elements of X to a given element of Y.
The input set is called the _domain_.
The output set is called the _codomain_.
The set of elements in the codomain that appear in f's mappings is called the _range_ (or _image_).
A _partial function_ does not map all the elements of its input.
The term _total function_ is a synonym to `function' as defined above.
The term total function is typically only used when a disambiguition to partial function is needed.
Given a _injective function_, each element of Y is mapped to at most once.
Given a _bijective function_, each element of Y is mapped to exactly once.
Given a _surjective function_, each element of Y is mapped to at least once.
A _k-to-1 function_ maps exactly k elements of X to every element of Y.

A function is bijective if and only if it's both injective and surjective.


== Counting & Combinatorics

Typically we are typically good at determining the size of a set S of sequences.
Thus when faced with the problem of finding the size of a set P, we often try to define a set S of sequences and a function f : S â†’ P.
Typically, once we defined the set S of sequences, the function f is obvious or implied by the definion of S.
Say we find a set of sequences S and a bijection function f : S â†’ P, then the product rule gives us |S|, and the bijection property gives us |P| = |S|.
Likewise, say we find a k-to-1 function f : S â†’ P, the product rule and the division rule give us |P| = |S| / k.

For example consider poker.
In poker a hand is 5 cards out of a deck of 52 cards.
Each card has a rank and a suit.
There are 13 ranks and 4 suits.

How many different hands have a four-of-a-kind?
We try to define a set S of sequences describing the problem, i.e. having a relation to the set P asked for by the problem statement.
A possible solution is that a sequence of S has the follwing 3 elements.
1) The first is the rank of the four card; 13 possibilities.
2) The second is the rank of the extra card; 12 possibilities.
3) The third is the suit of the extra card; 4 possibilities.
Due to the product rule, |S| = 13Â·12Â·4.
There's an obvious bijection from |S| to |P|.
Due to to bijective property, |P| = |S|.

How many diffrent hands have a two pairs; that is, two cards of one rank, two cards of another rank, and one card of a third rank?
We try to define a set S of sequences describing the problem, i.e. having a relation to the set P asked for by the problem statement.
A possible solution is that a sequence of S has the follwing 6 elements.
1) The rank of the first pair; 13 possibilities.
2) The suits of the first pair; C(4 2) possibilities.
3) The rank of the second pair; 12 possibilities.
4) The suits of the second pair; C(4 2) possibilities.
5) The rank of the extra card; 11 possibilities.
6) The suits of the extra card; C(4 1) possibilities. 
Due to the product rule, |S| = 13Â·C(4 2)Â·12Â·C(4 2)Â·11*C(4 1).
Since the first pair, i.e. element 1 and 2, and the second pair, element 3 and 4, are indistinguishable, there is a 2-to-1 mapping from |S| to |P|.
To be precise, it's 2 because there are 2! = 2 ways to permute an 2-element set.
Due to the 2-to-1 mapping and the division rule, |P| = |S| / 2.

_Pigeonhole principle_: If |A| > |B|, then for every total function f : A â†’ B, there exists (at least) two diffrent elements of A that are mapped to the same element of B.
The _generalized pigeonhole principle_ states that if |A| > kÂ·|B|, then every total function f : A â†’ B maps at least k + 1 different elements of A to the same element of
B.

_Product Rule_: Given the sets A~1~, ..., A~n~.
Let A~1~ â¨¯ ... â¨¯ A~n~ denote the set of all sequences whose first term is drawn from A~1~, the second term is drawn from A~2~ and so forth.
|A~1~ â¨¯ ... â¨¯ A~n~| = âˆ|A~n~|.
E.g. the number of different 3 digit hex numbers is 16Â·16Â·16.

_Division Rule_: If there is a k-to-1 function f : A â†’ B, then |A|=kÂ·|B|.

As a special case of the division rule: If there is a bijective function f : A â†’ B, then |A| = |B|.

_Sum Rule_: Given disjoint sets A~1~, ..., A~n~, then |â‹ƒA~n~| = âˆ‘|A~n~|.

_Inclusion rule_: |AâˆªB| = |A| + |B| - |Aâˆ©B|.
Intuition: Just imagine the generic Venn diagram of sets A and B.

_Inclusion-exclusion rule_: Is a generalisation of the inclusion rule.
For the special case of three sets: |AâˆªBâˆªC| = |A| + |B| + |C| - |Aâˆ©B| - |Aâˆ©C| - |Bâˆ©C| + |Aâˆ©Bâˆ©C|. For the formula of the general case of n sets, the internet is your friend.

_Boole's inequality_: |AâˆªB| â‰¤ |A| + |B|. Intuition: Follows from the inclusion rule.

_Union Bound_: |â‹ƒA~n~| â‰¤ |A~n~|. Intuition: Generalization of Boole's inequality.

_Monocity Rule_: If A âŠ† B, then |A| = |B| â‰¤ |B|.

_There are 2^n^ subsets of an n-element set_.
Proof: We define a sequence S from which there is a bijection to the problem set |P|.
The i-th element of the sequence S tells if element i of the original set is part of the subset or not.
The product rule gives |S|=2^n^, and the bijecton gives |P|=|S|.

A _k-combination_ of an n-element set S is a subset of k distinct elements of S.
The number of possible k-combinations is denoted by _C(n, k)_, pronounced `n choose k'.
Less concise formulated, it's the _number of k-element subsets of an n-element set_.
C(n, k) = n! / ((n-k)!k!).
Intuition: First we have n possibilities, then (n-1) and so on until (n-k+1).
That equals n! / (n-k)!.
So far we exactly have a k-permutation.
Since the order of those k elements doesn't matter, we have to devide by the number of permutations, which is k!.

C(n, k) = C(n, n-k)

C(n, 0) = C(n, n) = 1

_binomial theorem_ (aka _binomial expansion_): (x+y)^n^ = âˆ‘~0â‰¤kâ‰¤n~(C(n,k)Â·x^k^Â·y^n-k^). So C(n, k) is also called the _binomial coefficient_.

A _k-combination with repetitions_ (or _k-multicombination_, or _k-multisubset_) of an n-element set S is a multiset of k (possibly identical) elements of S.
The number of such k-multisubsets is denoted by \((n k)), pronounced `n multichoose k'.
\((n k)) = C(n+k-1, k).
Intuition, using the _stars and bars_ graphical aid.
Imagine the chosen multiset of elements Ï‰~1~ as a group of stars, the chosen multiset of elements Ï‰~2~ as another group of stars and so on.
More precisely, do it the following way.
You have a set of k+(n-1) positions.
Note that its a set, i.e. unordered.
The following visualizes it in an ordered manner, but conceptually it's unordered.
k positions are assigned a star, n-1 positions are assigned a bar.
The bars separate groups of stars.
For example for k=6 and n=3, a possible outcome is â˜…â˜…|â˜…â˜…â˜…|â˜….
Thus the original multicombination problem reduces to choosing a set of n-1 positions out of k+(n-1) positions in order to assign bars to.
C(k+(n-1), n-1) = C(k+(n-1), k) = C(n+k-1, k).
The first transformation is true due to the general rule C(n, k) = C(n, n-k).

A _k-permutation_ (or _variation_ or _partial permutation_) is a k-element sequence consisting of distinct elements out of an n-element set.
The nuber of possible k-permutations is denoted by _P(n,k)_ = C(n,k)*k! = n! / (n-k)!.
Intuition: First we have n possibilities, then (n-1) and so on until (n-k+1).
That equals n! / (n-k)! = C(n,k)*k!.

[[permutation_with_repetition]]
A _k-tuple_ (or _permutation with repetition_) is a k-element sequence consisting of (possibly identical) elements out of an n-element set.
The number of k-tubles of an n-element set is k^n^.
Intuition: First we have n possibilities, then again n, and so on, k times.

Overview denoting k-element entities and the number of such entities
given an n-element set (implies unordered and distinct):

|=====
|                    | without repetitions                | with repetitions
| subset (unordered) | k-combination, C(n, k)             | k-multicombination, C(n+k-1, k)
| sequence (ordered) | k-permutation, P(n, k) = C(n, k)k! | k-tuple, k^n^
|=====


Further typicall problems:

_bookkeeper rule_ (an inofficial term made up by the MIT): Given a k-element set {e~1~, ..., e~n~}, the number of sequences consisting of n~1~ e~1~, ..., n~k~ e~k~ is (âˆ‘n~i~)! / âˆ(n~i~!).
Intuition, using the problem of finding the number of ways to rearange the letters in the word `bookkeeper'.
There are n~1~=1 b's, n~2~=2 o's and so on up to n~6~ r's.
I.e. k=6, but that is not really important.
There is a total of âˆ‘n~i~ = 10 letters.
So there are 10! permutations of these letters.
However, we can't distinguish the n~2~=2 o's in each sequence, so we have to devide by 2!.
Likewise, we have to devide analogously for each of {b, o, k, e, p, r}.

Corollary to the bookkeeper rule: How many x-bit sequences contain y zeros? By the bookkeeper rule, n~1~ = y, n~2~ = x - y, thus x! / (y!Â·(x-y)!).

References:

- The above is largely based upon MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapter "Counting"


== Probability and Statistics

The _sample space_ S (or Î©) is the set of possible outcomes of an _experiment_.
An element Ï‰ âˆˆ S is called an _outcome_ (or _sample outcome_ or _element_ or _realization_ (is ambigous to the realization of a random variable)).
A subset E âŠ† S is called an _event_.
In other words, an event is a set of outcomes.
âˆ… denotes the _null event_ which is always false.
S denotes the _true event_ which is always true.
The set of `interesting' or `known' events is denoted ğ“•.
A _probability space_ (or _probability triple_) is the tripe (sample space S, set of events ğ“•, probability function Pr).
A _probability function_ (or _probability distribution_ or _propability measure_) Pr (or P or â„™) on a sample space S is, a bit sloppily defined, a total function Pr : ğ“• âŸ¶ [0, 1] having the following two properties:
1) Pr(Ï‰) â‰¥ 0 for all outcomes Ï‰ âˆˆ S.
2) âˆ‘~Ï‰âˆˆS~ Pr(Ï‰) = 1.
3) Pr(E) = âˆ‘~Ï‰âˆˆE~Pr(Ï‰).
It's a sloppy definition because it enforces that ğ“• contains every outcome.
A more precise definition is that a probability function is a total function Pr : ğ“• âŸ¶ [0, 1] satisfying the three _probability axioms_ (or _Kolmogorov axioms_):
1) Pr(E) â‰¥ 0 for all events E âˆˆ ğ“•.
2) Pr(S) = 1.
3) If E~1~, E~2~, ... are disjoint then Pr(â‹ƒE~i~) = âˆ‘Pr(E~i~).
There are multiple notations denoting the evaluation of the function Pr: Pr(...) or Pr[...] or Pr{...}.
A finite probability space S is said to be _uniform_ if Pr(Ï‰) is the same for every outcome Ï‰ âˆˆ S.
In an uniform probability space, Pr(E) = |E| / |S| for any event E âŠ† S.

_conditional probability_: The probability of event A given event B is known to be true is Pr(A|B) = Pr(Aâˆ©B) / Pr(B).
Pr(A) is also called the _prior probability_ of A and Pr(A|B) the _posterior probability_ of A.
Note that the order in time in which the events A and B occur does not matter.
Note that in general Pr(A|B)â‰ Pr(A|B). Pr(cute_thing|pubby) is high but Pr(pubby|cute_thing) is not so high.

Intuitively Pr(A|B) is the probability of event A when only considering the alternate sample space SB = B.

--------------------------------------------------
Areas are proportional to probabilities

  Sample space S      Pr(â‹…|B) intuitively defines
                      a new sample space SB = B
           A
 S   whole 'column'
  +----+------+       Pr(A|B) = Pr(Aâˆ©B) / Pr(B)
  |    |      |       = Probability of A in sample space SB
  |    |      |
  |    |      |     SB
  +----+--+---+       +-------+---+
 B|       |   |      B|       |   |
  +-------+---+       +-------+---+
                               Aâˆ©B
--------------------------------------------------

_bayes theorem_: Pr(A|B) = Pr(B|A)Pr(A) / Pr(B). +
From definition of conditional probability and community of âˆ©. +
Pr(B) often given by law of total probability.

_law of total probability_: Given a partition {A~1~, ...,A~n~} of the sample space S, then Pr(B) = âˆ‘Pr(Bâˆ©A~i~) = âˆ‘Pr(B|A~i~)Pr(A~i~).

Pr(Aâˆ©B) = Pr(A|B)Pr(B) = Pr(B|A)Pr(A) =~if Aâ««B~ Pr(A)Pr(B). +
From definition of conditional probability and community of âˆ©.

Pr(AâˆªB) = Pr(A) + Pr(B) - Pr(AâˆªB)

[[independence]]
Two events A and B are _independent_, denoted Aâ««B (or AâŸ‚B), if (Pr(A|B) = Pr(A) or Pr(B) = 0).
Or equivalently, called the _product rule for independent events_, iff Pr(Aâˆ©B) = Pr(A)Pr(B).
Note that disjoint does _not_ imply independent.
For example say A and B are disjoint and both are non-empty, then Pr(A|B) = 0 â‰  Pr(A).
Naturally independence is a symmetric relationship.
That's why we usually say `A and B are independend' rather than `A is independent of B'.
The form `Pr(A|B) = Pr(A) or if Pr(B) = 0' shows more clearly the meaning of `the occurence of B does not affect the probability of A'.
The form `Pr(Aâˆ©B) = Pr(A)Pr(B)' shows more clearly the symmetry of indpendence.

Informally stated, A and B are independend if the probability of A is independent of whether its relative to sample space S or when considering only the restricted sample space SB = B, _or_ vice versa for B.

--------------------------------------------------
Areas are proportional to probabilities

                   Pr(A|B) = Pr(A) or if (Pr(B)=0)
                   Informally: Ratio Aâˆ©B:B equals ratio A:S,
                   i.e. probability of A is independent of whether
                   its relative to SB or to S.
 S          A                      S          A
  +-------+---+                     +-------+---+
  |       |   |                     |       |   |
  |       |   |                     |       |   |
  |       |   |  SB                 |       |   |
  +-------+---+    +-------+---+    |       |   |
 B|       |   |   B|       |   |    |       |   |
  +-------+---+    +-------+---+    +-------+---+
                            Aâˆ©B
--------------------------------------------------

Example where A and B _are_ dependend:

--------------------------------------------------
Areas are proportional to probabilities

            A
  +-------+---+
  |       |   |
  |       +---+
  +-----+-+   |
 B|     |     |
  +-----+-----+
--------------------------------------------------


--------------------------------------------------
Areas are proportional to probabilities

            A        Pr(Aâˆ©B) = Pr(A)Pr(B)
  +-------+---+      Considering the above drawings,
  |       |   |      this can only be true if
  |       |   |      both of A and B can be drawn
  +-------+---+      with straight orthogonal lines,
 B|       |   |      in which case
  +-------+---+
--------------------------------------------------


To make that example more concrete, consider that blood can have a certain type and a certain rh factor.
Say the probability Pr(T) for type T is known, and the probability Pr(F) for rh factor F is known.
The previously described Venn diagram shows that the probability somebody has type T _and_ rh factor F equals Pr(T)Pr(F) _only_ if T and F are independent.
For independence, the ratio of people having rh factor F among all people (|F| / |S| = Pr(F)) must be equal to the ratio of people having rh factor F among those having also type T (|Fâˆ©T| / |T|).

The elements of Î±={A~1~, ..., A~n~} are _mutually independent_ iff Pr(â‹‚A~i~) = âˆPr(A~i~) for _any_ subset of Î±.
Mutual independence does imply pairwise indpendence, but not vice versa.

The elements of Î±={A~1~, ..., A~n~} are _pairwise independent_ iff for all unordered pairs {A~i~, A~j~} of distinct elements (i.e. i â‰  j), A~i~ and A~j~ are independent.
Pairwise independence does _not_ imply mutual independence.

A _decision tree_ is a graphic tool for working with outcomes and events of an probability space.
The root is the start and is not directly associated a meaning.
Given a vertex, each outward edge represents that a given `subevent' occures.
The definition of an edge's associated subevent includes that the the subevent associated with the edge's source vertex has occured.
`Subevent' is an inofficial term made up by the author.
Each vertex thus represents the subevent that all subevents of the edges of the path from the root to that vertex have occured.
Note that the subevents on the path are not required to happen in the order implied by the path.
One just has to compute the correct _conditional_ probabilities of the edges.
Each outward edge of a vertex is assigned the conditional probability that the edge's associated subevent occures, given that the subevent associated with the vertex has occured.
For each internal vertex, the sum of the probabilities of all its outward edges is 1.
By the the above definitions, given a path, the subevents associated with the edges are independent, thus they can be multiplied to get the probability of taking that path.
Each leaf represents an outcome of the experiment.
Thus the set of all leaves represents the sample space.
I.e. there is a 1 to 1 relationship between the set of all leaves and and the set of all outcomes.

Alternatively, draw the tree using the treemapping method.
You start out with a rectangle representing the root vertex of the tree.
For each child, draw a line to create a subrectangle, the sizes of the subrectangles according to the weight of the edges. All llines mutually parallel.
Recurse.
At each new level in the recursion, toggle between horizontal and vertical lines.
The result has resemblance to a Venn diagram, only that here a given event is represented by a set of possibly disconnected areas, as opposed to a single connected area.

Recipe for solving many probability problems:

. Consequently follow the rules.
Don't try to be fast.
Often the human intuition is wrong.

. Define the sample space, i.e. all possible outcomes.

. Define events of interest.

. Compute probabilities (of required outcomes). Possibly the following way: Use the tree diagram method.  Assign a probability to each (required) edge.  Calculating the probability of an outcome is then trivial.

. Compute probability of your events, which is trivial, now that you have the probabilities of the outcomes.

References:

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapter "Probability"

- MIT course 18.650 "Statistics for Applications", Fall 2016, https://www.youtube.com/playlist?list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0[videos], https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/MIT18_650F16_Introduction.pdf[lecture notes]

- Book ``All of statistics'', chapter ``1 Probability''

- Khan Academy, ``Statistics'' playlist: https://www.youtube.com/watch?v=uhxtUt_-GyM&list=PL1328115D3D8A2566


=== Random variables and expectations

Formally a random variable is a function mapping from sample space to measure space, as defined in the following.  In practice, we often think of a random variable like a random number.  In practice, the sample space associated to a random variable is rarely explicitelly mentioned, but keep in mind that it really is there.  Random variables can be interpreted as link between data and sample spaces.

--------------------------------------------------
 probability space := (sample space S, events ğ“•, probability function Pr)

            probability
 set of     function Pr
 events ğ“• =============> [0,1]
  ^
  |set of
  |subsets  random        measure
  |         variable R    space,      CDF_R(x) := Pr(Râ‰¤x)
 sample  ===============> mostly â„    ================> [0,1]  
 space S
                                      E[R] := âˆ«xÂ· CDF_RÊ¹(x)
                                      ----------------> measure space

                                      Var[R] := E[R-E[R]]Â²
                                      = âˆ«(x-E[R])Â²CDF_RÊ¹(x)
                                      ----------------> measure space

   S is countable    discrete R       PMF_R(x) := Pr(R = x)
   set                                if R is the identity: PMF_R = Pr
                                      ================> [0,1]

                                      E[R] = âˆ‘xÂ·PMF_R(x) = âˆ‘R(Ï‰)Â·Pr(Ï‰)

   S is infinit      continous R      PDF_R(x) = CDF_RÊ¹(x) (informally)
   noncountable      PDF_R exists     Pr(aâ‰¤Râ‰¤b) = integrate PDF_R(x) over [a,b]
   set                                if R is the identity: PDF_R = Pr
                                      ================> [0,1]

                                      E[R] = âˆ«xÂ·PDF_R(x)
--------------------------------------------------

A _random variable_ R is a measurable total function R : S âŸ¶ â„.
Technically the range of R is the _measure space_ E, but in computer science practice the measure space is mostly â„.
Roughly speaking, density functions exist only when the measuere space is â„.
The actually observed value of a random variable R is called _realization_ of R (or _observation_).
Note that the term `realization' is ambigously also used as a synonym for outcome Ï‰ âˆˆ S.
An _indicator random variable_ (or _Bernoulli variable_) is a random variable with codomain {0, 1}.
A random variable is _discrete_ if its domain is a countable set.
A random variable R is _continuous_ if there exists a probability density function for it.
Note that for a continuous random variable R, Pr(R = x) = 0 for every x.
We get a non-zero probability only in a non-empty range.

There's a strong relation between events and random variables.
Any assertion about the value of a random variable defines an event.
Say the random variable C counts number of heads in 3 coin flips.
The condition C = 1 defines the event {HTT, THT, TTH}, or the condition C â‰¤ 2 {TTT, HTT, THT, ...}.
Looking at it from the other direction, each event E is naturally associated with a corresponding indicator random variable I~E~, where I~E~(Ï‰) equals 1 if outcome Ï‰ âˆˆ E and and 0 otherwise.

Given a random variable R with measure space â„, its _cumulative distribution function_ (or _CDF_ or _cumulative density function_) CDF~R~ (or F~R~) : â„ âŸ¶ [0, 1] is defined as CDF~R~(x) = Pr(R â‰¤ x).

Given a random variable R with measure space â„, its _inverse CDF_ (or _quantile function_) is defined by CDF~R~^-1^(q) = inf{r: CDR~R~(x) > 1} for q âˆˆ [0, 1].
E.g. CDF~R~^-1^(1/2) tells you the x at which CDR(x) equals 1/2.
We call CDF~R~^-1^(1/4) the _first quartile_, CDF~R~^-1^(1/2) the _median_ (or _second quartile_) and CDF~R~^-1^(3/4) the _third quartile_.

_percentile_ is the same as quantile, only that it is in %, that is 100 times larger.

[[PDF]]
Given a continuos random variable R with measure space â„, its _probability density function_ (or _PDF_) PDF~R~ (or f~R~) : â„ âŸ¶ [0, 1] is a function satisfying:

1) Pr(a â‰¤ R â‰¤ b) = âˆ«~a~^b^PDF~R~(x)Â·dx for every a â‰¤ b. +
2) Pr(x) â‰¥ 0 for all x. +
3) âˆ«~-âˆ~^âˆ^PDF~R~(x)Â·dx = 1.

Note that according to these rules a PDF, unlike a PMF, can be bigger than 1; it can even be unbounded. See also <<population>>.

[[PMF]]
Given a discrete random variable R with measure space â„, its _probability mass function_ (or _PMF_ or _probability function_) PMF~R~ (or f~R~) is defined as PMF~R~(x) = Pr(R = x).  See also <<population>>.

Both the probability density function and the cumulative distribution function capture the same information about the random variable, so take your choice.

PDF~R~(x) = CDFÊ¹~R~(x) at all points x at which CDF~R~ is differentiable.

CDF~R~(x) = âˆ«~âˆ’âˆ~^x^PDF~R~(x)Â·dx.

In sloppy notation, CDF~R~(-âˆ) = 0 and CDF~R~(âˆ) = 1.

A _univariate distribution_ is a probability distribution of only one random variable.  A _multivariate distribution_ is the _joint probability distribution_ of two or more random variables.

Two random variables R~1~ and R~2~ are _equal_ if R~1~(Ï‰) = R~2~(Ï‰) for all outcomes Ï‰ âˆˆ S.

Two random variables R1 and R2 are _equal in distribution_ if CDF~R1~(x) = CDF~R2~(x) for all x.
Note that equal in distribution does not imply equal.
E.g. consider X = `number of heads' and Y = `number of tails' in N fair coin tosses.

Two random variables R~1~ and R~2~ are _independent_ iff for all x~1~ âˆˆ codomain(R~1~), x~2~ âˆˆ codomain(R~2~), the two events [R~1~ = x~1~] and [R~2~ = 2~1~] are independent.

Random variables R~1~, ..., R~n~ are _mutually independent_ iff for all x~1~, ..., x~n~ the events [R~1~ = x~1~], ..., [R~2~ = x~2~] are mutually independent.
They are _k-way independent_ iff every subset of k of them are mutually independent.

A set of random variables is _independent and identically distributed_ (or _iid_ or __i.i.d.__) if all random variables are mutually indpendent and each random variable has the same probability distribution as the others.

Two events are independent iff their indicator variables are independent.

Let R and S be independent random variables, then f\(R) and g(S) are also independent random variables, where f and g are some functions.

The _mode_ is the value of X where the PMF / PDF of X takes its maximum value. I.e. its the value of X that appears the most often.

Given a random variable R, then its _expected value_ (or _expectation_ or _mean_ or _average value_ or _first moment_, see also <<population_mean>>), denoted E[R] (or ğ”¼\(R) or ğ”¼R or Î¼ or Î¼~R~ or by the use of on overline), is defined by:

E[R] = âˆ«xÂ·CDFÊ¹~R~(x) +
If R is discrete: E[R] = âˆ‘x~i~Â·PMF~R~(x~i~) = âˆ‘~Ï‰âˆˆS~R(Ï‰)Â·Pr(Ï‰) +
If R is continuous: E[R] = âˆ«xÂ·PDF~R~(x)

The _conditional expectation_ E[R|A] of a random variable R given event A is E[R|A] = âˆ‘rÂ·Pr(R=r|A).

[[variance]]
Given a random variable R, its _variance_ (or _mean square deviation_, see also <<population_variance>>), denoted by Var[R] (or ğ•\(R) or ğ•R or ÏƒÂ² or ÏƒÂ²~R~), is a measure of spread and is defined by

Var[R] = E[(R-E[R])Â²] = E[RÂ²] - E[R]Â² = âˆ«(x-E[R])Â²CDFÊ¹~R~(x) +
If R is discrete: Var[R] = (âˆ‘xÂ²~i~PMF~R~(x~i~)) - E[R]Â² +
If R is continuous: Var[R] = (âˆ«xÂ²PDF~R~(x)) - E[R]Â²

Note that an alternative measure of spread, thought much less often used than variance, is E[|R-E[R]|].

Given a random variable R, its _standard deviation_, denoted Ïƒ (or Ïƒ~R~ or sd\(R)), is defined by Ïƒ = âˆšVar[R].

A set of random variables is called _homoscedastic_ if all of those random variables have the same finite variance.  This is also known as _homoscedasticity_ (or _homogeneity of variance_).  The complementrary notion is called _heteroscedasticity_.

The _covariance_ between two random variables R~1~ and R~2~ is defined as Cov[R~1~, R~2~] = E[(R~1~-E[R~1~])(R~2~-E[R~2~])] = E[R~1~R~2~] - E[R~1~]E[R~2~].

[[correlation]]
_Correlation_ is a statistical relationship between random variables, though in common usage it most often refers to how close two variables are to having a linear relationship with each other. E.g. the relationship between X and Y in regression/classification.

[[pearsons_correlation_coefficient]]
The _Pearson's product moment correlation cofficient_ (or _Pearson's correlation coefficient_ _correlation coefficient_ or simply _correlation_ (but see also <<correlation>>)) between two random variables R~1~ and R~2~ is the standardized covariance and is defined as Ï~R1,R2~[R~1~, R~2~] = Cov[R~1~, R~2~] / (âˆšVar[R~1~]âˆšVar[R~2~]).  Note that the codomain is [-1,1].  Intuitively, it measures how linear the relationship is.  It is 1 for a perfect linear relationship with positive slope, -1 for a perfect linear relationship with negative , and 0 for no relationship at all.

Two random variables R~1~ and R~2~ are said to be _uncorrelated_ if Cov[R~1~, R~2~] = 0.

independent â‡’ uncorrelated

_interaction_ is when the influence of two or more predictors on the response is not additive. E.g. say there are two predictors X1 and X2 and the response Y = f(X1,X2). Imagine the 3D graph/plane.  If a cut through the plane at X1 = some-constant and X2 = some-other-constant doesn't produce two same looking functions (appart from shift), then there's interaction.

If two predictors are highly correlated, it doesn't make sense to add an interaction between them to the model.

E[aÂ·R~1~ + bÂ·R~2~] = aÂ·E[R~1~] + bÂ·E[R~2~] (_linearity of expectation_)

R~1~, ..., R~n~ are mutually independent â‡’ E[âˆR~i~] = âˆE[R~i~]

Var[R] = Cov[R, R]

Var[aR+b] = aÂ²Var[R]

Var[R~1~ + R~2~] = Var[R~1~] + Var[R~2~] - 2Cov[R~1~, R~2~]

In general: Var[âˆ‘a~i~R~i~] = âˆ‘âˆ‘a~i~a~j~Cov(R~i~,R~j~) = (âˆ‘aÂ²~i~Var[R~i~]) + 2âˆ‘~j~âˆ‘~i<j~a~i~a~j~Cov[R~i~, R~j~]

If R~1~, ..., R~n~ are pairwise independent: Var[âˆ‘R~i~] = âˆ‘Var[R~i~]

Cov[R, R] = Var[R]

Cov[R~1~, R~2~] = E[R~1~R~2~] - E[R~1~]E[R~2~]

If R~1~ and R~2~ are independent: Cov[R~1~,R~2~] = Ï~R1,R2~ = 0.

_Law of Total Expectation_: Let R be a random variable, and suppose that A~1~, ..., A~n~ is a partition of the sample space S, then E[R] = âˆ‘~i~E[R|A~i~]Â·Pr(A~i~).

_Mean time to failure_: Given an event E and p = Pr(E), the number of independent experiments until E occures is 1 / p and the variance is (1-p)/pÂ².

_Markov's inequality_: For non-negative R. Pr(Râ‰¥a) â‰¤ E[R] / a.

_Chebyshev's inequality_: Pr(|R-E[R]| â‰¥ a) â‰¤ Var[R]/aÂ². Derived from Markov's inequality.

_Pairwise independent sampling_: Let R~1~, ..., R~n~ be pairwise independent random variables with the same mean Î¼ and same deviation Ïƒ, and let S be their sum: Pr(|S/n-Î¼| â‰¥ x) â‰¤ 1/n ÏƒÂ²/xÂ².

Given a sequence X~1~, ..., X~n~ of random variables.  X~n~, the last of the sequence, _converges in distribution_ (or _converges weakly_ or _converge in law_) towards the random variable X, denoted X~n~ Dâ†’ X (D above the arrow) or X~n~ â‡ X, if lim~nâ†’âˆ~ CDF~Xn~(x) = CDF~X~(x) âˆ€ x âˆˆ â„ at which CDF~X~ is continuous.  An estimator is said _asymptotically Normal_ if (Î¸Ì‚-Î¸)/se[Î¸] â‡ N(0,1).  (*to-do* Is the term "asymptotically" as used in this sense really restricted to "assymptotically normal" and to estimators? I.e. can I say "assymptotically exponential" and most statisticans will feel confortable by such an usage. Def is from all of statistics, p. 92)

Given a sequence X~1~, ..., X~n~ of random variables.  X~n~, the last of the sequence, _converges in probability_ towards the random variable X, denoted X~n~ Pâ†’ X (P above the arrow) or plim~nâ†’âˆ~ X~n~ = X, if for all Îµ > 0 lim~nâ†’âˆ~ Pr(|X~n~ - X| > Îµ) = 0. Convergence in probability implies convergence in distribution.

_Weak Law of Large Numbers_ (or _WLLN_ or _Khintchine's law_): Let X~1~, ..., X~n~ be iid random variables with the same mean Î¼ and same variance ÏƒÂ², and let XÌ„ = 1/n âˆ‘X~i~ denote their sample mean. WLLN states that XÌ„ Pâ†’ Î¼. Interpretation: The distributionh of XÌ„ becomes infinitely concentrated, i.e. 0 variance, around Î¼ as n gets large.  The sample mean is a consistent estimator for the population mean Î¼.  Note that while E[XÌ„] = Î¼ and Var[XÌ„] = ÏƒÂ²/n are also true, they are different statements.

_central limit theorem_ (_CLT_):  Let the random variables X~1~, ..., X~n~ be independent, each X~i~ with some arbitrary unknown distribution but with known mean Î¼~i~ and finite variance ÏƒÂ²~i~.  Then (âˆ‘X~i~ - âˆ‘Î¼~i~) / âˆšâˆ‘ÏƒÂ²~i~ â‡ N(0, 1), or formulated differently: 1/n âˆ‘X~i~ â‡ N(Î¼Ì„, ÏƒÌ„Â²/n)  where Î¼Ì„ = 1/n âˆ‘Î¼~i~ and ÏƒÌ„Â² = 1/n âˆ‘ÏƒÂ²~i~.  If additionally X~1~, ..., X~n~ are identically distributed with mean Î¼ and variance ÏƒÂ², this simplifies to XÌ„ = 1/n âˆ‘X~i~ â‡ N(Î¼, ÏƒÂ²/n).  (*to-do* 1) better understand what http://mathworld.wolfram.com/CentralLimitTheorem.html says more 2) relation to `converges in distribution'? See all of statistics p 72 3) How do you call this thing on the lhs of â‡? 4) Is it correct that I shouldn't use the term sample mean and thus also not the conventional XÌ„ = 1/n âˆ‘X~i~ in the first general case, since the term sample is reserved for the case of taking a sample from a population, and by the definition, population means that its members have the same distribution. How you call 1/n âˆ‘X~i~ in the first/general case? How you call (âˆ‘X~i~ - âˆ‘Î¼~i~) / âˆšâˆ‘ÏƒÂ²~i~ ?)

__WLLN vs CLT__: WLLN gives sample mean's value provided iid Xs.  CLT gives distribution of 1/n âˆ‘X~i~ only provided independent Xs.  (*to-do* But then CLT is a proper superset of WLLN, since knowing the distribution implies knowing the mean. So the question remains, whats the real difference between CLT and WLLN?)

References:

- Book ``All of Statistics'', chapters ``2 Random Variables'' and ``3 Expectation''

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapters "Random Variables" and "Deviation from the Mean"


=== Important distributions


==== Comparison of distributions

*to-do*

References:

- http://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/


==== Uniform distribution

X ~ Uniform(a, b), where a < b, if

PDF(x) = { +
1/(b-a) for x âˆˆ [a, b]
0 otherise

CDF(x) = { +
0 for x < a +
(x-a)/(b-a) for x âˆˆ [a, b] +
1 for x > 0

==== Normal distribution / Gaussian distribution

X ~ ğ“(Î¼, ÏƒÂ²), where Î¼âˆˆâ„ is the mean and Ïƒ>0 the standard deviation, if

PDF(x) = 1/(Ïƒâˆš(2Ï€)) exp(-1/(2ÏƒÂ²) (x-Î¼)Â²)

CDF(x) = Î¦((x-Î¼)/Ïƒ)

We say that X has _standard Normal distribution_ if Î¼=0 and Ïƒ=1. Tradition dictates that a standard Normal random variable is denoted by Z.  The PDF and the CDF of Z are denoted by ğœ™(z) and Î¦(z) respectively.

Î¦(z) = 1/âˆš(2Ï€) âˆ«~-âˆ to x~exp(-tÂ²/2)dt = +
1/2 + 1/2 erf(x/âˆš2)

Where erf(x) = 2/âˆšÏ€ âˆ«~0 to x~exp(-tÂ²)dt

A k-dimensional _multivariate normal distribution_ (or _k-variate normal distribution_) is denoted ğ“~k~(Î¼, ÏƒÂ²).

Some useful facts:

X \~ N(Î¼,ÏƒÂ²) â‡’ (X-Î¼)/Ïƒ ~ N(0,1)

Z \~ N(0,1) â‡’ X = Î¼ + ÏƒZ ~ N(Î¼, ÏƒÂ²)


==== Student's t-distribution

The _Student's t-distribution_ (or _t-distribution_) is the distribution of the sample mean where the population is normally distributed.  It is denoted t~Î½~, where Î½ is its single parameter, the degrees of freedom.  More precisely: Let Î¼ denote the population mean, XÌ„ the sample mean and SÂ² the unbiased sample variance, then (XÌ„-Î¼)/sdÌ‚[XÌ„] \~ t~n-1~, where sdÌ‚[XÌ„] = S/âˆšn, see estimator for standard error of the mean, and where t~n-1~ denotes a Student's t-distribution with n-1 degrees of freedom.

*to-do* I think that is not quite correct. It's just one of more possible use cases. After all many other statistics also have a t-distribution, no?

*to-do* list common statistics which follow a t-distribution (e.g. when statistic g1 follows a normal distribution and a scaling parameter depends on the data, e.g estimator sdÌ‚[g1], then, under certain conditions, g2=g1/sdÌ‚[g1] follows a student's distribution)

<<t_statistic>>
The _(Student's) t-statistic_ for an estimator Î²Ì‚ \~ ğ“ of unknown parameter Î² is defined as t~Î²Ì‚~ = (Î²Ì‚ - Î²~0~) / seÌ‚[Î²Ì‚], where Î²~0~ is a fixed value which may or may not match Î².  Î²Ì‚ must be normally distributed, which in case of OLS is the case if E[epsiolon]=0.  The t-statistic is commonly used in hypothesis testing, where the null hypothesis is that Î² = Î²~0~.  Typically Î²~0~ is 0.  If Î²Ì‚ is an ordinary least squares estimator for a coefficient in the classical linear regression model, and if the true value of parameter Î² is equal to Î²~0~, then t~Î²Ì‚~ \~ t~n-p~ where n is the number of observations, and p is the number of predictors (including the intercept).

Etymology: the term ``t-statistic'' is abbreviated from ``hypothesis test statistic''.

*to-do* I am confused. Here the denominator is se[Î²Ì‚], in the t-distribution its sdÌ‚[XÌ„] (the key point being that the later is an estimator).  Also apparently the Student's t-statstic is not guaranteed to be Student t-distributed, I find that confusing from a terminology point of view. How you call then the statistic used above in the definition of t-distribution?

*to-do* Also in <<t_test>> there multiple examples of t-statistics, all of which have as denominator an estimator, not se[...].  Only when we wanted a t-statistic for a t-test for a estimator Î²Ì‚ of a OLS model coefficient Î², we used t~Î²Ì‚~ = (Î²Ì‚ - Î²~0~) / se(Î²Ì‚).


==== Chi-squared-distribution (ğœ’Â²)

Given random variables X~1~, ..., X~k~ iid~ ğ“(0,1), then

âˆ‘X~i~Â² \~ ğœ’~k~Â²

*to-do* what if X~1~, ..., X~k~ iid~ ğ“(Î¼, ÏƒÂ²)?


==== F-distribution / Fisher-Snedecor distribution

A random variable X having a F distribution with parameters d~1~ and d~2~ is denoted X \~ F(d~1~, d~2~).

It is the distribution of X = (U~1~/d~1~) / (U~2~/d~2~), where U~1~ and U~2~ are independent and have distributions ğœ’Â²(d~1~) and ğœ’Â²(d~2~) respectively, where ğœ’Â² denotes the chi-squared distribution.

Or equivalently, it's the distribution of X = ...

*to-do*

Independence of U~1~ and U~2~ might be demonstrated by applying Cochran's theorem.

Applications: Appears often as the distribution of the test statistic in ANOVA.


=== Testing

==== Hypothesis testing, basic idea and algorithm

A _statistical hypothesis test_ is a method of statistical inference.

A _two sided test_ (or _two tailed test_) is concerned with both regions of rejection, of the distribution.  A _one sided test_ (or _one tailed test_) is concerned with the region of rection for only one of the two tails of the distribution, and it states which one it is concerned with.  The researcher has to decide which variant he prefers.  He can do it based on his educated guess what the alternate hypothesis is, and more specifically, what distribution of the alternate hypothesis is.  The goal is to maximize power, given a type I error rate.  For a concrete alternate hypothesis, power could be calculated by simulating: Do multiple times: Simulate data under the alternate hypothesis, calculate p-value, count H~0~ rejecetions (i.e. `H~a~ acceptances'). Over all this delivers power = H~0~-rejection-count / simulation-count. This way one can calculate power for multiple alternate hypothesises.

one sided vs two sided:

pro one sided test: higher power, i.e. less type II error rate.

*to-do* more pros & cons

Hypothesis test algorithm:

- Choose a suitable test statsistic T.  Compute its observed value t~obs~.

- Define the _null hypothesis_ and the complementary _alternate hypothesis_.  The null hypothesis (the hypothesis to be nullified), denoted H~0~, is a statement usually along the lines ``there is no relationship'' or ``there is no effect''.  The complementary alternate hypothesis is denoted H~a~ (or H~1~).  Note that in a one side test, H~0~ should not use =, but â‰¤ or â‰¥, while the complementary H~a~ then uses > or < respectively.  However it's mathematically still correct for the H~0~ to use = (*to-do* why is that?)

- Compute the p-value, see definition below.

- Choose a significance level Î±, see definition below.  Typically the significance level is chosen to be 5% or 1%.

- _Reject H~0~_ iff p-value < Î±.  Otherwise you _fail to reject H~0~_; you can't accept H~0~, see below.  An equivalent alternative criterion is to reject H~0~ when t~obs~ lies within the critical region, see definition below.

Hypothesis testing really is ``__proof by contradiction__''.  Only that we can't really proof or disprove anything,  since we only work with probabilities.  We only can gather evidence.  We start out assuming H~0~ is true and try to build a contradiction.  If we observe a t~obs~ such that p-value < Î±, then that is a `contradiction' to our assumption.  It's not a contradiction in a strict sense, but it's evidence that our assumption was incorrect.  In the other case, if p-value > Î±, we fail to build a contradiction, i.e. we fail to reject H~0~.  However we do not accept H~0~ either.  No conclusion can be drawn if you fail to build a contradiction.  The evidence is insufficient to support any conclussion about either H~0~ or H~a~.  Recall that we optained the p-value by assuming H~0~ is true, so we certainly can't derive from a p-value that H~0~ is true.

The _p-value_ (or _probability value_ or _asymptotic significance_) for a two sided test is Pr(Tâ‰¥|t~obs~-E[T]| | H~0~), for a one sided test it is Pr(Tâ‰¥t~obs~|H~0~) or Pr(Tâ‰¤t~obs~|H~0~) respectively.  The interpretation of the p-value is: _Given_ H~0~ is true, then in (p-value)Â·100% of any hypothesis tests we see an result as extrem or more extrem (further away from mean) than t~obs~.  I.e. _given_ H~0~ is true, in (p-value)Â·100% of these tests we would incorrectly reject the null hypothesis.  The p-value is _not_ the probability that either hypothesis is correct.  Regarding the case of a one sided H~a~, where the very unlikely case occures that t~obs~ is of on the `other' side of H~0~'s distribution:  then the p-value will be very large, and we will not reject H~0~, which is correct in that we didn't accept H~a~.

The _significance level_ (or _type I error rate_) Î± is the probability of rejecting H~0~ given that H~0~ is true. Or in other words, the probability of a false discovery.  Or equavilently, Î± is the area below the H~0~ distribution in the critical region.  Î± is choosen by the user, see algorithm above.  Typically we want to control type I error rate, since a false discovery is worse than accidentaly not making a discovery.

The _type II error rate_ Î² is the probability of not rejecting H~0~ given that H~a~ is true.  Or equivalently, Î² is the area below the H~a~ distribution in the acceptance region.  Note that the distribution of H~a~ is unknown. Î² = 1 - power.

The _power_ (or _statistical power_) of a test is the probability of making a true discovery, given that H~a~ is true.  I.e. it is the probability of rejecting H~0~ given that H~a~ is true.  Or equivalently, power equals the area below the distribution of H~a~ in the critical region.  power = 1 - Î².

The _critical region_ (or _rejection region_):  In a two sided test the critical region is [-âˆ,t~crit_a~] âˆª [t~crit_b~,âˆ],  where the _critical values_ crit_a and crit_b are defined via Pr(Tâ‰¤t~crit_a~|H~0~) = Î±/2 and Pr(Tâ‰¥t~crit_b~|H~0~) = Î±/2.  Or equivalently via the H~0~ distribution's quantile: t~crit_a~ = H0_dist_quantile(Î±/2) and t~crit_b~ = H0_dist_quantile(1-Î±/2).  In a onesided test its [-âˆ,t~crit~] where Pr(Tâ‰¤t~crit~|H~0~) = Î±, or the other way round.  See also definition of significance level.

The _acceptance region_ is the complement to the critical region.

|=====
|                       | H~0~ really true | H~a~ really true
| failed to reject H~0~ | true positive | false negative, type I error, Î²
| H~0~ rejected         | false postive, type II error, false discovery, significance level Î± | true negative, true discovery, power
|=====


==== Data snooping / selection effect

[[data_snooping]]
_Data snooping_ (or _data dredging_, _data snooping_, _p-hacking_) is searching patterns in data that then can be presented as statistically significant, without first devising a hypothesis.  The proper way is to first come up with a hypothesis, independently of the test data, and only afterwards test that hypothesis with test data.  Some patterns contained in large amounts of data (especially when number of predictors is huge and the number of observations is moderate) will be only due to chance.  When doing data snooping and actively searching for patterns, we are likely to find patterns, maybe ones that are there only due to chance (e.g. the few values of a predictor happen to correlate with the response by chance).  When then doing a hypothesis test with that same data, the p-value is meaningless, because the hypothesis is based on that data.

[[selection_effect]]
_Selection effect_: Any time we use the data to make a decision (e.g. select a model), we introduce a selection effect (bias). E.g. forward stepwise, lasso etc.


See also <<inference_after_model_selection>>


==== Multiple testing

The problem we're trying to solve here is this: If we make many hypothesis tests, each with significance level Î±, we're bound to make a false discovery Î±Â·100% of the times, because that's what significance level Î± says.  See also https://xkcd.com/882/ :-).

As in the case of finding the best expectedTestMSE, i.e. the best trade-off between increasing variance and decreasing bias, we now liked to find the best trade-off between inceease in type I error and increase in power.

_Classificaton of multiple hypothesis tests_: Consider m hypothesis tests. The following table defines variables counting how often each case occures. Upper case variables (U V T S and R) are random variables, lower case variables (m and m~0~) are fixed. The number of tests m is known, number of tests m~0~ where H~0~ is really true is unknown, the number of rejected H~0~ R is observable, the others are unobservable.

|=====
|                       | H~0~ really true | H~a~ really true | Total
| failed to reject H~0~ | U                | T                | m-R
| H~0~ rejected         | V                | S                | R
| Total                 | m~0~             | m-m~0~           | m
|=====

Q = V/R is the _false discovery proportion_ (_FDP_). By convention, if V = R = 0, then Q = 0.

The case of that H~0~ is always true, i.e. m = m~0~, is called the _gobal null_ (or _complete null_).

The _False discovery rate_ (_FDR_) is defined as FDR = E[Q] = E[V/R]. I.e. FDR is the expected proportion of type I errors (aka false discoveries) relative to all discoveries.

The _Famility wise error rate_ (_FWER_) is defined as FWER = Pr[Vâ‰¥1].  I.e. FWER is the probability that we make an type I error (aka false discovery) at all.

Î´ = per test type I error rate +
FWER â‰¥ FDR +
FWER = FDR given global null +
FWER = 1 - (1-Î´)^m^  given global null and independend tests +
FWER â‰ˆ Î´m given global null and independend tests and small Î´ +
Î´ â‰¤ FWER â‰¤ Î´m

A procedure offers _weak control_ at level Î± if FWER â‰¤ Î± holds is guaranteed only under global null.  A procedure offers _strong control_ at level Î± if FWER â‰¤ Î± holds always.  Note that here Î± denotes _not_ the same thing as the significance level Î± of an individual test; here, it's the ``overall significance level''.

Techniques which control FWER: <<bonferroni_correction>>, <<westfall_young>>

Techniques which control FDR: <<bejamini_hochberg>>


===== No correction

*to-do*


[[bonferroni_correction]]
===== Bonferroni correction

Control of the FWER: goal is to get an FWER â‰¤ Î±.  Do each of the m individual tests at a significance level Î´ = Î± / m. As a result we get FWER â‰¤ Î±.

Neutral: Sensible if all tests are independent, because then FWER â‰ˆ Î´m (assuming global null), see formulas after definition of FWER.

Contra: Can be too conservative (i.e. Î´ is smaller than needed), especially if the test statistics are positively correlated.  This is because the Boferroni correction assumes the worst case, which is mutually independent tests.  As an extreme example, under perfect positive dependence, there is effectively only one test, and thus we could choose Î´ = Î± and still have FWER = Î±, but instead we `needlessly' did choose Î´ = Î± / m.

Contra: As always wenn decrasing the siginificance level Î±, that comes at the cost of decreased statistical power, or equivalently, at the cost of increasing type II error rate.

*to-do* How much of the above applies to controlling FWER in general, and how much applies to Bonferroni in particular?


[[benjamini_hochberg]]
===== Benjamini Hochberg

Controls FDR.  *to-do*


[[westfall_young]]
===== Westfall Young permutation procedure

Weak control of FWER. Strong control of FWER under some assumptions.  Computes a significance level Î´ to be used for each test.

*to-do* what are these assumptions?

For all (or some, to save time) permutations allowed under H~0~: Compute p-value for each test, and find the minimum p-value. Overall this gives us an empirical distribution D of the minimal p-values. Compute Î´ = quantile~D~(Î±). Use Î´ as significance level for each of the tests.

This works because: FWER = P(Vâ‰¥1) = P(p~i~â‰¤Î´ for some p~i~) = P(min(p~1~, ..., p~m~)â‰¤Î´)

*to-do* properly understand why this works; why does the formula for Î´ work. see my lecture notes.

References:

- Slides7.pdf


==== Misc. test properties / characteristics

_paramtetric test_: Assumes distribution family of the test statistics

_non-parametric test_ (aka _distribution free_): No assumpotions on the distribution of the test statistic.


==== One sample test

_one sample test_: Only one sample, only one test statistic, treat every member of the sample the same way.


==== Unpaired two sample test

_unpaired two sample test_ (or _independent two sample test_): Two samples, e.g. one treated with treatment A and the other with treatment B (which might be `no treatment at all'). More formally, each of the two samples is drawn from another population, and the two populations have potentially different distributions.  Often the test statistic d is the difference or some kind of `difference', often standardized in some way, between the two sample means. The H~0~ is that the two population distributions are equal, which often means d = 0.

Disadvantage:

- The groups need to be really similar.  E.g. by chance the elements in either group might have something in common which has nothing to do with their treatment, but still influences the outcome of the test statistic.

- There might be a big variance in the test static.  E.g. if we measure how long people sleep, after treatment A and after treatment B: there is anyway a rather large variance in how long different people sleep on average (opposed to how long a given person sleeps in a given night).   We don't want that variance to have an influence on our result.  In the paired two sample test, that variance cancels out in the step of building the difference.

Examples:

- parametric unpaired two sample tests: H~0~: XÌ„~1~ and XÌ„~2~ are equal

  * <<z_test>> (assumes normal distr. with known variance): z = (XÌ„~1~ - XÌ„~2~) / (Ïƒâˆš(1/n~1~ + 1/n~2~)) ~ N(0, 1)

  * <<t_test>> (assumes normal distr. with unknown variance):

    ** equal sample sizes, equal variance: test statistic t = (XÌ„~1~ - XÌ„~2~) / (s~p~Â·âˆš(2/n)) \~ t~2n-2~

    ** equal variance: test statistic t = (XÌ„~1~ - XÌ„~2~) / (sÌ‚~p~âˆš(1/n~1~ + 1/n~2~)) \~ t~n1+n2-2~, where s~p~ denotes the pooled variance.

    ** general: Welch's t-test *to-do*

- non-parametric unpaired two sample tests:

  * <<permutation_test>>

  * <<wilcoxon_rank_sum_test>>


==== Paired two sample test

_paired two sample test_  (or _paired difference test_ or _paired sample test_): Treat every element in the sample with treatment A and with treatment B (again, can be `no treatent at all').  The test statistic is for example the mean of the differences of each pair.

Alternatively, we can match _match_ (or _pair_) every element in the treatment group with an element of the control group, the control group and the matching in a way that the matched pair shares similat observable characteristics.  Matching is however prominently critized.

*to-do* I don't see how the term two sample test still applies here -- the whole point is that its _not_ two samples

**to-do**(5) Are the terms "paired difference test" and "unpaired two sample test" really refering to exactly the same thing?

*to-do* In case of matching, what is then the difference to unpaired two sample test?

Examples: <<wilcoxon_signed_rank_test>>


[[permutation_test]]
==== Permutation test / randomization test

A non-parametric two sample test. General idea: Use permutations of group assignments to destroy the relationship that is to be tested under H~0~ while keeping all other relevant structure.  For each permutation, compute the test statistic, which overall delivers an empirical distribution called _permutation distribution_. Provides type I error control, proof below.

Informal proof for type I error control: When the data does come from H~0~, then the obtained permutation distribution is the distribution of the test statistic under H~0~. This is all we need for type I error control, since we need to control the probability of a false decision under H~0~.

t-test is an approximation to a permutation test.  Permutation tests are known since long, but for a long time we didn't had the computational power to make them feasible, and as a consequence were forced to use approximations like t-test.  Nowadays permutation tests are feasible.

Pro: No parametric assumptions

Pro: Free to use any test statistic

Pro: p-values and type I error control are exact if all permutations are considered. If only a subset of permutations are considered, it's an approximation.

**to-do**(3) Also the lecture scripts list "Paired two sample test / one-sample test for symmetry" as an example (or examples?) for perumatation test.  I don't understand that.

Contra: Computationally expensive

Contra: Not everything can be formulated as permutation test. E.g. in linear regression, there is no straightforward permutation test for individual coefficients.


===== As unpaired two sample test

Given population F~1~ and F~2~, and a sample from each, Y~1~^(1)^, ..., Y~n1~^(1)^ \~ F~1~ and Y~1~^(2)^,...Y~n2~^(2)^ \~ F~2~. H~0~: F~1~ = F~2~ (i.e. treatment has no effect), H~a~ : F~1~ is a shifted version of F~2~ (either in a two tailed or one tailed way).  The test statistic is a function of two samples, measuring some kind of difference between the two samples. For example sum of ranks (ranks with respect to combined sample) of sample1 (i.e. <<wilcoxon_rank_sum_test>> as permutation test), or median(sample1) - median(sample2).

- Compute t~obs~ using the original two samples.

- For all possible permutations (i.e. group/sample assignments) (or, computationally cheaper, repeatedly for a permutation selected uniformely at random from all possible permutations): compute t~i~, where i denotes the i-th permutation.  We can permute since under H~0~ assignment to sampe 1 or sample 2 is irrelevant.

- The set of t~i~ s form the emprical conditional distribution of test statistic T given the data, also calle the _permutation distribution_.

- Compute the p-value using t~obs~ and the obtained permutation distribution.

*to-do* What are properties of a good test statistics?  It seems often to be same sort of difference.  Note that rank sum of group1 is also sort of a difference.  It must be a function where the permutation has no effect under H~0~.

*to-do* add or replace with alternative version where instead an combinedsample we have sample1 and sample2 seperately.

------------------------------------------------------------
  combinedsample <- ... # sample1 concatenate sample2
  n1 <- ... # size of sample1
  repetitioncount <- ... #

  # function underlying test statistic T
  g <- function(combinedsample, n1) { ... }

  g_on_permuted_sample <- function(combinedsample, n1) {
    n <- nrow(combinedsample)
    permutedcombinedsample <- combinedsample[sample(1:n, n, replace=F)]
    return(g(permutedcombinedsample));
  }

  t.obs.all <- replicate(repetitioncount, g_on_permuted_sample(combinedsample, n1))
  t.obs <- g(combinedsample)
  pvalue <- (sum(t.obs.all<=t.obs)+1) / (repetitioncount+1)

  hist(t.obs.all)
  abline(v=t.obs)
------------------------------------------------------------


===== As paired two sample test

Same concept as before. However as in any paired two sample test, we no longer have two populations and thus two samples.  We have one single sample from one population, each element being the difference of a elementpair from sample A and sample B.  The test statistic t is a function on that sample consisting of differences.  A possible concrete test statistic is the mean (of the differences).

Under H~0~, the signs of the observations are random, so we can permute them, which overall delivers the empircal distribution of t.  With that, we can conduct a normal hypothesis test.

------------------------------------------------------------
  g <- function(sample) { ... }

  g_on_permuted_sample <- function(sample) {
    n <- nrow(sample)
    signs <- sample(c(-1,1), n, replace=T)
    sample.new <- signs * sample
    return(g(sample.new))
  }
------------------------------------------------------------


===== Permutationt test example: Correlation

Regression/classification setting. H~0~: no relationship between X and Y. Thus under H~0~, we can permute the Y values (or the X values/rows). As test statistic, we can for example use a rank correlation test statistic, for example Spearman's rank correlation coefficient.


===== Permutationt test example: Correlation / Linear regression

Given Y = Î²~0~ + Î²~1~X~1~ + ... + Î²~p~X~p~ + Îµ. H~0~: Î²~0~ = ... = Î²~p~ = 0.  Thus under H~0~, we can permute the Y values (or the X values/rows).  As test statistic, we can use for example the f-statistic of the linear regression fit.

Example: Exercise series 7, exercise 3


==== Monte Carlo test

We want to make an hypothesis test, but when the distribution of the test statistic is unknown or infeasible to work with, we may can simulate it instead.  For example number of duplicates in a set of n numbers choosen from [m].  We do a number of simulations.  Each simulation chooses n numbers out of [m] and we count the duplicates.  That delivers an empirical distribution, and we can then finaly conduct a hypothesis test using that empirical distribution.


==== Comparison of tests

*to-do* flow chart with all the test: t-test, z-test, Wilcoxon, the Wald, .... Overview with pros and cons. E.g. http://health.uottawa.ca/biomech/courses/apa3381/hyp_test.pdf


[[z_test]]
==== Z-test

A _Z-test_ is any statistical hypothesis test in which the test statistic follows approximately a Normal distribution under the null hypothesis.  Because of the central limit theorem, many test statistics are approximately normally distributed for large samples.

Examples: see those of Student's t-test. Only that in an Z-test, we know the variance ÏƒÂ² of the population, or have a good enough estimator for it, which is often the case for large samples.  So e.g. building on t-test's example of a one sample test, see below, we just would change the test statistic to z = (xÌ„ - Î¼~0~) / sd[xÌ„], which is standard Normal distributed.  Recall that sd[xÌ„] = Ïƒ/âˆšn, see standard error of the mean.


[[t_test]]
==== Student's t-test

A _Student's t-test_ (or simply _t-test_) is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis.

_As one sample test_:  Given one sample with sample mean xÌ„.  We hypothise that Î¼~0~ is the population mean and want to test that.  Let Î¼ denote the (true) population mean and SÂ² the unbiased sample variance.  H~0~: Î¼ = Î¼~0~.  As test statistic we use the t-statistic t = (xÌ„ - Î¼~0~) / sdÌ‚[xÌ„], where sdÌ‚[xÌ„] = S/âˆšn, see also estimator for standard error of the mean.  Under H~0~ it's distribution is t~n-1~.

_As unpaired two sample test_:  Given two samples of equal size n and equal variance, one treated with treatment A and the other with treatment B (no treatment at all, or different treatment).  We want to test whether treatment A has an effect.  Let s~p~ denote the <<pooled_variance>>, XÌ„~A~ and XÌ„~A~ are the sample means.  H~0~: XÌ„~A~ = XÌ„~B~.  As test statistic we use the t-statistic t = (X~A~ - X~B~) / s~p~âˆš(2/n).  Under H~0~ it's distribution is t~2n-2~.

_As paired two sample test_:  Given one sample, for each member, we calculate the difference of some test statistic after treatment A and after treatment B (no effect / controll), see also paire two sample test.  We want to test whether treatment A has an effect.  Let n denote the sample size, X~D~ the average of the differences and sÂ²~D~ the variance of the differences.  H~0~: X~D~ = Î¼~0~ (often 0):  As test statistic we use the t-statistic t = (X~D~ - Î¼~0~) / sdÌ‚[X~D~], where sdÌ‚[X~D~] = s~D~/âˆšn, see also estimator for standard error of the mean.  Under H~0~ its distsribution is t~n-1~.

_Linear regression_, testing wether a coefficient has an effect: see <<linear_regression_models>>


*to-do* See also Wilcoxon, The Wald test


==== The Wald test

*to-do*


==== F-Test & ANOVA

An F-test is a generic name for a class of statistical tests that share the property that the test-statistic follows an F-distribution (given the null-hypothesis).

One of the most common cases where a test-statistic `ends up' having an F-distribution, is when the ratio between two variances is calculated.

An ANOVA is a specific type of procedure that produces an F-statistic, because it tests the ratio between systematic variance and error-variance.


[[wilcoxon_rank_sum_test]]
==== Wilcoxon rank sum test (or Mann-Whitney U test)

A two sample test using the test statistic U which is the sum of ranks (ranks with respect to the combined sample) in smaple/group 1 (or sample/group 2, doesn't matter), and the null hypothesis H~0~ that the distributions of the two samples are equal.  If the two sample sizes are equal, the distribution of U under H~0~ is known.  For small sample sizes (~20), it's given by tables, for large sample sizes it can be approximated by a Normal distribution.

Don't confuse with <<wilcoxon_signed_rank_test>>.

Pro: No parametric assumptions

Pro: Robust, because the sum of ranks of group 1 statistic is robust.  E.g. if the largest value in a sample gets even larger, the mean would change, but the sum of ranks doesn't.

Pro: Doesn't require the two populations to be normally distributed, which is an advantage over the t-test.

Neutral: Power almost identical to that of t-test if distributions are Normal.

Pro: The null distribution (i.e. U under H~0~) is independent of F~1~ and F~2~.

_As a non-parametric unpaired two sample test_:  Regular hypothesis test. Given population F~1~ and F~2~, and a sample from each.  H~0~: F~1~ = F~2~, H~a~ : F~1~ is a shifted version of F~2~ (either in a two tailed or one tailed way).  Compute u~obs~ from the given sample, and from u~obs~, using the known distribution of U, the p-value.

_As unpaired two sample permutation test_:  An unpaired two sample permuatation test where the test statistic is U.


[[wilcoxon_signed_rank_test]]
==== Wilcoxon signed rank test

Don't confuse with <<wilcoxon_rank_sum_test>>.

Is a non-parametric paired two sample permutation test.  Let X~1~, ..., X~m~ \~ F~X~ and Y~1~, ..., Y~m~ \~ F~Y~ be independent, where (X~i~, Y~i~) is measured on the same subject i. Let D~i~ = X~i~ - Y~i~. The test statistic V is the following.  First remove all D~i~ = 0, resulting in a set of DÊ¹~i~.  V = âˆ‘rank~i~Â·H(DÊ¹~i~), where rank~i~ is the rank of |DÊ¹~i~| among all |DÊ¹~i~|, and H(x) is the heavyside step function (0 for x < 0, 1 for x > 0).

The null hypothesis H~0~: The distribution of the test statistic V is symmetric around a = 0 (or equivalently, F~X~ = F~Y~).

See <<permuatation_test>> how to conduct the test as a whole. In brief: Exercise all possible permutations (or exercise a subset of N of those permutations).  Permuting here means permute (X~i~, Y~i~) for any i (or alternatively, for each D~i~, at random flip sign).  For each permutation, compute the test statistic V~i~.  Overall this delivers a empirical permutation distribution of V.  With this distribution and v~obs~ we can compute the p-value.

Pro: Doesn't require the two populations to be normally distributed, which is an advantage over the t-test.

Sidenote: Under H~0~, the distribution of V is a known distribution, however with no simple expression.  As the sample size increases, it converges to a normal distribution.  Thus we could also conduct a non-permutation test, and use that known distribution instead of the permuatation distribution presented here.


== Algebra and Arithmetic


=== Exponentiation, root, logarithm

base^exponent^ = power

^degree^âˆšradicand = root

log~base~(antilogarithm) = logarithm

References:

- Notes on Logarithms and Units: https://www.cs.auckland.ac.nz/courses/compsci314s1c/resources/logNotes.pdf


=== Number theory

â„• natural numbers. Whether 0 âˆˆ â„• is not clearly defined.

â„•~0~, â„•^0^, â„¤~â‰¥0~, â„¤^*^ non-negative integers

â„•~>0~, â„¤^+^ positive integers

â„¤ integers. Z is for the German word Zahlen.

â„š rational numbers. Q is for the German word Quotient.

â„ real numbers

0 is neither positive nor negative.

References:

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapters "Number Theory"

- Book "Introduction to algorithms", chapter "31 Number-Theoretic Algorithms"


==== Divisibility and greatest common divisor

**In this subchapter, we're only looking at integers.**

a _divides_ b (or a is a _divisor_ of b, or b is _divisible_ by a), denoted a | b, iff there is a k such that ak=b.  b and 1 are so-called _trivial divisors_ of b.  Nontrivial divisors of b are called _factors_ of b.  If additionally k â‰¥ 1, we say b is a _multiple_ of a.

Divisibility is reflexiv and transitiv, bot not symmetric. *to-do* write more explicitely using formulas

a|0 (by agreement)

f|a and f|b â‡’ f|(sa+tb) for any s and t (a linear combination of a and b is divisible by any common factor of a and b)

a|b and b|a â‡’ a=b

n is a _linear combination_ of b~0~, ..., b~k~ â‡” n = âˆ‘s~i~b~i~.

A _commonon divisor_ of a and b is a number that divides them both.  The _greatest common divisor_ (_GCD_) (or _greatest common factor_ or _highest common divisor_) of a and b is denoted gcd(a,b).  By convention gcd(0, 0) = 0.

gcd(a,b) = gcd(b,a) (commutative)

gcd(a, gcd(b,c)) = gcd(gcd(a,b), c) (associative)

gcd(a, b, c) = gcd(gcd(a, b), c) (gcd of more than two arguments)

d|a and d|b â‡’ d|gcd(a,b)

a|bc and gcd(a,b) = d â‡’ a/d | c

gcd(ma, mb) = m gcd(a, b) âˆ€ m âˆˆ â„•

gcd(m + mb, b) = gcd(a, b)

gcd(a, ma) = a

gcd(a,0) = |a|

gcd(a,c)=1 and gcd(b,c)=1 â‡” gcd(ab,c)=1

gcd(a,b) = gcd(b, a mod b) (see Euclid's algorithm)

Two integers and b are _relative prime_ if gcd(a,b) = 1.

From the fundamental theorem of arithmetic directly follows that gcd(a, b) = product of primes common to a and b.  Thus an inefficient algorithm to compute gcd(a, b) is to prime factorize a and b, compare the factors, and build the product of the common factors.

_BÃ©zout's lemma_ (or _BÃ©zout's idendity_): For any nonzero a and b:
1) gcd(a,b) = sa+tb for some s and t; i.e. gcd(a,b) is a linear combination of a and b.
2) gcd(a,b) is the smallest positive integer that can be written as sa+tb.
3) sa+tb | gcd(a,b) for any s and t; i.e. every linear combination of a and b is a multiple of gcd(a,b).

_Euclid's algorithm_ Recursively solve gcd(a,b) by gcd(a,b) = gcd(b, a mod b). The bottom case is b = 0, in which case gcd(a,0) = |a|.

binary method to compute gcd: *to-do*


==== Prime numbers

**In this subchapter, we're only looking at integers.**

A _prime_ is a number greater than 1 that is divisible only by itself and 1. A number other than 0, 1 and -1 that is not a prime is called _composite_.

_Fundamental Theorem of Arithmetic_: Every positive integer is a product of a unique weakly decreasing sequence of primes.

For all primes p and any a,b: if p|ab then p|a or p|b.

There are infinitely many primes.

The _prime-counting function_ Ï€(x) is the function giving the number of primes less than or equal to a given number x.

_Prime Number Theorem_: Ï€(x) ~ x/ln(x). Thus as a rule of thumb, a given integer x is prime with a probability of about 1/ln(x). For x>67: Ï€(x) > x/ln(x).

_Chebyshev's Theorem on Prime Density_: Ï€(x) > x / (3 ln x).

See also algorithms_and_data_structures.adoc, chapters ``primalty testing'' and ``generating primes''.


==== Modular arithmetic

**In this subchapter, we're only looking at integers.**

_Division Theorem_ (or _Division Algorithm_): Let n (_numerator_) and d (_denominator_) â‰  0 be integers, then there exists a unique pair of integers q (_quotient_) and r (_remainder_) such that qÂ·d + r = n and 0 â‰¤ r < |d|.
Note that by this definition, the remainder is always nonnegative, as opposed to how many programming languages define it.

Common notations for the _remainder operation_ (or _modulo operation_) are n mod d or rem(n, d).  Common notations for _quotient operation_ are n div d or qcnt(n, d).

_Modular arithmetic_ (or _clock arithmetic_) is the arithmetic of congruences.

A _congruence relation_ (or simply _congurence_) is an equivalence relation on an algebraic structure that is compatible with the structure.

_Congruence modulo n_ on the set of integers is a congruence relation. a â‰¡ b (mod n) denotes ``a is congruent to b (modulo n)'' or ``a and b are congruent modulo n''.  The number n is called the _modulus_.  These three claims are equivalent:

a â‰¡ b (mod n) â‡” +
n | (a-b) â‡” +
a = b + kn âˆ€ k âˆˆ â„•

In the following, the explicit (mod n) is omitted for brevity.

An a^-1^ such that aÂ·a^-1^ â‰¡ 1 is called _modular multiplicative inverse_ of a modulo n.  a^-1^ exists iff a is coprime with n.

a â‰¡ a [reflexiv]

a â‰¡ b â‡” b â‰¡ a [symetric]

a â‰¡ b and b â‰¡ c â‡’ a â‰¡ c [transitiv]

a â‰¡ b â‡” a + k â‰¡ b + k âˆ€ k âˆˆ â„¤ [compatibility with translation]

a â‰¡ b â‡’ ka â‰¡ kb âˆ€ k âˆˆ â„¤ [compatibility with scaling]

ka â‰¡ kb and k is coprime with n â‡’ a â‰¡ b

a â‰¡ b â‡’ a^k^ â‰¡ b^k^ âˆ€ k âˆˆ â„• [compatibility with exponentation]

a â‰¡ b and c â‰¡ d â‡’ a + c â‰¡ b + d [compatibility with addition]

a â‰¡ b and c â‰¡ d â‡’ a - c â‰¡ b - d [compatibility with subtraction]

a â‰¡ b and c â‰¡ d â‡’ ac â‰¡ bd [compatibility with multiplication]

a â‰¡ b and a^-1^ exists â‡’ a^-1^ â‰¡ b^-1^ [compatibility with multiplicative inverse]

When a = xÂ² mod p for an a âˆˆ â„¤~p~ and any x âˆˆ â„¤~p~, then a is called a _quadratic residue_.

When a â‰  xÂ² mod p for an a âˆˆ â„¤~p~ and all x âˆˆ â„¤~p~, then a is called a _quadratic nonresidue_.

Exactly half of the nonzero elements of field â„¤~p~ are quadratic residues.

_Legendre symbol_: Leg(a|p) â‰¡ {1 if a is a quadratic residue, -1 if a is a quadratic non-residue, 0 if p|a}.

p is odd prime â‡” a^(p-1)/2^ â‰¡ Leg(a|p) âˆ€ a âˆˆ â„¤~p~ - \{0}. [Euler's Criterion]

_Jacobi Symbol_: Jac(a|n) = âˆ~1â‰¤iâ‰¤l~Leg(a|p~i~)^k~i~^ = âˆ~1â‰¤iâ‰¤l~(a^(p~i~-1)/1^ mod p~i~)^k~i~^ = {1, -1}, where gcd(a,n) = 1 and where p~1~^k~1~^ Â· ... Â· p~l~^k~l~^ is the prime factorization of n.

p is prime and a âˆˆ â„¤ and gcd(a,p) = 1 â‡’ a^p-1^ â‰¡ 1 [Fermat's little theorem]

p is prime and a âˆˆ â„¤~p~ - \{0} â‡’ a^-1^ = a^p-2^ mod p [Consequence of Fermat's little theorem]

p is prime â‡” (p-1)! â‰¡ -1 [Wilson's theorem]

Chinese Remainder Theorem:  Let m = m~1~Â·...Â·m~k~ where k âˆˆ â„¤^+^ and m~i~ âˆˆ â„¤^â‰¥2^ are pairwise coprimes. For any sequence r~1~ âˆˆ â„¤~m1~, ..., r~k~ âˆˆ â„¤~mk~ there is an unique r âˆˆ â„¤~m~ such that r â‰¡ r~i~ (mod m~i~) âˆ€ i âˆˆ [k].


==== Abstract Algebra

A set S is _closed_ under an n-ary operation f if f: S^n^ â†’ S. A set S is closed under a collection of operations if it is closed under each of the operations individually.

An _algebraic structure_ (or simply _algebra_) is a pair (S, F) where S is a set closed under a set F of operations.

Given an algebra (S, âˆ—) where âˆ— is a binary operation.  An element e âˆˆ S is called a _left identity_ if e âˆ— x = x âˆ€ x âˆˆ S, and a _right identity_ if x âˆ— e = x âˆ€ x âˆˆ S.  If e is both a left and a right identity, then it is called a _two-sided identity element_ (or _two-sided neutral element_ or simply _identity_) according to âˆ— in S.

Given an algebra (S, âˆ—) where âˆ— is a binary operation, elements a, b âˆˆ S, and the neutral element e âˆˆ S.  If a âˆ— b = e, then a is called a _left inverse_ of b and b is called a _right inverse_ of a.  If an element is both a left and a right inverse, it is called a _two-sided inverse_ (or simply _inverse_).  The inverse element of element x âˆˆ S is denoted x^-1^ (or i(x) or -x if the algebra's operation is denoted +).

Note that the algebra (â„¤, +) where + denotes normal addition, subtraction a - b is modeled by adding the inverse, i.e. a + i(b) (or a + -b).  Likewise for division in algebra (â„, Â·) where Â· denotes normal multiplication: division a / b is modeled by multiplying the inverse, i.e. a Â· i(b) (or a Â· b^-1^).

A _semigroup_ is an algebra (S, âˆ—) where âˆ— is a binary associative operation on S.

A _monoid_ is an algebra (M, âˆ—) where âˆ— is a binary associative operation and S has a neutral element.

A _group_ is an algebra (S, âˆ—) where âˆ— is a binary associative operation and S has a neutral element and every element x âˆˆ S has an inverse element.

The _order_ (or _cardinality_) of a group G (or ring or field), denoted |G|, is the number of elements it contains.

A group is _commutativ_ (or _abelian_) if x âˆ— y = y âˆ— x âˆ€ x,y âˆˆ S.  If a group is not commutative, its called _noncommutative_ (or _non-abelian_).

Let (S, âˆ—) be a group with the neutral element e. The _i-th power_ of x âˆˆ S, denoted x^i^, is inductively defined as follows, for any x âˆˆ S and i âˆˆ â„¤:

i) x^0^ = e (x^0^ is called the _trivial power of x_)

ii) x^1^ = x

iii) x^i^ = x âˆ— x^i-1^ âˆ€ i > 1 (x^i^ is called a _nontrivial power of a_)

iv) x^-i^ = (i(x))^i^ âˆ€ i â‰¥ 1

Given a group G = (S, âˆ—), an element x âˆˆ S is called a _generator_ of that group if S = {x^i^|iâˆˆâ„¤}.  We also denote that with âŸ¨xâŸ© = G. If a group has a generator, then the group is called _cyclic_.

Given a group (S, âˆ—) with identity e. The _order_ of an element x âˆˆ S, denoted |x|, is the smallest positive integer n such that x^n^ = e. If there is no such n, then element x has _infinite order_.

A _ring_ is an algebra (R, +, Â·) where (R, +) is a commutative group and (R, Â·) is a semigroup and Â· is distributive over + (*to-do* unspecified whether left/right/total distributive).  Wether or not a ring requires an identity under Â· is under debate, see also ring with identity.

A ring (R, +, Â·) with neutral element 0 under + is called _zero division free_ if x Â· y â‰  0 âˆ€ x, y âˆˆ R - \{0}.

My personal derivation: The neutral element 0 under + has no inverse under Â·, so we want to prohibit having to take the inverse of 0.  We take the inverse of an element when a = c Â· 1/b â‡” a Â· b = c.  So in our use case we want to prohibit (for a,c âˆˆ R - \{0}) that a Â· 0 = c â‡” 1/a Â· a Â· 0 = 1/a Â· c â‡” 0 = 1/a Â· c, which is what zero division free said.

If the Â· operation of a ring (R, +, Â·) is commutative, it's called a _commutative ring_.  If the Â· operation is not commutative, it's called a _noncommutative ring_ (or simply ring).

If the Â· operation of a ring (R, +, Â·) with identity 0 under + has an identity for every element in R - \{0}, it's called a _ring with identity_.

A _field_ (_KÃ¶rper_ in German) (R, +, Â·) with neutral element 0 under + is a zero division free ring where for Â· the following holds: commutative, R - \{0} has an identity and there's an inverse for all x âˆˆ R - \{0}.

Note that some authors say that a ring (R, +, Â·) is the commutative group (R, +) with identity 0 and the commutative group (R-\{0}, Â·).  This is not entirely correct because it technically says that Â· is closed under R-\{0} which is not what we mean.  We do want to allow 0 as operand and result of Â·, we only want to disallow division by 0.

For example, â„š and â„ build fields with respect to addition and multiplication.  However for â„¤ it is impossible to define division.

Given a field K = (R, +, Â·) with identity 1 under + and identity 0 under Â·, the _field characteristic_ ch(K) is the minimum number of times 1 has to be added (e.g. 1+1 counts as two times) to equal 0.  If 0 is never reached, then ch(K) = 0.

A _finite field_ (or _Galois field_) is a field with a finite field order.  The order of a finite field is always a prime power p^k^, where p is a prime and k is a positive integer.  All finite fields of a given order are isomorphic.  In case k = 1, the finite field is called a _prime field_, denoted GF(p) (or ğ”½~p~), and is the field of residue classes modulo p, where the elements of GF(p) are denoted 0, ..., p-1.  Thus a = b in GF(p) means the same as a â‰¡ b (mod p).  p is the characterstic of the prime field.  The inverse with respect to Â· can be computed with the _extended Eucledian algorithm_ (*to-do*).  In case k > 1, the finite field is denoted GF(p^k^) (or ğ”½~p^k^~).

â„¤/pâ„¤ denotes a special case of a quotient group (recall a group as only one operation), but is apparently sometimes used to denote a prime field (recall that a field has two operations).

â„¤~n~ denotes an abstract algebra over set {0, ..., n-1} with mod n modular arithmetic.  Wether â„¤~n~ denotes a finite group or a finite field (and thus prime field) depends on the context.

*to-do* vector space, norm, module,

Summary:

R denotes the set of elements of the algebraic structure.  ba denotes binary associative operation.  The identity under +, if it exists, is denoted 0.  NA denotes not available.  d denotes that Â· is distributive over +.  e denotes existence of an identity under the given operation.  e/0 denotes existence of an identity within R - \{0} under given operation.  inv denotes the existence of a inverse element for every element of the algebraic structure under the given operation.   inv/0 denotes the existence of a inverse element for every element R - \{0} under the given operation.  zdf denotes zero division free, see there.

|=====
|                    |      | + (ba)    | Â· (ba, d)
| semigroup          | +    |           | NA
| monoid             | +    | e         | NA
| group              | +-   | e, inv    | NA
| commutative group  | +-   | e, inv, c | NA
| ring               | +-Â·  | e, inv, c | [e/0]
| ring with identity | +-Â·  | e, inv, c | e/0
| commutative ring   | +-Â·  | e, inv, c | c
| zdf ring           | +-Â·  | e, inv, c | zdf
| field              | +-Â·/ | e, inv, c | zdf, c, e/0, inv/0
|=====


References:

- https://www.youtube.com/playlist?list=PLi01XoE8jYoi3SgnnGorR_XOW3IcK-TP6

- Book ``Algorithmics for Hard Problems: Introduction to Combinatorial Optimization, Randomization, Approximation, and Heuristics'', 2nd Edition, Juray HromkoviÄ, chapter ``2.2.4 Algebra and Number Theory''

- Book ``Design and Analysis of Randomized Algorithms'', chapter ``A.2 Algebra and Number Theory'' starting p. 239 bottom

=== Linear algebra

The _determinant_ of a square matrix A is denoted det(A) or |A|.

In the 2D case:

--------------------------------------------------
      |a b|
|A| = |   | = ad - bc
      |c d|
--------------------------------------------------

The geometric interpretation is that, when you think about the matrix representing a linear transformation, the absolute value of the determinant is the factor applied to an area (in the 2D case, volume in 3D case and so on).  Also, in the 2D case, if A is build by combining column vectors v1 and v2 side by side, the determinant is positive when v1 is clockwise from v2 (their tails coinciding), negative when v1 is counterclockwise, and zero when the two are colinear.



== Geometry

A _metric space_ M is an ordered pair (S, d) where S is a set and d is a metric on S.  A _metric_ (or _distance function_ or simply _distance_) is a function d that defines a distance between each pair of elements of a set S.  It is defined as d: Sâ¨¯S â†’ â„~+~, where for all x,y,z âˆˆ S the following conditions are satisfied:

d(x,y) â‰¥ 0 [small]#(non-negatity)# +
d(x,y) = 0 â‡” x = y [small]#(identity of indiscernibles)# +
d(x,y) = d(y,x) [small]#(symmetry)# +
d(x,z) â‰¤ d(x,y) + d(y,z) [small]#(triangle inequality)#

_triangle inequality_: Definition above. In other words, detours (two edges) are never shorter (in terms of d(Â·,Â·)) than the direct edge.

A _right angle_ is an angle of exactly 90Â° (Ï€/2 radians).  Two vectors u and v are _perpendicular_, denoted uâŸ‚v, iff their angle is a right angle, or equivalently, if their scalar product is zero.  A set of vectors is _orthogonal_ iff they are pairwise perpendicular.  A _normal_ vector of a point on a smooth surface is any vector perpendicular to the plane.

The _dot product_ (or _scalar product_) of two vectors xâƒ— and yâƒ— is defined as xâƒ—Â·yâƒ— = âˆ‘x~i~y~i~ = â€–xâƒ—â€–â€–yâƒ—â€–cos(Î¸).  The former variant is the algebraic interpretation, the later is the geometric interpration.  More concretely, the geometric interpretation is that xâƒ—Â·(yâƒ—/â€–yâƒ—â€–) is the projection of xâƒ— onto yâƒ—, when the two vectors are placed so that their tails coincide.

The _inner product_ generalizes the dot product to abstract vector spaces over a field of scalars. It is usually denoted using angular brackets by âŸ¨a,bâŸ©.  In Euclidean geometry, the two are equivalent.

The _cross product_ (or _vector product_ or _directed area product_ (in Euclidean geometry)) of two vectors xâƒ— and yâƒ— is defined as xâƒ—â¨¯yâƒ— = â€–xâƒ—â€–â€–yâƒ—â€–sin(Î¸)nâƒ—.  nâƒ— is the unit vector normal to the plane containing xâƒ— and yâƒ—.  By convention, the direction of nâƒ— is given by the _right-hand rule_: The index finger represents xâƒ—, the middle finger yâƒ—, and the thumb xâƒ—â¨¯yâƒ—.  The maginitude of the cross product can be interpreted as the area of the parallelogram having xâƒ— and yâƒ— as sides: â€–xâƒ—â¨¯yâƒ—â€– = â€–xâƒ—â€–â€–yâƒ—â€–sin(Î¸).  Cross product is zero â‡” the lines are parallel. Cross product is positive (negative) â‡” xâƒ— is clockwise (counterclockwise) from yâƒ— (their tails coinciding).



== Misc

=== Fibonacci sequence / numbers

reccurence relation: F~n~ = F~n-1~ + F~n-2~

closed form expression: F~n~ = (Ï•^n^ - Ïˆ^n^) / âˆš5 = [Ï•^n^ / âˆš5], where
Ï• is golden ratio and Ïˆ=1-Ï•, and [x] is the nearest integer function
(aka round function).

Note: lim~nâ†’âˆ~ F~n~ / F~n-1~ = Ï•

Applications: Fibonacci heap


=== Golden ratio

Ï• = (1+âˆš5)/2 â‰ˆ 1.618â€¦

Two quantities a and b are in the golden ratio Ï• iff a+b / a = a / b =
Ï•, i.e. a=Ï•b

=== Factorial

reccurence relation: x! = x*(x-1) and 0!=1

stirlings approximation: n! ~ âˆš(2Ï€n)*(n/e)^n^



== References

- MIT course 6.042 "Mathematics for computer science".
  * spring 2015, index: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/course-index/
  * spring 2015, textbook: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/readings/MIT6_042JS15_textbook.pdf
  * fall 2010, video lectures: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/video-lectures/
  * fall 2010, readings: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/

- MIT course 18.650 Statistics for Applications, fall 2016: https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/[lecture notes], https://www.youtube.com/watch?v=VPZD_aij8H0&list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0[videos]

- Book ``Algorithmics for Hard Problems: Introduction to Combinatorial Optimization, Randomization, Approximation, and Heuristics'', 2nd Edition, Juray HromkoviÄ. The Introduction chapter serves as good summary of computer science fundamentals.

- Book ``Design and Analysis of Randomized Algorithms'', chapters ``A Fundamentals of Mathematics'' p. 227 and ``2.2 Elementary Probability Theory'' p. 20

- Book ``Modern Cryptography: Theory and Practice'' has a mathematical foundations part


== to-do

- skalarproduct
- greatest common divider/divisor
- log/exp relation to mul/div
- angle between vector
