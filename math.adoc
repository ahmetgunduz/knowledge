// The markup language of this document is AsciiDoc
:encoding: UTF-8
:toc:
:toclevels: 4


= Mathematics

Mainly mathematics tailored towards the nees of computer science.


== Misc basics

A _set_ is an unordered collection of distinct _elements_ (or _objects_, or _components_).
The notiation {a, b} defines a set consisting of elements a and b.
The _size_ (or _cardinality_) of a set S is the number of its elements, denoted as |S|.
A _sequence_ is an ordered collection of possibly duplicate elements,
The notation (b, a, a) defines a sequence.

The _powerset_ (or _power set_) of a set S is the set of all subsets of S, including the empty set and S itself.

[[permutation]]
A _permutation_ of a set S is a sequence containing every element of S exactly once.
The number of permutations of a n-element set is n!.

A (total) _function_ f : X → Y maps all elements of its input set X to elements of its output set Y.
Note a function is allowed to map multiple elements of X to a given element of Y.
The input set is called the _domain_.
The output set is called the _codomain_.
The set of elements in the codomain that appear in f's mappings is called the _range_ (or _image_).
A _partial function_ does not map all the elements of its input.
The term _total function_ is a synonym to `function' as defined above.
The term total function is typically only used when a disambiguition to partial function is needed.
Given a _injective function_, each element of Y is mapped to at most once.
Given a _bijective function_, each element of Y is mapped to exactly once.
Given a _surjective function_, each element of Y is mapped to at least once.
A _k-to-1 function_ maps exactly k elements of X to every element of Y.

A function is bijective if and only if it's both injective and surjective.


== Counting & Combinatorics

Typically we are typically good at determining the size of a set S of sequences.
Thus when faced with the problem of finding the size of a set P, we often try to define a set S of sequences and a function f : S → P.
Typically, once we defined the set S of sequences, the function f is obvious or implied by the definion of S.
Say we find a set of sequences S and a bijection function f : S → P, then the product rule gives us |S|, and the bijection property gives us |P| = |S|.
Likewise, say we find a k-to-1 function f : S → P, the product rule and the division rule give us |P| = |S| / k.

For example consider poker.
In poker a hand is 5 cards out of a deck of 52 cards.
Each card has a rank and a suit.
There are 13 ranks and 4 suits.

How many different hands have a four-of-a-kind?
We try to define a set S of sequences describing the problem, i.e. having a relation to the set P asked for by the problem statement.
A possible solution is that a sequence of S has the follwing 3 elements.
1) The first is the rank of the four card; 13 possibilities.
2) The second is the rank of the extra card; 12 possibilities.
3) The third is the suit of the extra card; 4 possibilities.
Due to the product rule, |S| = 13·12·4.
There's an obvious bijection from |S| to |P|.
Due to to bijective property, |P| = |S|.

How many diffrent hands have a two pairs; that is, two cards of one rank, two cards of another rank, and one card of a third rank?
We try to define a set S of sequences describing the problem, i.e. having a relation to the set P asked for by the problem statement.
A possible solution is that a sequence of S has the follwing 6 elements.
1) The rank of the first pair; 13 possibilities.
2) The suits of the first pair; C(4 2) possibilities.
3) The rank of the second pair; 12 possibilities.
4) The suits of the second pair; C(4 2) possibilities.
5) The rank of the extra card; 11 possibilities.
6) The suits of the extra card; C(4 1) possibilities. 
Due to the product rule, |S| = 13·C(4 2)·12·C(4 2)·11*C(4 1).
Since the first pair, i.e. element 1 and 2, and the second pair, element 3 and 4, are indistinguishable, there is a 2-to-1 mapping from |S| to |P|.
To be precise, it's 2 because there are 2! = 2 ways to permute an 2-element set.
Due to the 2-to-1 mapping and the division rule, |P| = |S| / 2.

_Pigeonhole principle_: If |A| > |B|, then for every total function f : A → B, there exists (at least) two diffrent elements of A that are mapped to the same element of B.
The _generalized pigeonhole principle_ states that if |A| > k·|B|, then every total function f : A → B maps at least k + 1 different elements of A to the same element of
B.

_Product Rule_: Given the sets A~1~, ..., A~n~.
Let A~1~ ⨯ ... ⨯ A~n~ denote the set of all sequences whose first term is drawn from A~1~, the second term is drawn from A~2~ and so forth.
|A~1~ ⨯ ... ⨯ A~n~| = ∏|A~n~|.
E.g. the number of different 3 digit hex numbers is 16·16·16.

_Division Rule_: If there is a k-to-1 function f : A → B, then |A|=k·|B|.

As a special case of the division rule: If there is a bijective function f : A → B, then |A| = |B|.

_Sum Rule_: Given disjoint sets A~1~, ..., A~n~, then |⋃A~n~| = ∑|A~n~|.

_Inclusion rule_: |A∪B| = |A| + |B| - |A∩B|.
Intuition: Just imagine the generic Venn diagram of sets A and B.

_Inclusion-exclusion rule_: Is a generalisation of the inclusion rule.
For the special case of three sets: |A∪B∪C| = |A| + |B| + |C| - |A∩B| - |A∩C| - |B∩C| + |A∩B∩C|. For the formula of the general case of n sets, the internet is your friend.

_Boole's inequality_: |A∪B| ≤ |A| + |B|. Intuition: Follows from the inclusion rule.

_Union Bound_: |⋃A~n~| ≤ |A~n~|. Intuition: Generalization of Boole's inequality.

_Monocity Rule_: If A ⊆ B, then |A| = |B| ≤ |B|.

_There are 2^n^ subsets of an n-element set_.
Proof: We define a sequence S from which there is a bijection to the problem set |P|.
The i-th element of the sequence S tells if element i of the original set is part of the subset or not.
The product rule gives |S|=2^n^, and the bijecton gives |P|=|S|.

A _k-combination_ of an n-element set S is a subset of k distinct elements of S.
The number of possible k-combinations is denoted by _C(n, k)_, pronounced `n choose k'.
Less concise formulated, it's the _number of k-element subsets of an n-element set_.
C(n, k) = n! / ((n-k)!k!).
Intuition: First we have n possibilities, then (n-1) and so on until (n-k+1).
That equals n! / (n-k)!.
So far we exactly have a k-permutation.
Since the order of those k elements doesn't matter, we have to devide by the number of permutations, which is k!.

C(n, k) = C(n, n-k)

C(n, 0) = C(n, n) = 1

_binomial theorem_ (aka _binomial expansion_): (x+y)^n^ = ∑~0≤k≤n~(C(n,k)·x^k^·y^n-k^). So C(n, k) is also called the _binomial coefficient_.

A _k-combination with repetitions_ (or _k-multicombination_, or _k-multisubset_) of an n-element set S is a multiset of k (possibly identical) elements of S.
The number of such k-multisubsets is denoted by \((n k)), pronounced `n multichoose k'.
\((n k)) = C(n+k-1, k).
Intuition, using the _stars and bars_ graphical aid.
Imagine the chosen multiset of elements ω~1~ as a group of stars, the chosen multiset of elements ω~2~ as another group of stars and so on.
More precisely, do it the following way.
You have a set of k+(n-1) positions.
Note that its a set, i.e. unordered.
The following visualizes it in an ordered manner, but conceptually it's unordered.
k positions are assigned a star, n-1 positions are assigned a bar.
The bars separate groups of stars.
For example for k=6 and n=3, a possible outcome is ★★|★★★|★.
Thus the original multicombination problem reduces to choosing a set of n-1 positions out of k+(n-1) positions in order to assign bars to.
C(k+(n-1), n-1) = C(k+(n-1), k) = C(n+k-1, k).
The first transformation is true due to the general rule C(n, k) = C(n, n-k).

A _k-permutation_ (or _variation_ or _partial permutation_) is a k-element sequence consisting of distinct elements out of an n-element set.
The nuber of possible k-permutations is denoted by _P(n,k)_ = C(n,k)*k! = n! / (n-k)!.
Intuition: First we have n possibilities, then (n-1) and so on until (n-k+1).
That equals n! / (n-k)! = C(n,k)*k!.

[[permutation_with_repetition]]
A _k-tuple_ (or _permutation with repetition_) is a k-element sequence consisting of (possibly identical) elements out of an n-element set.
The number of k-tubles of an n-element set is k^n^.
Intuition: First we have n possibilities, then again n, and so on, k times.

Overview denoting k-element entities and the number of such entities
given an n-element set (implies unordered and distinct):

|=====
|                    | without repetitions                | with repetitions
| subset (unordered) | k-combination, C(n, k)             | k-multicombination, C(n+k-1, k)
| sequence (ordered) | k-permutation, P(n, k) = C(n, k)k! | k-tuple, k^n^
|=====


Further typicall problems:

_bookkeeper rule_ (an inofficial term made up by the MIT): Given a k-element set {e~1~, ..., e~n~}, the number of sequences consisting of n~1~ e~1~, ..., n~k~ e~k~ is (∑n~i~)! / ∏(n~i~!).
Intuition, using the problem of finding the number of ways to rearange the letters in the word `bookkeeper'.
There are n~1~=1 b's, n~2~=2 o's and so on up to n~6~ r's.
I.e. k=6, but that is not really important.
There is a total of ∑n~i~ = 10 letters.
So there are 10! permutations of these letters.
However, we can't distinguish the n~2~=2 o's in each sequence, so we have to devide by 2!.
Likewise, we have to devide analogously for each of {b, o, k, e, p, r}.

Corollary to the bookkeeper rule: How many x-bit sequences contain y zeros? By the bookkeeper rule, n~1~ = y, n~2~ = x - y, thus x! / (y!·(x-y)!).

References:

- The above is largely based upon MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapter "Counting"


== Probability and Statistics

The _sample space_ S (or Ω) is the set of possible outcomes of an _experiment_.
An element ω ∈ S is called an _outcome_ (or _sample outcome_ or _element_ or _realization_ (is ambigous to the realization of a random variable)).
A subset E ⊆ S is called an _event_.
In other words, an event is a set of outcomes.
∅ denotes the _null event_ which is always false.
S denotes the _true event_ which is always true.
The set of `interesting' or `known' events is denoted 𝓕.
A _probability space_ (or _probability triple_) is the tripe (sample space S, set of events 𝓕, probability function Pr).
A _probability function_ (or _probability distribution_ or _propability measure_) Pr (or P or ℙ) on a sample space S is, a bit sloppily defined, a total function Pr : 𝓕 ⟶ [0, 1] having the following two properties:
1) Pr(ω) ≥ 0 for all outcomes ω ∈ S.
2) ∑~ω∈S~ Pr(ω) = 1.
3) Pr(E) = ∑~ω∈E~Pr(ω).
It's a sloppy definition because it enforces that 𝓕 contains every outcome.
A more precise definition is that a probability function is a total function Pr : 𝓕 ⟶ [0, 1] satisfying the three _probability axioms_ (or _Kolmogorov axioms_):
1) Pr(E) ≥ 0 for all events E ∈ 𝓕.
2) Pr(S) = 1.
3) If E~1~, E~2~, ... are disjoint then Pr(⋃E~i~) = ∑Pr(E~i~).
There are multiple notations denoting the evaluation of the function Pr: Pr(...) or Pr[...] or Pr{...}.
A finite probability space S is said to be _uniform_ if Pr(ω) is the same for every outcome ω ∈ S.
In an uniform probability space, Pr(E) = |E| / |S| for any event E ⊆ S.


_conditional probability_: The probability of event A given event B is known to be true is Pr(A|B) = Pr(A∩B) / Pr(B).
Pr(A) is also called the _prior probability_ of A and Pr(A|B) the _posterior probability_ of A.
Note that the order in time in which the events A and B occur does not matter.

Intuitively Pr(A|B) is the probability of event A when only considering the alternate sample space SB = B.

--------------------------------------------------
Areas are proportional to probabilities

  Sample space S      Pr(⋅|B) intuitively defines
                      a new sample space SB = B
           A
 S   whole 'column'
  +----+------+       Pr(A|B) = Pr(A∩B) / Pr(B)
  |    |      |       = Probability of A in sample space SB
  |    |      |
  |    |      |     SB
  +----+--+---+       +-------+---+
 B|       |   |      B|       |   |
  +-------+---+       +-------+---+
                               A∩B
--------------------------------------------------

_bayes theorem_: Pr(A|B) = Pr(B|A)Pr(A) / Pr(B).


_law of total probability_: Given a partition {B~1~, ...,B~n~} of the sample space S, then Pr(A) = ∑Pr(A∩B~i~) = ∑Pr(A|B~i~)Pr(B~i~).


[[independence]]
Two events A and B are _independent_ if Pr(A|B) = Pr(A) or if (Pr(B) = 0).
Or equivalently, called the _product rule for independent events_, iff Pr(A∩B) = Pr(A)Pr(B).
Note that disjoint does _not_ imply independent.
For example say A and B are disjoint and both are non-empty, then Pr(A|B) = 0 ≠ Pr(A).
Naturally independence is a symmetric relationship.
That's why we usually say `A and B are independend' rather than `A is independent of B'.
The form `Pr(A|B) = Pr(A) or if Pr(B) = 0' shows more clearly the meaning of `the occurence of B does not affect the probability of A'.
The form `Pr(A∩B) = Pr(A)Pr(B)' shows more clearly the symmetry of indpendence.

Informally stated, A and B are independend if the probability of A is independent of whether its relative to sample space S or when considering only the restricted sample space SB = B, _or_ vice versa for B.

--------------------------------------------------
Areas are proportional to probabilities

                   Pr(A|B) = Pr(A) or if (Pr(B)=0)
                   Informally: Ratio A∩B:B equals ratio A:S,
                   i.e. probability of A is independent of whether
                   its relative to SB or to S.
 S          A                      S          A
  +-------+---+                     +-------+---+
  |       |   |                     |       |   |
  |       |   |                     |       |   |
  |       |   |  SB                 |       |   |
  +-------+---+    +-------+---+    |       |   |
 B|       |   |   B|       |   |    |       |   |
  +-------+---+    +-------+---+    +-------+---+
                            A∩B
--------------------------------------------------

Example where A and B _are_ dependend:

--------------------------------------------------
Areas are proportional to probabilities

            A
  +-------+---+
  |       |   |
  |       +---+
  +-----+-+   |
 B|     |     |
  +-----+-----+
--------------------------------------------------


--------------------------------------------------
Areas are proportional to probabilities

            A        Pr(A∩B) = Pr(A)Pr(B)
  +-------+---+      Considering the above drawings,
  |       |   |      this can only be true if
  |       |   |      both of A and B can be drawn
  +-------+---+      with straight orthogonal lines,
 B|       |   |      in which case
  +-------+---+
--------------------------------------------------


To make that example more concrete, consider that blood can have a certain type and a certain rh factor.
Say the probability Pr(T) for type T is known, and the probability Pr(F) for rh factor F is known.
The previously described Venn diagram shows that the probability somebody has type T _and_ rh factor F equals Pr(T)Pr(F) _only_ if T and F are independent.
For independence, the ratio of people having rh factor F among all people (|F| / |S| = Pr(F)) must be equal to the ratio of people having rh factor F among those having also type T (|F∩T| / |T|).

The elements of α={A~1~, ..., A~n~} are _mutually independent_ iff Pr(⋂A~i~) = ∏Pr(A~i~) for _any_ subset of α.
Mutual independence does imply pairwise indpendence, but not vice versa.

The elements of α={A~1~, ..., A~n~} are _pairwise independent_ iff for all unordered pairs {A~i~, A~j~} of distinct elements (i.e. i ≠ j), A~i~ and A~j~ are independent.
Pairwise independence does _not_ imply mutual independence.

A _decision tree_ is a graphic tool for working with outcomes and events of an probability space.
The root is the start and is not directly associated a meaning.
Given a vertex, each outward edge represents that a given `subevent' occures.
The definition of an edge's associated subevent includes that the the subevent associated with the edge's source vertex has occured.
`Subevent' is an inofficial term made up by the author.
Each vertex thus represents the subevent that all subevents of the edges of the path from the root to that vertex have occured.
Note that the subevents on the path are not required to happen in the order implied by the path.
One just has to compute the correct _conditional_ probabilities of the edges.
Each outward edge of a vertex is assigned the conditional probability that the edge's associated subevent occures, given that the subevent associated with the vertex has occured.
For each internal vertex, the sum of the probabilities of all its outward edges is 1.
By the the above definitions, given a path, the subevents associated with the edges are independent, thus they can be multiplied to get the probability of taking that path.
Each leaf represents an outcome of the experiment.
Thus the set of all leaves represents the sample space.
I.e. there is a 1 to 1 relationship between the set of all leaves and and the set of all outcomes.

Alternatively, draw the tree using the treemapping method.
You start out with a rectangle representing the root vertex of the tree.
For each child, draw a line to create a subrectangle, the sizes of the subrectangles according to the weight of the edges. All llines mutually parallel.
Recurse.
At each new level in the recursion, toggle between horizontal and vertical lines.
The result has resemblance to a Venn diagram, only that here a given event is represented by a set of possibly disconnected areas, as opposed to a single connected area.

Recipe for solving many probability problems:

. Consequently follow the rules.
Don't try to be fast.
Often the human intuition is wrong.

. Define the sample space, i.e. all possible outcomes.

. Define events of interest.

. Compute probabilities (of required outcomes). Possibly the following way: Use the tree diagram method.  Assign a probability to each (required) edge.  Calculating the probability of an outcome is then trivial.

. Compute probability of your events, which is trivial, now that you have the probabilities of the outcomes.

References:

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapter "Probability"

- MIT course 18.650 "Statistics for Applications", Fall 2016, https://www.youtube.com/playlist?list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0[videos], https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/MIT18_650F16_Introduction.pdf[lecture notes]

- Book ``All of statistics'', chapter ``1 Probability''

- Khan Academy, ``Statistics'' playlist: https://www.youtube.com/watch?v=uhxtUt_-GyM&list=PL1328115D3D8A2566


=== Random variables and expectations

Formally a random variable is a function mapping from sample space to measure space, as defined in the following.  In practice, we often think of a random variable like a random number.  In practice, the sample space associated to a random variable is rarely explicitelly mentioned, but keep in mind that it really is there.  Random variables can be interpreted as link between data and sample spaces.

--------------------------------------------------
 probability space := (sample space S, events 𝓕, probability function Pr)

            probability
 set of     function Pr
 events 𝓕 =============> [0,1]
  ^
  |set of
  |subsets  random        measure
  |         variable R    space,      CDF_R(x) := Pr(R≤x)
 sample  ===============> mostly ℝ    ================> [0,1]  
 space S
                                      E[R] := ∫x· CDF_Rʹ(x)
                                      ----------------> measure space

                                      Var[R] := E[R-E[R]]²
                                      = ∫(x-E[R])²CDF_Rʹ(x)
                                      ----------------> measure space

   S is countable    discrete R       PMF_R(x) := Pr(R = x)
   set                                if R is the identity: PMF_R = Pr
                                      ================> [0,1]

                                      E[R] = ∑x·PMF_R(x) = ∑R(ω)·Pr(ω)

   S is infinit      continous R      PDF_R(x) = CDF_Rʹ(x) (informally)
   noncountable      PDF_R exists     Pr(a≤R≤b) = integrate PDF_R(x) over [a,b]
   set                                if R is the identity: PDF_R = Pr
                                      ================> [0,1]

                                      E[R] = ∫x·PDF_R(x)
--------------------------------------------------

A _random variable_ R is a measurable total function R : S ⟶ ℝ.
Technically the range of R is the _measure space_ E, but in computer science practice the measure space is mostly ℝ.
Roughly speaking, density functions exist only when the measuere space is ℝ.
The actually observed value of a random variable R is called _realization_ of R (or _observation_).
Note that the term `realization' is ambigously also used as a synonym for outcome ω ∈ S.
An _indicator random variable_ (or _Bernoulli variable_) is a random variable with codomain {0, 1}.
A random variable is _discrete_ if its domain is a countable set.
A random variable R is _continuous_ if there exists a probability density function for it.
Note that for a continuous random variable R, Pr(R = x) = 0 for every x.
We get a non-zero probability only in a non-empty range.

There's a strong relation between events and random variables.
Any assertion about the value of a random variable defines an event.
Say the random variable C counts number of heads in 3 coin flips.
The condition C = 1 defines the event {HTT, THT, TTH}, or the condition C ≤ 2 {TTT, HTT, THT, ...}.
Looking at it from the other direction, each event E is naturally associated with a corresponding indicator random variable I~E~, where I~E~(ω) equals 1 if outcome ω ∈ E and and 0 otherwise.

Given a random variable R with measure space ℝ, its _cumulative distribution function_ (or _CDF_ or _cumulative density function_) CDF~R~ (or F~R~) : ℝ ⟶ [0, 1] is defined as CDF~R~(x) = Pr(R ≤ x).

Given a random variable R with measure space ℝ, its _inverse CDF_ (or _quantile function_) is defined by CDF~R~^-1^(q) = inf{r: CDR~R~(x) > 1} for q ∈ [0, 1].
E.g. CDF~R~^-1^(1/2) tells you the x at which CDR(x) equals 1/2.
We call CDF~R~^-1^(1/4) the _first quartile_, CDF~R~^-1^(1/2) the _median_ (or _second quartile_) and CDF~R~^-1^(3/4) the _third quartile_.

_percentile_ is the same as quantile, only that it is in %, that is 100 times larger.

[[PDF]]
Given a continuos random variable R with measure space ℝ, its _probability density function_ (or _PDF_) PDF~R~ (or f~R~) : ℝ ⟶ [0, 1] is a function satisfying:

1) Pr(a ≤ R ≤ b) = ∫~a~^b^PDF~R~(x)·dx for every a ≤ b. +
2) Pr(x) ≥ 0 for all x. +
3) ∫~-∞~^∞^PDF~R~(x)·dx = 1.

Note that according to these rules a PDF, unlike a PMF, can be bigger than 1; it can even be unbounded. See also <<population>>.

[[PMF]]
Given a discrete random variable R with measure space ℝ, its _probability mass function_ (or _PMF_ or _probability function_) PMF~R~ (or f~R~) is defined as PMF~R~(x) = Pr(R = x).  See also <<population>>.

Both the probability density function and the cumulative distribution function capture the same information about the random variable, so take your choice.

PDF~R~(x) = CDFʹ~R~(x) at all points x at which CDF~R~ is differentiable.

CDF~R~(x) = ∫~−∞~^x^PDF~R~(x)·dx.

In sloppy notation, CDF~R~(-∞) = 0 and CDF~R~(∞) = 1.

A _univariate distribution_ is a probability distribution of only one random variable.  A _multivariate distribution_ is the _joint probability distribution_ of two or more random variables.

Two random variables R~1~ and R~2~ are _equal_ if R~1~(ω) = R~2~(ω) for all outcomes ω ∈ S.

Two random variables R1 and R2 are _equal in distribution_ if CDF~R1~(x) = CDF~R2~(x) for all x.
Note that equal in distribution does not imply equal.
E.g. consider X = `number of heads' and Y = `number of tails' in N fair coin tosses.

Two random variables R~1~ and R~2~ are _independent_ iff for all x~1~ ∈ codomain(R~1~), x~2~ ∈ codomain(R~2~), the two events [R~1~ = x~1~] and [R~2~ = 2~1~] are independent.

Random variables R~1~, ..., R~n~ are _mutually independent_ iff for all x~1~, ..., x~n~ the events [R~1~ = x~1~], ..., [R~2~ = x~2~] are mutually independent.
They are _k-way independent_ iff every subset of k of them are mutually independent.

A set of random variables is _independent and identically distributed_ (or _iid_ or __i.i.d.__) if all random variables are mutually indpendent and each random variable has the same probability distribution as the others.

Two events are independent iff their indicator variables are independent.

Let R and S be independent random variables, then f\(R) and g(S) are also independent random variables, where f and g are some functions.

Given a random variable R, then its _expected value_ (or _expectation_ or _mean_ or _average value_ or _first moment_, see also <<population_mean>>), denoted E[R] (or 𝔼\(R) or 𝔼R or μ or μ~R~ or by the use of on overline), is defined by:

E[R] = ∫x·CDFʹ~R~(x) +
If R is discrete: E[R] = ∑x~i~·PMF~R~(x~i~) = ∑~ω∈S~R(ω)·Pr(ω) +
If R is continuous: E[R] = ∫x·PDF~R~(x)

The _conditional expectation_ E[R|A] of a random variable R given event A is E[R|A] = ∑r·Pr(R=r|A).

[[variance]]
Given a random variable R, its _variance_ (or _mean square deviation_, see also <<population_variance>>), denoted by Var[R] (or 𝕍\(R) or 𝕍R or σ² or σ²~R~), is a measure of spread and is defined by

Var[R] = E[(R-E[R])²] = E[R²] - E[R]² = ∫(x-E[R])²CDFʹ~R~(x) +
If R is discrete: Var[R] = (∑x²~i~PMF~R~(x~i~)) - E[R]² +
If R is continuous: Var[R] = (∫x²PDF~R~(x)) - E[R]²

Note that an alternative measure of spread, thought much less often used than variance, is E[|R-E[R]|].

Given a random variable R, its _standard deviation_, denoted σ (or σ~R~ or sd\(R)), is defined by σ = √Var[R].

A set of random variables is called _homoscedastic_ if all of those random variables have the same finite variance.  This is also known as _homoscedasticity_ (or _homogeneity of variance_).  The complementrary notion is called _heteroscedasticity_.

The _covariance_ between two random variables R~1~ and R~2~ is defined as Cov[R~1~, R~2~] = E[(R~1~-E[R~1~])(R~2~-E[R~2~])] = E[R~1~R~2~] - E[R~1~]E[R~2~].

[[correlation]]
_Correlation_ is a statistical relationship between random variables, though in common usage it most often refers to how close two variables are to having a linear relationship with each other. E.g. the relationship between X and Y in regression/classification.

[[pearsons_correlation_coefficient]]
The _Pearson's product moment correlation cofficient_ (or _Pearson's correlation coefficient_ _correlation coefficient_ or simply _correlation_ (but see also <<correlation>>)) between two random variables R~1~ and R~2~ is the standardized covariance and is defined as ρ~R1,R2~[R~1~, R~2~] = Cov[R~1~, R~2~] / (√Var[R~1~]√Var[R~2~]).  Note that the codomain is [-1,1].  Intuitively, it measures how linear the relationship is.  It is 1 for a perfect linear relationship with positive slope, -1 for a perfect linear relationship with negative , and 0 for no relationship at all.

Two random variables R~1~ and R~2~ are said to be _uncorrelated_ if Cov[R~1~, R~2~] = 0.

independent ⇒ uncorrelated

_interaction_ is when the influence of two or more predictors on the response is not additive. E.g. say there are two predictors X1 and X2 and the response Y = f(X1,X2). Imagine the 3D graph/plane.  If a cut through the plane at X1 = some-constant and X2 = some-other-constant doesn't produce two same looking functions (appart from shift), then there's interaction.

If two predictors are highly correlated, it doesn't make sense to add an interaction between them to the model.

E[a·R~1~ + b·R~2~] = a·E[R~1~] + b·E[R~2~] (_linearity of expectation_)

R~1~, ..., R~n~ are mutually independent ⇒ E[∏R~i~] = ∏E[R~i~]

Var[R] = Cov[R, R]

Var[aR+b] = a²Var[R]

Var[R~1~ + R~2~] = Var[R~1~] + Var[R~2~] - 2Cov[R~1~, R~2~]

In general: Var[∑a~i~R~i~] = ∑∑a~i~a~j~Cov(R~i~,R~j~) = (∑a²~i~Var[R~i~]) + 2∑~j~∑~i<j~a~i~a~j~Cov[R~i~, R~j~]

If R~1~, ..., R~n~ are pairwise independent: Var[∑R~i~] = ∑Var[R~i~]

Cov[R, R] = Var[R]

Cov[R~1~, R~2~] = E[R~1~R~2~] - E[R~1~]E[R~2~]

If R~1~ and R~2~ are independent: Cov[R~1~,R~2~] = ρ~R1,R2~ = 0.

_Law of Total Expectation_: Let R be a random variable, and suppose that A~1~, ..., A~n~ is a partition of the sample space S, then E[R] = ∑~i~E[R|A~i~]·Pr(A~i~).

_Mean time to failure_: Given an event E and p = Pr(E), the number of independent experiments until E occures is 1 / p and the variance is (1-p)/p².

_Markov's inequality_: For non-negative R. Pr(R≥a) ≤ E[R] / a.

_Chebyshev's inequality_: Pr(|R-E[R]| ≥ a) ≤ Var[R]/a². Derived from Markov's inequality.

_Pairwise independent sampling_: Let R~1~, ..., R~n~ be pairwise independent random variables with the same mean μ and same deviation σ, and let S be their sum: Pr(|S/n-μ| ≥ x) ≤ 1/n σ²/x².

Given a sequence X~1~, ..., X~n~ of random variables.  X~n~, the last of the sequence, _converges in distribution_ (or _converges weakly_ or _converge in law_) towards the random variable X, denoted X~n~ D→ X (D above the arrow) or X~n~ ⇝ X, if lim~n→∞~ CDF~Xn~(x) = CDF~X~(x) ∀ x ∈ ℝ at which CDF~X~ is continuous.  An estimator is said _asymptotically Normal_ if (θ̂-θ)/se[θ] ⇝ N(0,1).  (*to-do* Is the term "asymptotically" as used in this sense really restricted to "assymptotically normal" and to estimators? I.e. can I say "assymptotically exponential" and most statisticans will feel confortable by such an usage. Def is from all of statistics, p. 92)

Given a sequence X~1~, ..., X~n~ of random variables.  X~n~, the last of the sequence, _converges in probability_ towards the random variable X, denoted X~n~ P→ X (P above the arrow) or plim~n→∞~ X~n~ = X, if for all ε > 0 lim~n→∞~ Pr(|X~n~ - X| > ε) = 0. Convergence in probability implies convergence in distribution.

_Weak Law of Large Numbers_ (or _WLLN_ or _Khintchine's law_): Let X~1~, ..., X~n~ be iid random variables with the same mean μ and same variance σ², and let X̄ = 1/n ∑X~i~ denote their sample mean. WLLN states that X̄ P→ μ. Interpretation: The distributionh of X̄ becomes infinitely concentrated, i.e. 0 variance, around μ as n gets large.  The sample mean is a consistent estimator for the population mean μ.  Note that while E[X̄] = μ and Var[X̄] = σ²/n are also true, they are different statements.

_central limit theorem_ (_CLT_):  Let the random variables X~1~, ..., X~n~ be independent, each X~i~ with some arbitrary unknown distribution but with known mean μ~i~ and finite variance σ²~i~.  Then (∑X~i~ - ∑μ~i~) / √∑σ²~i~ ⇝ N(0, 1), or formulated differently: 1/n ∑X~i~ ⇝ N(μ̄, σ̄²/n)  where μ̄ = 1/n ∑μ~i~ and σ̄² = 1/n ∑σ²~i~.  If additionally X~1~, ..., X~n~ are identically distributed with mean μ and variance σ², this simplifies to X̄ = 1/n ∑X~i~ ⇝ N(μ, σ²/n).  (*to-do* 1) better understand what http://mathworld.wolfram.com/CentralLimitTheorem.html says more 2) relation to `converges in distribution'? See all of statistics p 72 3) How do you call this thing on the lhs of ⇝? 4) Is it correct that I shouldn't use the term sample mean and thus also not the conventional X̄ = 1/n ∑X~i~ in the first general case, since the term sample is reserved for the case of taking a sample from a population, and by the definition, population means that its members have the same distribution. How you call 1/n ∑X~i~ in the first/general case? How you call (∑X~i~ - ∑μ~i~) / √∑σ²~i~ ?)

__WLLN vs CLT__: WLLN gives sample mean's value provided iid Xs.  CLT gives distribution of 1/n ∑X~i~ only provided independent Xs.  (*to-do* But then CLT is a proper superset of WLLN, since knowing the distribution implies knowing the mean. So the question remains, whats the real difference between CLT and WLLN?)

References:

- Book ``All of Statistics'', chapters ``2 Random Variables'' and ``3 Expectation''

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapters "Random Variables" and "Deviation from the Mean"


=== Important distributions


==== Comparison of distributions

*to-do*

References:

- http://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/


==== Uniform distribution

X ~ Uniform(a, b), where a < b, if

PDF(x) = { +
1/(b-a) for x ∈ [a, b]
0 otherise

CDF(x) = { +
0 for x < a +
(x-a)/(b-a) for x ∈ [a, b] +
1 for x > 0

==== Normal distribution / Gaussian distribution

X ~ 𝓝(μ, σ²), where μ∈ℝ is the mean and σ>0 the standard deviation, if

PDF(x) = 1/(σ√(2π)) exp(-1/(2σ²) (x-μ)²)

CDF(x) = Φ((x-μ)/σ)

We say that X has _standard Normal distribution_ if μ=0 and σ=1. Tradition dictates that a standard Normal random variable is denoted by Z.  The PDF and the CDF of Z are denoted by 𝜙(z) and Φ(z) respectively.

Φ(z) = 1/√(2π) ∫~-∞ to x~exp(-t²/2)dt = +
1/2 + 1/2 erf(x/√2)

Where erf(x) = 2/√π ∫~0 to x~exp(-t²)dt

A k-dimensional _multivariate normal distribution_ (or _k-variate normal distribution_) is denoted 𝓝~k~(μ, σ²).

Some useful facts:

X \~ N(μ,σ²) ⇒ (X-μ)/σ ~ N(0,1)

Z \~ N(0,1) ⇒ X = μ + σZ ~ N(μ, σ²)


==== Student's t-distribution

The _Student's t-distribution_ (or _t-distribution_) is the distribution of the sample mean where the population is normally distributed.  It is denoted t~ν~, where ν is its single parameter, the degrees of freedom.  More precisely: Let μ denote the population mean, X̄ the sample mean and S² the unbiased sample variance, then (X̄-μ)/sd̂[X̄] \~ t~n-1~, where sd̂[X̄] = S/√n, see estimator for standard error of the mean, and where t~n-1~ denotes a Student's t-distribution with n-1 degrees of freedom.

*to-do* I think that is not quite correct. It's just one of more possible use cases. After all many other statistics also have a t-distribution, no?

*to-do* list common statistics which follow a t-distribution (e.g. when statistic g1 follows a normal distribution and a scaling parameter depends on the data, e.g estimator sd̂[g1], then, under certain conditions, g2=g1/sd̂[g1] follows a student's distribution)

<<t_statistic>>
The _(Student's) t-statistic_ for an estimator β̂ \~ 𝓝 of unknown parameter β is defined as t~β̂~ = (β̂ - β~0~) / sê[β̂], where β~0~ is a fixed value which may or may not match β.  β̂ must be normally distributed, which in case of OLS is the case if E[epsiolon]=0.  The t-statistic is commonly used in hypothesis testing, where the null hypothesis is that β = β~0~.  Typically β~0~ is 0.  If β̂ is an ordinary least squares estimator for a coefficient in the classical linear regression model, and if the true value of parameter β is equal to β~0~, then t~β̂~ \~ t~n-p~ where n is the number of observations, and p is the number of predictors (including the intercept).

Etymology: the term ``t-statistic'' is abbreviated from ``hypothesis test statistic''.

*to-do* I am confused. Here the denominator is se[β̂], in the t-distribution its sd̂[X̄] (the key point being that the later is an estimator).  Also apparently the Student's t-statstic is not guaranteed to be Student t-distributed, I find that confusing from a terminology point of view. How you call then the statistic used above in the definition of t-distribution?

*to-do* Also in <<t_test>> there multiple examples of t-statistics, all of which have as denominator an estimator, not se[...].  Only when we wanted a t-statistic for a t-test for a estimator β̂ of a OLS model coefficient β, we used t~β̂~ = (β̂ - β~0~) / se(β̂).


==== Chi-squared-distribution (𝜒²)

Given random variables X~1~, ..., X~k~ iid~ 𝓝(0,1), then

∑X~i~² \~ 𝜒~k~²

*to-do* what if X~1~, ..., X~k~ iid~ 𝓝(μ, σ²)?


==== F-distribution / Fisher-Snedecor distribution

A random variable X having a F distribution with parameters d~1~ and d~2~ is denoted X \~ F(d~1~, d~2~).

It is the distribution of X = (U~1~/d~1~) / (U~2~/d~2~), where U~1~ and U~2~ are independent and have distributions 𝜒²(d~1~) and 𝜒²(d~2~) respectively, where 𝜒² denotes the chi-squared distribution.

Or equivalently, it's the distribution of X = ...

*to-do*

Independence of U~1~ and U~2~ might be demonstrated by applying Cochran's theorem.

Applications: Appears often as the distribution of the test statistic in ANOVA.


=== Testing

==== Hypothesis testing, basic idea and algorithm

A _statistical hypothesis test_ is a method of statistical inference.

A _two sided test_ (or _two tailed test_) is concerned with both regions of rejection, of the distribution.  A _one sided test_ (or _one tailed test_) is concerned with the region of rection for only one of the two tails of the distribution, and it states which one it is concerned with.  The researcher has to decide which variant he prefers.  He can do it based on his educated guess what the alternate hypothesis is, and more specifically, what distribution of the alternate hypothesis is.  The goal is to maximize power, given a type I error rate.  For a concrete alternate hypothesis, power could be calculated by simulating: Do multiple times: Simulate data under the alternate hypothesis, calculate p-value, count H~0~ rejecetions (i.e. `H~a~ acceptances'). Over all this delivers power = H~0~-rejection-count / simulation-count. This way one can calculate power for multiple alternate hypothesises.

one sided vs two sided:

pro one sided test: higher power, i.e. less type II error rate.

*to-do* more pros & cons

Hypothesis test algorithm:

- Choose a suitable test statsistic T.  Compute its observed value t~obs~.

- Define the _null hypothesis_ and the complementary _alternate hypothesis_.  The null hypothesis (the hypothesis to be nullified), denoted H~0~, is a statement usually along the lines ``there is no relationship'' or ``there is no effect''.  The complementary alternate hypothesis is denoted H~a~ (or H~1~).  Note that in a one side test, H~0~ should not use =, but ≤ or ≥, while the complementary H~a~ then uses > or < respectively.  However it's mathematically still correct for the H~0~ to use = (*to-do* why is that?)

- Compute the p-value, see definition below.

- Choose a significance level α, see definition below.  Typically the significance level is chosen to be 5% or 1%.

- _Reject H~0~_ iff p-value < α.  Otherwise you _fail to reject H~0~_; you can't accept H~0~, see below.  An equivalent alternative criterion is to reject H~0~ when t~obs~ lies within the critical region, see definition below.

Hypothesis testing really is ``__proof by contradiction__''.  Only that we can't really proof or disprove anything,  since we only work with probabilities.  We only can gather evidence.  We start out assuming H~0~ is true and try to build a contradiction.  If we observe a t~obs~ such that p-value < α, then that is a `contradiction' to our assumption.  It's not a contradiction in a strict sense, but it's evidence that our assumption was incorrect.  In the other case, if p-value > α, we fail to build a contradiction, i.e. we fail to reject H~0~.  However we do not accept H~0~ either.  No conclusion can be drawn if you fail to build a contradiction.  The evidence is insufficient to support any conclussion about either H~0~ or H~a~.  Recall that we optained the p-value by assuming H~0~ is true, so we certainly can't derive from a p-value that H~0~ is true.

The _p-value_ (or _probability value_ or _asymptotic significance_) for a two sided test is Pr(T≥|t~obs~-E[T]| | H~0~), for a one sided test it is Pr(T≥t~obs~|H~0~) or Pr(T≤t~obs~|H~0~) respectively.  The interpretation of the p-value is: _Given_ H~0~ is true, then in (p-value)·100% of any hypothesis tests we see an result as extrem or more extrem (further away from mean) than t~obs~.  I.e. _given_ H~0~ is true, in (p-value)·100% of these tests we would incorrectly reject the null hypothesis.  The p-value is _not_ the probability that either hypothesis is correct.  Regarding the case of a one sided H~a~, where the very unlikely case occures that t~obs~ is of on the `other' side of H~0~'s distribution:  then the p-value will be very large, and we will not reject H~0~, which is correct in that we didn't accept H~a~.

The _significance level_ (or _type I error rate_) α is the probability of rejecting H~0~ given that H~0~ is true. Or in other words, the probability of a false discovery.  Or equavilently, α is the area below the H~0~ distribution in the critical region.  α is choosen by the user, see algorithm above.  Typically we want to control type I error rate, since a false discovery is worse than accidentaly not making a discovery.

The _type II error rate_ β is the probability of not rejecting H~0~ given that H~a~ is true.  Or equivalently, β is the area below the H~a~ distribution in the acceptance region.  Note that the distribution of H~a~ is unknown. β = 1 - power.

The _power_ (or _statistical power_) of a test is the probability of making a true discovery, given that H~a~ is true.  I.e. it is the probability of rejecting H~0~ given that H~a~ is true.  Or equivalently, power equals the area below the distribution of H~a~ in the critical region.  power = 1 - β.

The _critical region_ (or _rejection region_):  In a two sided test the critical region is [-∞,t~crit_a~] ∪ [t~crit_b~,∞],  where the _critical values_ crit_a and crit_b are defined via Pr(T≤t~crit_a~|H~0~) = α/2 and Pr(T≥t~crit_b~|H~0~) = α/2.  Or equivalently via the H~0~ distribution's quantile: t~crit_a~ = H0_dist_quantile(α/2) and t~crit_b~ = H0_dist_quantile(1-α/2).  In a onesided test its [-∞,t~crit~] where Pr(T≤t~crit~|H~0~) = α, or the other way round.  See also definition of significance level.

The _acceptance region_ is the complement to the critical region.

|=====
|                       | H~0~ really true | H~a~ really true
| failed to reject H~0~ | true positive | false negative, type I error, β
| H~0~ rejected         | false postive, type II error, false discovery, significance level α | true negative, true discovery, power
|=====


==== Data snooping / selection effect

[[data_snooping]]
_Data snooping_ (or _data dredging_, _data snooping_, _p-hacking_) is searching patterns in data that then can be presented as statistically significant, without first devising a hypothesis.  The proper way is to first come up with a hypothesis, independently of the test data, and only afterwards test that hypothesis with test data.  Some patterns contained in large amounts of data (especially when number of predictors is huge and the number of observations is moderate) will be only due to chance.  When doing data snooping and actively searching for patterns, we are likely to find patterns, maybe ones that are there only due to chance (e.g. the few values of a predictor happen to correlate with the response by chance).  When then doing a hypothesis test with that same data, the p-value is meaningless, because the hypothesis is based on that data.

[[selection_effect]]
_Selection effect_: Any time we use the data to make a decision (e.g. select a model), we introduce a selection effect (bias). E.g. forward stepwise, lasso etc.


See also <<inference_after_model_selection>>


==== Multiple testing

The problem we're trying to solve here is this: If we make many hypothesis tests, each with significance level α, we're bound to make a false discovery α·100% of the times, because that's what significance level α says.  See also https://xkcd.com/882/ :-).

As in the case of finding the best expectedTestMSE, i.e. the best trade-off between increasing variance and decreasing bias, we now liked to find the best trade-off between inceease in type I error and increase in power.

_Classificaton of multiple hypothesis tests_: Consider m hypothesis tests. The following table defines variables counting how often each case occures. Upper case variables (U V T S and R) are random variables, lower case variables (m and m~0~) are fixed. The number of tests m is known, number of tests m~0~ where H~0~ is really true is unknown, the number of rejected H~0~ R is observable, the others are unobservable.

|=====
|                       | H~0~ really true | H~a~ really true | Total
| failed to reject H~0~ | U                | T                | m-R
| H~0~ rejected         | V                | S                | R
| Total                 | m~0~             | m-m~0~           | m
|=====

Q = V/R is the _false discovery proportion_ (_FDP_). By convention, if V = R = 0, then Q = 0.

The case of that H~0~ is always true, i.e. m = m~0~, is called the _gobal null_ (or _complete null_).

The _False discovery rate_ (_FDR_) is defined as FDR = E[Q] = E[V/R]. I.e. FDR is the expected proportion of type I errors (aka false discoveries) relative to all discoveries.

The _Famility wise error rate_ (_FWER_) is defined as FWER = Pr[V≥1].  I.e. FWER is the probability that we make an type I error (aka false discovery) at all.

δ = per test type I error rate +
FWER ≥ FDR +
FWER = FDR given global null +
FWER = 1 - (1-δ)^m^  given global null and independend tests +
FWER ≈ δm given global null and independend tests and small δ +
δ ≤ FWER ≤ δm

A procedure offers _weak control_ at level α if FWER ≤ α holds is guaranteed only under global null.  A procedure offers _strong control_ at level α if FWER ≤ α holds always.  Note that here α denotes _not_ the same thing as the significance level α of an individual test; here, it's the ``overall significance level''.

Techniques which control FWER: <<bonferroni_correction>>, <<westfall_young>>

Techniques which control FDR: <<bejamini_hochberg>>


===== No correction

*to-do*


[[bonferroni_correction]]
===== Bonferroni correction

Control of the FWER: goal is to get an FWER ≤ α.  Do each of the m individual tests at a significance level δ = α / m. As a result we get FWER ≤ α.

Neutral: Sensible if all tests are independent, because then FWER ≈ δm (assuming global null), see formulas after definition of FWER.

Contra: Can be too conservative (i.e. δ is smaller than needed), especially if the test statistics are positively correlated.  This is because the Boferroni correction assumes the worst case, which is mutually independent tests.  As an extreme example, under perfect positive dependence, there is effectively only one test, and thus we could choose δ = α and still have FWER = α, but instead we `needlessly' did choose δ = α / m.

Contra: As always wenn decrasing the siginificance level α, that comes at the cost of decreased statistical power, or equivalently, at the cost of increasing type II error rate.

*to-do* How much of the above applies to controlling FWER in general, and how much applies to Bonferroni in particular?


[[benjamini_hochberg]]
===== Benjamini Hochberg

Controls FDR.  *to-do*


[[westfall_young]]
===== Westfall Young permutation procedure

Weak control of FWER. Strong control of FWER under some assumptions.  Computes a significance level δ to be used for each test.

*to-do* what are these assumptions?

For all (or some, to save time) permutations allowed under H~0~: Compute p-value for each test, and find the minimum p-value. Overall this gives us an empirical distribution D of the minimal p-values. Compute δ = quantile~D~(α). Use δ as significance level for each of the tests.

This works because: FWER = P(V≥1) = P(p~i~≤δ for some p~i~) = P(min(p~1~, ..., p~m~)≤δ)

*to-do* properly understand why this works; why does the formula for δ work. see my lecture notes.

References:

- Slides7.pdf


==== Misc. test properties / characteristics

_paramtetric test_: Assumes distribution family of the test statistics

_non-parametric test_ (aka _distribution free_): No assumpotions on the distribution of the test statistic.


==== One sample test

_one sample test_: Only one sample, only one test statistic, treat every member of the sample the same way.


==== Unpaired two sample test

_unpaired two sample test_ (or _independent two sample test_): Two samples, e.g. one treated with treatment A and the other with treatment B (which might be `no treatment at all'). More formally, each of the two samples is drawn from another population, and the two populations have potentially different distributions.  Often the test statistic d is the difference or some kind of `difference', often standardized in some way, between the two sample means. The H~0~ is that the two population distributions are equal, which often means d = 0.

Disadvantage:

- The groups need to be really similar.  E.g. by chance the elements in either group might have something in common which has nothing to do with their treatment, but still influences the outcome of the test statistic.

- There might be a big variance in the test static.  E.g. if we measure how long people sleep, after treatment A and after treatment B: there is anyway a rather large variance in how long different people sleep on average (opposed to how long a given person sleeps in a given night).   We don't want that variance to have an influence on our result.  In the paired two sample test, that variance cancels out in the step of building the difference.

Examples:

- parametric unpaired two sample tests: H~0~: X̄~1~ and X̄~2~ are equal

  * <<z_test>> (assumes normal distr. with known variance): z = (X̄~1~ - X̄~2~) / (σ√(1/n~1~ + 1/n~2~)) ~ N(0, 1)

  * <<t_test>> (assumes normal distr. with unknown variance):

    ** equal sample sizes, equal variance: test statistic t = (X̄~1~ - X̄~2~) / (s~p~·√(2/n)) \~ t~2n-2~

    ** equal variance: test statistic t = (X̄~1~ - X̄~2~) / (ŝ~p~√(1/n~1~ + 1/n~2~)) \~ t~n1+n2-2~, where s~p~ denotes the pooled variance.

    ** general: Welch's t-test *to-do*

- non-parametric unpaired two sample tests:

  * <<permutation_test>>

  * <<wilcoxon_rank_sum_test>>


==== Paired two sample test

_paired two sample test_  (or _paired difference test_ or _paired sample test_): Treat every element in the sample with treatment A and with treatment B (again, can be `no treatent at all').  The test statistic is for example the mean of the differences of each pair.

Alternatively, we can match _match_ (or _pair_) every element in the treatment group with an element of the control group, the control group and the matching in a way that the matched pair shares similat observable characteristics.  Matching is however prominently critized.

*to-do* I don't see how the term two sample test still applies here -- the whole point is that its _not_ two samples

**to-do**(5) Are the terms "paired difference test" and "unpaired two sample test" really refering to exactly the same thing?

*to-do* In case of matching, what is then the difference to unpaired two sample test?

Examples: <<wilcoxon_signed_rank_test>>


[[permutation_test]]
==== Permutation test / randomization test

A non-parametric two sample test. General idea: Use permutations of group assignments to destroy the relationship that is to be tested under H~0~ while keeping all other relevant structure.  For each permutation, compute the test statistic, which overall delivers an empirical distribution called _permutation distribution_. Provides type I error control, proof below.

Informal proof for type I error control: When the data does come from H~0~, then the obtained permutation distribution is the distribution of the test statistic under H~0~. This is all we need for type I error control, since we need to control the probability of a false decision under H~0~.

t-test is an approximation to a permutation test.  Permutation tests are known since long, but for a long time we didn't had the computational power to make them feasible, and as a consequence were forced to use approximations like t-test.  Nowadays permutation tests are feasible.

Pro: No parametric assumptions

Pro: Free to use any test statistic

Pro: p-values and type I error control are exact if all permutations are considered. If only a subset of permutations are considered, it's an approximation.

**to-do**(3) Also the lecture scripts list "Paired two sample test / one-sample test for symmetry" as an example (or examples?) for perumatation test.  I don't understand that.

Contra: Computationally expensive

Contra: Not everything can be formulated as permutation test. E.g. in linear regression, there is no straightforward permutation test for individual coefficients.


===== As unpaired two sample test

Given population F~1~ and F~2~, and a sample from each, Y~1~^(1)^, ..., Y~n1~^(1)^ \~ F~1~ and Y~1~^(2)^,...Y~n2~^(2)^ \~ F~2~. H~0~: F~1~ = F~2~ (i.e. treatment has no effect), H~a~ : F~1~ is a shifted version of F~2~ (either in a two tailed or one tailed way).  The test statistic is a function of two samples, measuring some kind of difference between the two samples. For example sum of ranks (ranks with respect to combined sample) of sample1 (i.e. <<wilcoxon_rank_sum_test>> as permutation test), or median(sample1) - median(sample2).

- Compute t~obs~ using the original two samples.

- For all possible permutations (i.e. group/sample assignments) (or, computationally cheaper, repeatedly for a permutation selected uniformely at random from all possible permutations): compute t~i~, where i denotes the i-th permutation.  We can permute since under H~0~ assignment to sampe 1 or sample 2 is irrelevant.

- The set of t~i~ s form the emprical conditional distribution of test statistic T given the data, also calle the _permutation distribution_.

- Compute the p-value using t~obs~ and the obtained permutation distribution.

*to-do* What are properties of a good test statistics?  It seems often to be same sort of difference.  Note that rank sum of group1 is also sort of a difference.  It must be a function where the permutation has no effect under H~0~.

*to-do* add or replace with alternative version where instead an combinedsample we have sample1 and sample2 seperately.

------------------------------------------------------------
  combinedsample <- ... # sample1 concatenate sample2
  n1 <- ... # size of sample1
  repetitioncount <- ... #

  # function underlying test statistic T
  g <- function(combinedsample, n1) { ... }

  g_on_permuted_sample <- function(combinedsample, n1) {
    n <- nrow(combinedsample)
    permutedcombinedsample <- combinedsample[sample(1:n, n, replace=F)]
    return(g(permutedcombinedsample));
  }

  t.obs.all <- replicate(repetitioncount, g_on_permuted_sample(combinedsample, n1))
  t.obs <- g(combinedsample)
  pvalue <- (sum(t.obs.all<=t.obs)+1) / (repetitioncount+1)

  hist(t.obs.all)
  abline(v=t.obs)
------------------------------------------------------------


===== As paired two sample test

Same concept as before. However as in any paired two sample test, we no longer have two populations and thus two samples.  We have one single sample from one population, each element being the difference of a elementpair from sample A and sample B.  The test statistic t is a function on that sample consisting of differences.  A possible concrete test statistic is the mean (of the differences).

Under H~0~, the signs of the observations are random, so we can permute them, which overall delivers the empircal distribution of t.  With that, we can conduct a normal hypothesis test.

------------------------------------------------------------
  g <- function(sample) { ... }

  g_on_permuted_sample <- function(sample) {
    n <- nrow(sample)
    signs <- sample(c(-1,1), n, replace=T)
    sample.new <- signs * sample
    return(g(sample.new))
  }
------------------------------------------------------------


===== Permutationt test example: Correlation

Regression/classification setting. H~0~: no relationship between X and Y. Thus under H~0~, we can permute the Y values (or the X values/rows). As test statistic, we can for example use a rank correlation test statistic, for example Spearman's rank correlation coefficient.


===== Permutationt test example: Correlation / Linear regression

Given Y = β~0~ + β~1~X~1~ + ... + β~p~X~p~ + ε. H~0~: β~0~ = ... = β~p~ = 0.  Thus under H~0~, we can permute the Y values (or the X values/rows).  As test statistic, we can use for example the f-statistic of the linear regression fit.

Example: Exercise series 7, exercise 3


==== Monte Carlo test

We want to make an hypothesis test, but when the distribution of the test statistic is unknown or infeasible to work with, we may can simulate it instead.  For example number of duplicates in a set of n numbers choosen from [m].  We do a number of simulations.  Each simulation chooses n numbers out of [m] and we count the duplicates.  That delivers an empirical distribution, and we can then finaly conduct a hypothesis test using that empirical distribution.


==== Comparison of tests

*to-do* flow chart with all the test: t-test, z-test, Wilcoxon, the Wald, .... Overview with pros and cons. E.g. http://health.uottawa.ca/biomech/courses/apa3381/hyp_test.pdf


[[z_test]]
==== Z-test

A _Z-test_ is any statistical hypothesis test in which the test statistic follows approximately a Normal distribution under the null hypothesis.  Because of the central limit theorem, many test statistics are approximately normally distributed for large samples.

Examples: see those of Student's t-test. Only that in an Z-test, we know the variance σ² of the population, or have a good enough estimator for it, which is often the case for large samples.  So e.g. building on t-test's example of a one sample test, see below, we just would change the test statistic to z = (x̄ - μ~0~) / sd[x̄], which is standard Normal distributed.  Recall that sd[x̄] = σ/√n, see standard error of the mean.


[[t_test]]
==== Student's t-test

A _Student's t-test_ (or simply _t-test_) is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis.

_As one sample test_:  Given one sample with sample mean x̄.  We hypothise that μ~0~ is the population mean and want to test that.  Let μ denote the (true) population mean and S² the unbiased sample variance.  H~0~: μ = μ~0~.  As test statistic we use the t-statistic t = (x̄ - μ~0~) / sd̂[x̄], where sd̂[x̄] = S/√n, see also estimator for standard error of the mean.  Under H~0~ it's distribution is t~n-1~.

_As unpaired two sample test_:  Given two samples of equal size n and equal variance, one treated with treatment A and the other with treatment B (no treatment at all, or different treatment).  We want to test whether treatment A has an effect.  Let s~p~ denote the <<pooled_variance>>, X̄~A~ and X̄~A~ are the sample means.  H~0~: X̄~A~ = X̄~B~.  As test statistic we use the t-statistic t = (X~A~ - X~B~) / s~p~√(2/n).  Under H~0~ it's distribution is t~2n-2~.

_As paired two sample test_:  Given one sample, for each member, we calculate the difference of some test statistic after treatment A and after treatment B (no effect / controll), see also paire two sample test.  We want to test whether treatment A has an effect.  Let n denote the sample size, X~D~ the average of the differences and s²~D~ the variance of the differences.  H~0~: X~D~ = μ~0~ (often 0):  As test statistic we use the t-statistic t = (X~D~ - μ~0~) / sd̂[X~D~], where sd̂[X~D~] = s~D~/√n, see also estimator for standard error of the mean.  Under H~0~ its distsribution is t~n-1~.

_Linear regression_, testing wether a coefficient has an effect: see <<linear_regression_models>>


*to-do* See also Wilcoxon, The Wald test


==== The Wald test

*to-do*


==== F-Test & ANOVA

An F-test is a generic name for a class of statistical tests that share the property that the test-statistic follows an F-distribution (given the null-hypothesis).

One of the most common cases where a test-statistic `ends up' having an F-distribution, is when the ratio between two variances is calculated.

An ANOVA is a specific type of procedure that produces an F-statistic, because it tests the ratio between systematic variance and error-variance.


[[wilcoxon_rank_sum_test]]
==== Wilcoxon rank sum test (or Mann-Whitney U test)

A two sample test using the test statistic U which is the sum of ranks (ranks with respect to the combined sample) in smaple/group 1 (or sample/group 2, doesn't matter), and the null hypothesis H~0~ that the distributions of the two samples are equal.  If the two sample sizes are equal, the distribution of U under H~0~ is known.  For small sample sizes (~20), it's given by tables, for large sample sizes it can be approximated by a Normal distribution.

Don't confuse with <<wilcoxon_signed_rank_test>>.

Pro: No parametric assumptions

Pro: Robust, because the sum of ranks of group 1 statistic is robust.  E.g. if the largest value in a sample gets even larger, the mean would change, but the sum of ranks doesn't.

Pro: Doesn't require the two populations to be normally distributed, which is an advantage over the t-test.

Neutral: Power almost identical to that of t-test if distributions are Normal.

Pro: The null distribution (i.e. U under H~0~) is independent of F~1~ and F~2~.

_As a non-parametric unpaired two sample test_:  Regular hypothesis test. Given population F~1~ and F~2~, and a sample from each.  H~0~: F~1~ = F~2~, H~a~ : F~1~ is a shifted version of F~2~ (either in a two tailed or one tailed way).  Compute u~obs~ from the given sample, and from u~obs~, using the known distribution of U, the p-value.

_As unpaired two sample permutation test_:  An unpaired two sample permuatation test where the test statistic is U.


[[wilcoxon_signed_rank_test]]
==== Wilcoxon signed rank test

Don't confuse with <<wilcoxon_rank_sum_test>>.

Is a non-parametric paired two sample permutation test.  Let X~1~, ..., X~m~ \~ F~X~ and Y~1~, ..., Y~m~ \~ F~Y~ be independent, where (X~i~, Y~i~) is measured on the same subject i. Let D~i~ = X~i~ - Y~i~. The test statistic V is the following.  First remove all D~i~ = 0, resulting in a set of Dʹ~i~.  V = ∑rank~i~·H(Dʹ~i~), where rank~i~ is the rank of |Dʹ~i~| among all |Dʹ~i~|, and H(x) is the heavyside step function (0 for x < 0, 1 for x > 0).

The null hypothesis H~0~: The distribution of the test statistic V is symmetric around a = 0 (or equivalently, F~X~ = F~Y~).

See <<permuatation_test>> how to conduct the test as a whole. In brief: Exercise all possible permutations (or exercise a subset of N of those permutations).  Permuting here means permute (X~i~, Y~i~) for any i (or alternatively, for each D~i~, at random flip sign).  For each permutation, compute the test statistic V~i~.  Overall this delivers a empirical permutation distribution of V.  With this distribution and v~obs~ we can compute the p-value.

Pro: Doesn't require the two populations to be normally distributed, which is an advantage over the t-test.

Sidenote: Under H~0~, the distribution of V is a known distribution, however with no simple expression.  As the sample size increases, it converges to a normal distribution.  Thus we could also conduct a non-permutation test, and use that known distribution instead of the permuatation distribution presented here.


=== Misc statistic

==== Formula table

[cols="1,3"]
|=====
| 𝔉 = { f(x;θ) : θ ∈ Θ }  | Parametric model
| Pr~θ~[·],  E~θ~[·],Var~θ~[·]  | Probability is with respect to PDF/PMF f(x;θ)
| N | Size of population
| n | Size of sample
| μ | Population mean
| μ̂ = x̄ | Common estimator for μ
| σ² | Population variance
| σ̂² = S² | Common estimator for σ²
| x̄ = 1/n ∑x~i~ | Sample mean
| S² = 1/(n-1) ∑(x~i~-x̄)² | Unbiased sample variance
| 1/n ∑(x~i~-x̄)² | Biased sample variance
| s²~p~ = (∑^k^(n~i~-1)s²~i~) / (∑^k^(n~i~-1)) | Pooled variance
| se[·] = sd[·] | Standard error of a statistic = standard deviation that statistic
| SEM = se[x̄] = sd[x̄] = σ / √n | Standard error of the mean, assuming independence and same variance σ²
| SEM̂ = sê[x̄] = S / √n | Common estimator for se[x̄]
| g(X~1~, ..., X~n~) | Statistic: Result of function g on a random sample
| θ̂ or θ̂~n~ | Estimator for quantity θ. Estimator = a statistic plus stating which quantity is estimated.
| Bias~θ~[θ̂] = E~θ~[θ̂] - θ = E~θ~[θ̂ - θ] | Bias of estimator θ̂ with respect to θ
| θ̂ is said to be consistent if θ̂ P→ θ |
| MSE[θ̂] = E~θ~[(θ̂-θ)²] |
| MSE[θ̂] = Bias²~θ~[θ̂] + Var~θ~[θ̂] |
| (X̄-μ) / se[X̄] ~ 𝓝(0,1) | For random variables {X~i~:i∈[n]} iid ~ 𝓝(μ, σ²)
| (X̄-μ) / sê[X̄] \~ t~n-1~ | For random variables {X~i~:i∈[n]} iid ~ 𝓝(μ, σ²)
|=====


==== Population & Sample

[[statistical_parameter]]
A _statistical parameter_ is a numeric characteristic of a population or statistical model.  Typically unkown. Often denoted using Greek letters.

[[population]]
A _(statistical) population_ is the same as the <<PDF>> / <<PMF>>.  So a population can be finite or infinite.  That's my personal definition.  Commonly the population is defined as the set of all possible observations of a random variable.  Personally I find that misleading. At least such an definiton should add ``where different observations having the same value are still different members in the set''.

A _population parameter_ is a specialication of <<statistical_parameter>> describing a numeric characteristic of a population. Often unobservable because the population is to large to evaluate every member.  Prominent examples are population mean μ and population variance σ².

[[population_mean]]
The _population mean_ μ is a population parameter and is the same as the expectation of the corresponding distribution.  A common estimator for the population mean is the sample mean X̄.

[[population_variance]]
The _population variance_ σ² is a population parameter and is the same as the variance of the corresponding distribution.

In general a _sample_ is a `subset' (however elements might be repeated) of a population optained through _sampling_.  Sampling is some process of selecting members of the population, possibly randomly, possibly based on a certain criteria.

A _(simple random) sample_ (_SRS_) is a set of n random variables X~1~, ..., X~n~ iid~ P, where P is some population.  Often a simple random sample is also defined as a subset of the population, drawn uniformly with replacement.  However that important part ``with replacement'' is unfortunately often omitted.

A _statistic_ is a numeric characteristic of a sample, as explained in detail in chapter <<statistic>>.

The _sample mean_ (or _empirical mean_), denoted X̄, of a sample X~1~, ..., X~n~ is the arithmetic mean, as defined below.  Is a statistic, i.e. a random variable.  The sample mean is a consistent estimator for the population mean μ, by the LLN.

X̄ = 1/n ∑X~i~ +
X̄ P→ μ +
E[X̄] = μ +
Var[X̄] = σ²/n

The _unbiased sample variance_ (or _Bessel-corrected sample variance_), denoted S², is definied as follows.  Is a statistic, i.e. a random variable.  Can be used as unbiased estimator for the population variance.

S² = 1/(n-1) ∑(X~i~-X̄)² +
E[S²] = σ²

Similarily, the _biased sample variance_ is defined by 1/n ∑(X~i~-X̄)².  Is a statistic, i.e. a random variable.

[[pooled_variance]]
Given k samples of k populations with common variance σ² and possibly different means.  Let s²~i~ denote the unbiased sample variance of the i-th sample, and n~i~ the size of the i-th sample.  The _pooled variance_ (or _combined variance_ or _composite variance_ or _overall variance_) is the weighed average of the individual unbiased sample variances, weighed by (n~i~-1): s²~p~ = (∑^k^(n~i~-1)s²~i~) / (∑^k^(n~i~-1)).  In the special case of k=2 and n~1~ = n~2~,  s²~p~ = (s²~1~+s²~2~)/2.  The pooled variance s²~p~ can be used as unbiased estimator for the common populaton variance σ².


[[statistic]]
==== Statistic

A _statistic_, often denoted T (or T~n~), is a function, often denoted g, which has a sample X~1~, ..., X~n~ as its domain. Formally: T = g(X~1~, ..., X~n~).  Thus a statistic is a random variable since it depends on the random sample X~1~, ..., X~n~ of the population.  In other words, a statistic is an attribute of a sample.  Unfortunately the term statistic can mean two things.  The term statistic can mean the random variable as described before, in which case it's often denoted uppercase T.  The term statistic can also mean the _observed value_ (or _realized value_) of that random variable, in which case it's often denoted lowercase t (or t~obs~).  Prominent examples are sample mean and (unbiased) sample variance.

The _sampling distribution (of a statistic)_ (or _finite-sample distribution_) is the probability distribution of a given statistic.  Recall that a statistic is a random variable, and thus has a distribution.  If we would take infinitly many same sized samples and calculate the statistic each time, we would get the sampling distribution.

The _standard error_ (or _SE_) of a statistic is defined by the standard deviation of that statistic, i.e. by the standard deviation of its distribution.  Standard error can be used to compute confidence intervals.  The 95% confidence interval for some variable a is approximately mean(a) ± 2SE(a), assuming a is normal distributed. The _68-95-99.7_ rule says that those are the approximate percentage values of the confidence intervals for 1SE(a), 2SE(a) and 3SE(a) respectively.

If the statistic is the mean, the standard error is called the _standard error of the mean_ (_SEM_) and is defined as follows.  However the population variance σ² is seldom known, thus the SEM is often estimated via estimating the population variance σ² by the unbiased sample variance S².

SEM = se[x̄] = sd[x̄] = +
σ / √n (if indpendent and same variance σ²)

SEM̂ = sê[x̄] = S / √n

Proof for sd[x̄] = σ / √n if independent and same variance σ²:  Var[x̄] = Var[1/n ∑x~i~] = 1/n² Var[∑x~i~] =(independent) 1/n² ∑Var[x~i~] =(same variance) 1/n² n Var[x~i~] = Var[x~i~] / n = σ² / n.

*to-do* ISLR p. 65 says that SE can be use to estimate how far off a single μ̂ might be from the true μ. But then the SE doesn't make sense if we calculate it on the basis of the population, since there we know μ exactly. Similarily, why is SE independent of the ratio populationsize:samplesize?

**to-do**(5) What is done in the R script from lecture week 2?


==== Estimator

An _estimator_ (or _point estimator_ or _(point) estimate_), denoted θ̂ (or θ̂~n~), of a parameter θ, is technically a statistic g(X~1~, ..., X~n~) plus conceptually stating which paramater θ its an estimator of.  In other words, an estimator θ̂ is a single ``best guess'' of parameter θ.  An estimator is a random variable since a statistic is one, see there.

The _bias_ of an estimator θ̂ with respect to an unknown parameter θ is defined as Bias~θ~[θ̂] = E~θ~[θ̂] - θ = E~θ~[θ̂ - θ].  An estimator with zero bias is called _unbiased_.  Otherwise the estimator is said to be _biased_.  Note that there's also an analogously defined bias for the estimate f̂ of an regression function f, see there.

An estimator θ̂ with respect to an unknown parameter θ is said to be _consistent_ if: θ̂ P→ θ.

If bias[θ̂]→0 and se[θ̂]→0 as sample size n→∞, then estimator θ̂ is consistent.

[[MSE_of_estimator]]
The _mean squared error_ (or _MSE_) of an estimator θ̂ with respect to an unknown parameter θ is defined as follows. The MSE can be used to assess the quality of the estimator θ̂. Note that there's also an analogously defined MSE for the estimate f̂ of an regression function f, see there.

MSE[θ̂] = E~θ~[(θ̂-θ)²] = +
Bias²~θ~[θ̂] + Var~θ~[θ̂] (see also <<bias_variance_trade_off>>)

Recall that an estimator is a statistic and thus a random variable, so the _mean_ and the _variance_ of an estimator are defined the usual way.


==== Confidence Interval & Prediction Interval

Let C~n~ = (a,b) denote a 1-α _confidence interval_ for an unknown parameter θ, where a and b are statistics, and where 1-α is called the _confidence level_ (or _coverage_ of the interval).  A 1-α confidence interval is an interval such that in (1-α)·100% of the times you make an 1-α confidence interval for some parameter,  possibly each time for another parameter, the interval contains the true parameter.  See next paragraph for further explanations.  Common choices for the confidence level are 95% or 1%.

Note that a 1-α confidence interval does _not_ mean that given a realized interval there is a 1-α probability that it contains the true parameter.  The probability statement is about the interval which is defined by the statistics a and b, i.e. random parameters.  The probability statement is not about the fixed unknown parameter θ.  No probability statement concerning its value may be made.  (*to-do* 1) I don't get the difference.  What's the consequence whether (a,b) are random and θ is fixed or vice versa?  If you are given a 1-α confidence interval and the game is to predict whether it contains the true parameter, what percentage of your bet must the casino give you in order for the game to be fair?  At least in this example, I think it doesn't make a difference.  2) See forumula (6.9) on p. 92 in book "all of statistics". I'd say its _not_ P~θ~, its P~a,b~  3) See also Example 6.14 p. 93 in Book "All of statistics")

Note that confidence intervals are always for things we don't know (the unknown parameter θ), never for unknown things like an estimator θ̂.

_prediction interval_: An estimate of an interval in which a realization of a random variable (in other words, a future observation) will fall with a given probability.  E.g. given X ~ 𝓝(μ, σ²), the 95% prediction interval [μ-1.96σ, μ+1.96σ] for X will contain the next realization of X with a probability of 95%.  Often used for regression, where it's often used for Ŷ.

**to-do**(3) In a prediction interval it's really about the percantage of future observations (Y is a random variable after all) being within the one calculated interval (which however is based on random variables X Y), opposed to percentage of prediction intervals that will cover a value (there's no single true value, as said, Y is a random variable), right?


==== Statistical Model

A _statistical model_ 𝔉 is a set of distributions or regression functions (*to-do* but regression functions are quite a different thing than distributions; I don't understand). A _parametric model_ is set 𝔉 that can be parameterized by a finite number of parameters: 𝔉 = { f(x;θ) : θ ∈ Θ}, where θ is an _parameter_, or vector of parameters, that can take values in the _parameter space_ Θ. f is a function of x, parameterized by θ.

*to-do* clean up relation to <<statistical_parameter>>.

There's an loose distinction between parameters determined during fitting the model and _hyper-parameters_ which are determined before fitting the model, e.g by the user or during the higher level process of model selection.  You may think of splitting complete set of parameters into two subsets.  The values of the subset labeled parameters is computable cheaply when being provided with the values of the subset labeled hyper-parameters. References: https://stats.stackexchange.com/questions/149098/what-do-we-mean-by-hyperparameters?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa (*to-do* isn't a further difference that hyper-paremeters can influence the number of parameters, such as in polynomial regression?)

A _tuning parameter_  if the parameter's job is primarily a transient parameter of the learning algorithm.  Tuning parameters are also called hyper-parameters, conflicting somewhat the previous definition.  (*to-do* 1) But in this sence, a hyper 2) Clean up that parameter - hyper-parameter - tuning parameter mess)

The notations Pr~θ~[·],  E~θ~[·] and Var~θ~[·] mean that the probability is with respect to PDF/PMF f(x;θ), i.e. averaging over all possible observations x, the generating PDF/PMF being f(x;θ). (*to-do* in the context of an estimator θ̂, what if θ is not a parameter of a model, but some other population parameter)

p denotes the number of predictors and n the number of data points.  Predictors and data points will be defined shortly.  Given is a n ⨯ p matrix X called _design matrix_ (or _model matrix_ or _regsessor matrix_).  Each column of X represents a _predictor_ (or _covariable_ or _covariate_ or _input variable_ or _independent variable_ or _feature_ or _regressor_ or _explanatory variables_ just _variable_).  Given is a n ⨯ 1 vector Y of _response variables_ (or just _reponse_ or _output variables_ or _dependent varables_).  The tuple (Y[i], X[i-th row]) represents the i-th _observation_ (or _data point_).  We assume that there is some fixed but unknown relationship between the response Y and the predictors X.  We model that by the _regression function_ f (or _population regression function_ or _PRF_) by writing Y = f(X) + ε. This can be read as ``__is modeled as__'' or Y _is regressed_ on X.  f represents the _systematic_ information that the predictors provide about the response.  Ŷ is the resulting _prediction_ for Y.  The elements ŷ~i~ of Ŷ are called _fitted values_ (or _predicted values_).

[[error_terms]]
ε is a n ⨯ 1 vector of a _error terms_ (or _noise_ or _disturbance_), which are independent of X.  Each error term is an unobservable random variable.  It is a catch-all for all we miss with our model f.  The true relationship might not according to model f, there might be other variables that cause variation in Y that we didn't measure, and there may be measurement error....  If the model f is the correct model, then these error terms are random and have no systematic error (i.e. E[ε~i~] = 0 ∀ ε~i~).  We liked to have an estimate f̂ for f and use it like so Ŷ = f̂(X).

*to-do* better merge the above paragraph with the first few paragraphs of this chapter

In general, we can use regression only for prediction of a response variable given new predictors.  In general the observiations on which the regression is based do not allow for conclusions about causal relations. (*to-do* Some reference to a trusted source which concisely accurately states this)

The variance Var(ε~i~) of the error terms ε~i~ is in general not known.  Often it is assumed that all error terms have the same constant variance σ²,  and that constant variance often is estimated via σ̂ = RSE.  Note that the error terms are in direction of the y axis, as opposed to perpendicular to a linear regression hyperplane.  This is important to note because the later is what most humans intuitively do in the 2D case when guessing which of multiple regression lines is a better fit.

[[residual]]
e~i~ = y~i~ - ŷ~i~ is the i-th _residual_.

_Studentized residual_ (or _standardized residual_) t~i~ = e~i~ / sê[e~i~]. Can be used to dedect outliers, see there.

See more statistics and definitions in <<measuring_model_performance>>.

[[trainingsampe_testsample_notation]]
Notation: In Pr~training~[·], E~training~[·], Var~training~[·], Bias~training~[·] etc. the sample space is the set of all possible training samples taken from the population.  Each training sample trains the estimate f̂.  Thus f̂, or more specically its estimated coefficients β̂, are random variables with a sample space as described before.  In Pr~test~[·], E~test~[·], Var~test~[·], Bias~test~[·] etc., the sample space is the set of all possible test samples taken from the population.

|=====
| n | Number of samples
| p | Number of predictors
| X | Predictors. Given by being part of the given data set.
| Y | Responses. Given by being part of the given data set.
| ε | Error terms. Unknown.
| Often: Var[ε] = const = σ² | σ is in general not known. Often assumed to be constant.
| Often: Var̂[ε] = σ̂² = RSE² | Common estimator
| f | Regression function. Is guessed.
| Y ≈ f(X) | ``Approximately modeled as'' or ``X is regressed on Y''.
| Y = f(X) + ε |
| f̂ | Estimate for f
| Ŷ = f̂(X) | Predictions (or fitted values)
| e = Y - Ŷ | Residuals
| t~i~ = e / sê[e] | Studentized (or standardized) residuals. For sê[e] see your specific model.
|=====

References:

- Statisitic Cheat Sheet: http://web.mit.edu/~csvoss/Public/usabo/stats_handout.pdf


=== Model selection, model validation

[[measuring_model_performance]]
==== Measuring model performance

===== Formula table

|=====
| t~i~ > 3 | Rule of thumb for identifying outliners
| p~ii~ > 2p̄ or p~ii~ > 3p̄  | Rule of thumb for identifying high-leverage data points, where p~ii~ is a diagonal cell in the projection matrix P and p̄=p/n is the mean leverage value.
| D~i~ = 1/p · t²~i~ · (P~ii~/(1-P~ii~)) = 1/(pσ̂²) · ∑~j~(ŷ~j~-ŷ~j(i)~)² | Cook's distance of i-th observation.  P is the projection matrix.  ŷ~j(i)~ excludes the i-th row.
| D~i~ > 1 or D~i~ > 4/n | Rules of thumb for identifying influencial data points.
| TSS = ∑(y~i~ - ȳ)² | Total sum of squares. ȳ is the sample mean, see there.
| RSS = ∑e²~i~ | Residual sum of squares
| RSE = √(RSS/(n-p)) | Residual standard error
| Var̂[ε~i~] = RSE | Common estimator for Var[ε~i~] = σ
| (unadjusted) R² = (TSS - RSS) / TSS = 1 - RSS/TSS |
| adjusted R² = 1 - (RSS/(n-p)) / (TSS/(n-1)) |
| trainingMSE = RSS/n |
|=====


===== Model performance measures

The _performance_ of a model is a measure of how `good' a model models a given population, most often in respect to its predictive power, i.e. its prediction capability on independent unseen test data.  For example, one could use the expected test MSE, that is, an estimate thereof.  (*to-do* is it always a property of the model, or can it also be a property of a concrete already trained estimate f̂? In what way does the difference between prediction and inference influence the topic of model performance)

Let e denote <<residual>>s.

The _residual sum of squares_ (_RSS_) (or _error sum of squares_) is defined as RSS = |Y-Ŷ|² = |e|² = ∑~1≤i≤n~e²~i~.  Can be thought of as the amount of variability that is left unexplained after performing the regression.  Is a training error.  When LS fit was used, decreases monotonically as more predictors are added to the model (current model is optimal with respect to RSS; if an predictor is added that cannot reduce the RSS, then the respective coefficent is set to zero).  n-p degrees of freedem are associated with RSS.

**to-do**(3) If the true model is linear, and the training set is infinitely large, then β̂ = β and e = ε.  I.e. altough e is non-zero, it is fully explained.  So we should say (RSS-|ε|²) is the amount of variability that is left unexplained after performing the regression?

The _explained sum of squares_ (_ESS_ or _SSE_) (or _model sum of squares_ or _sum of quares due to regression_ (_SSR_)) is defined as ESS = |Ŷ-Ȳ|².  ESS can be thought of as the amount of variability in the response that is explained by performing the regression.  Is a training error.  p-1 degree of freedeom are associated with ESS.

The _total sum of squares_ (or _TSS_ or _SST_) is defined as TSS = |Y-Ȳ|² = RSS + ESS = ∑~1≤i≤n~(y~i~-ȳ)².  Can be thought of as the amount of variability inherent in the response before the regression is performed.  Is a training error.  n-1 degrees of freedom are associated with TSS.

The _residual standard error_ (or _RSE_) is given by RSE = √(RSS/(n-p)).  The RSE is considered an absolute measure of the lack of fit of the model to the data.  Roughly speaking RSE is the average amount that the response will deviate from the true regression hyperplane.  Even if the model were absolutely correct and the parameters of the model were known exactly, any prediction Ŷ is still off by RSE.  RSE is often used as an estimator for the variance Var[ε~i~] ≈ σ̂² = RSE² of the error terms ε~i~.  Is a training error.

*to-do* derive given formula from √Var[e] or whatever is the basis, and then write RSE = √Var[e] = √(RSS/(n-p)).

The __R²__ (or _coefficient of determination_) statistic is defined as R² = (TSS - RSS) / TSS = 1 - RSS/TSS = ESS / TSS.  R² measures the proportion of variability in the response that can be explained by our model.  R² ∈ [0,1], 1 meaning good, 0 meaning bad.  Small values might occur becaus the used model (e.g. linear) is wrong or the inherent error σ² is high, or both.  The advantage of R² over RSE is that the former is relative (lies in [0,1]) and the later is absolute.  Is a training error.  Increases monotonically as more predictors are added, see also RSS.

The __adjusted R²__ statistic is defined as 1 - (RSS/(n-p)) / (TSS/(n-1)).  Adjusted R² is not as well motivated in statistical theory as AIC / BIC / C~p~, but is popular in practice.  Increases only when non-noise predictor is added.  Intuition:  Look at the nominator (RSS/(n-p)):  Adding noise predictors will lead to only a very small decrease in RSS, but the larger p will make the whole nominator smaller, ... *to-do*. i.e. adjus  Is an adjusted training error. *to-do* but an estimate for test error

**to-do**(3) I still don't get the difference between adjusted R² and R². What are pros / cons?

For model fitted with LS, __Mallows's C~p~__ is an estimate of test MSE, and thus an adjusted training error, and defined as C~p~ = 1/n (RSS + 2·p·σ̂²). C~p~ is unbiased if σ̂ is unbiased.  Intuition: 2·p·σ̂² is a penalty for the fact that training error tends to underestimate the test error.

_Bayesian Information Criterion_ (_BIC_) is BIC = 1/n (RSS + log(n)·p·σ̂²).  Is an adjusted training error.

_Akaike Information Criterion_ (_AIC_) is AIC = 1/σ̂² C~p~.

All of Mallow's C~p~, AIC, BIC are asymptotically (for large n) good in the sense that the best model out of a set will have the highest Mallow's C~p~ from all models, the highest AIC from all models etc.

**to-do**(3) Adjusted training error vs estimated expected test error. When can I reasonably use adusted training error? E.g. in best subset, when can I use adjusted training error opposed to having to pay CV?

Recall: The notation E~training~[·], Var~training~[·], E~test~[·], Var~test~[·] etc. is defined by <<trainingsampe_testsample_notation>>.

The __bias of estimate f̂__ with respect to regression function f __at x~0~__ is defined analogous to the bias for an estimator θ̂, see there. f̂(x~0~) is a random variable since f̂, i.e. its parameters, depends on the random training data.

Bias~training~[f̂(x~0~)] = E~training~[f̂(x~0~)] - f(x~0~)

Bias~training,test~[f̂] = E~test~[Bias~training~[f̂(x~test~)]]

*to-do* how is variance of f̂ formally defined ?

Var~training~[f̂(x~0~)] = ...?

*to-do* how is Var~training~[f̂(x~0~)] defined? In general, for OLS linear regression?

Var~training,test~[f̂] = E~test~[Var~training~[f̂(x~test~)]

*to-do* confidence interval for f̂

A _loss function_ L(Y, Ŷ) measures the error between Y and Ŷ = f̂(X). Typical choices are squared error (Y - Ŷ)² or absolute error |Y - Ŷ|.

The _training error_ Err~training~ of a model is the average loss over the training sample.  Can theoretically be used as estimator for the expected test error, but it generally would be a rather bad one since rather biased, see also *to-do*.

Err~training~ = 1/|trainingsample| ∑L(y~training,i~, f̂(x~training,i~))

The _test error_ (or _generalization error_) is the expected loss of a concrete trained model on a test sample (aka unseen data).  It is defined as follows.  f̂ is fixed and was trained by some training sample.  The stated two variants are equivalent.  In the 1st variant the randomness lies in repeatedly randomly picking a test sample from the population.  y~test,i~ and x~test,i~ correspond to the i-th observation in that random sample set and thus are fixed values.  In the 2nd variant the randomness lies in X and Y being random variables.

Err~test~ = +
1/|testsample| ∑L(y~test,i~, f̂(x~test,i~)) or +
E~test~[L(Y, f̂(X))|f̂]

[[expected_test_error]]
The _expected test error_ (or _expected prediction error_) is the expected loss of a model `template' (i.e. not yet trained), when it is trained multiple times and each trained model is tested.  For a concrete example see <<expected_test_mse>>.  In respect to test error, now also the training sample is choosen at random from the population.

Err = E~training~[Err~test~]

Regarding `the' _MSE_, there are multiple variants, depending on what exactly we want to describe.  The basic idea is always the same, but depending on the specific MSE variant, it is calculated over different data and the used estimate f̂ is either fixed or varies by repeteadly training it with some data. See also MSE for an estimator θ̂, which is analogously defined.

The _training MSE_ is calculated using the training data and a fixed estimate f̂ which was trained using that training data:

trainingMSE = RSS/n **to-do**(3) or does denomitor subtract 1 or 2?

[[expected_test_mse]]
The _expected test MSE_ is conceptually calculated in two levels. One level iterates over all possible training data sets, each iteration training a new estimate f̂.  For each of those f̂, the other level iterates over all possible test data sets. See also <<expected_test_error>> for the more general concept.

expectedTestMSE = E~training~[E~test~[(y~test~ - f̂(x~test~))²]] = +
E~test~[expectedTestMSE(x~test~)]

The __expected test MSE at x~0~__ is analogous, but here we only look at a fixed x~0~.  In the following formula, the inner E[·] is for the random variable Y~0~.   Note that the Y~0~ corresponding to x~0~ is a random variable due to the error term ε.

expectedTestMSE(x~0~) = E~training~[E[(Y~0~ - f̂(x~0~))²]]

[[bias_variance_trade_off]]
The _bias-variance trade-off_ (or _bias-variance dilemma_) states that that the expected test MSE at x~0~ can always be decomposed into three parts as as follows.  One important point is that we can not get rid off the irreducable error Var[ε].  Recall that we saw the same pattern also with an estimator.  It is a general principle in statistics that when bias decreases, then variance must increase and vice versa which is not directly captured by the following formula.  As model flexibily increases, bias decreases, variance increases, and the expectedTestMSE will be convex, i.e. have a U-shape.  Thus the goal is to find the model with minimal expectedTestMSE.  When we say ``optimizing the bias-variance trade-off'', we really mean finding the minimal expected test MSE.

expectedTestMSE(x~0~) = (Bias~training~[f̂(x~0~)])² + Var~training~[f̂(x~0~)] + Var[ε]

Yes: If Var goes down, bias must go up, and vice verca. This is a principle, not a proof, in the context of our lecture.

*to-do*: Is all of the above about MSE & bias truly acurate? Be picky! E.g. I suspect I use non-standard / unusual notation.


[[assess_model_coefficients]]
===== Assess model coefficients

The _bias of model parameter estimator β̂_ is defined the usual way the bias of an estimator is defined: Bias~training~[β̂] = E~training~[β̂] - β

_t-statstic_ for an estimator β̂ of unknown parameter β: see <<t_statistic>>.

In a linear regression model the coefficents β̂ found by OLS are <<BLUE>>.


===== Assess data points

An _outlier_ is a data point for which its response y~i~ is unusual by being far from the value predicted by the model. Alternatively: A data point with large studentized residual.  Observations whose studentized residuals are greater than 3 in absolute value are possible outliers [ISLR chapter 3.3.3 Potential Problems, Section 4. Outliers].  In linear regression, typically an outlier has only a small influence on the regression hyper-plane.  However it may have a big influence on RSE and R².  And since RSE is used as estimator for σ, also a big influence on confidence intervals and p-values, i.e. a big influence on the interpretation of such a fit (*to-do* are those statements restrictued to linear regression?).

A data point with high _leverage_ is one for which its predictor is unusual by being far away from the mean of the predictors.  Regarding linear regression, given projection matrix P, the leverages are defined as diag(P). Recall that P (also denoted H) only depends on the predictors X, and that Ŷ = PY, i.e. ŷ~i~ = p~i1~y~1~ + ... + p~ii~y~i~ + ... + p~in~y~n~.  You see from this formula that the leverage p~ii~ quantifies the influence the response y~i~ has on its predicted value ŷ~i~.  When having a high-leverage data point, the lack of neighboring predictors means that the fitted regression model will pass close to that particular observation.  As a rule of thumb, a leverage value greater than 2p̄ (other authors say 3p̄) is considered large, where p̄=p/n is the mean leverage value.

*to-do* I only understand that for simple linear regression, but not confidentaly for multiple linear regression. Is figuratively `far away' the Eucledian distance in ℝ^p^?. ISLR has an example: Figgure 3.13, middle plot, p.98. Has the red predictor higher leverage than the predictors at the right/left border of the ellipse? Its closer in Eucledian distance to the center/mean.

*to-do* The notion of outlier seems to be applicable not only to linear regression, but the notion of leverage seems only to be applicable to linear regression. Correct? Why this asymetry? If not, what is the general definition / formula for leverage?

*to-do* There seem to be two intuitions for leverage: "a measure how far of a predictor is from the predictor mean" and "a measure of the influence of a response on its predicted value", and I cant bring them together in my head

**to-do**(4) I don't understand how high leverage by itself is a problem.  If I have high leverage but a tiny outlineingless, at least in linear rergession with OLS nothing bad at all happens, no?

[[influencial_data_point]]
An _influencial data point_ is one whose deletion would noticeably change the calculation. In particular, in regression analysis it has a large effect on the parameter estimates. In other words, a measure of how influencial a data point is, is a measure of the effect of deleting that data point.  One possible measure is the <<cooks_distance>>.  Note that outliers and high-leverage data points have the potential to be influencial, but they not necessarily are influencial. For models with two parameters, a possible way to visually identify influencial data points is to do n `experiments', each removing the i-th data point and then fit the model using the remaining data points, and then draw a scatter plot of the two optained parameters of each `experiment'  (e.g. β~0~ on the x axis and β~1~ on the y axis).  All points should be close together.  References: https://onlinecourses.science.psu.edu/stat501/node/337.

*to-do* If an data point both is an outlier and has high-leverage, is it guaranteed to be influencial or only very likely to be influencial? According to cooks distance, it is guaranteed to be influencial, no?

[[cooks_distance]]
The _Cook's distance_ D~i~ is a commonly used estimate of the <<influencial_data_point,influence>> of the i-th data point when performing least-squares regression analysis.  In an OLS analysis it can also be used to indicate regions of the design space where it would be good to obtain more data points.  The Cook's distance is defined as D~i~ = 1/p · t²~i~ · (P~ii~/(1-P~ii~)) = 1/(pσ̂²) · ∑~j~(ŷ~j~-ŷ~j(i)~)², where t~i~ is the i-th studentized residual, and ŷ~j(i)~ excludes the i-th row.  If the `outlineniness' (middle term t²~i~) is high and the leverage (last term) is large then the Cook's distance is large and thus the data point is deemed influencial.  Thresholds for identifying highly influential data points are controversal.  One is D~i~ > 1, another is D~i~ > 4/n. References: https://onlinecourses.science.psu.edu/stat501/node/340

*to-do* I still don't understand the summation definition. Why is the nominator not mostly zero?

*to-do* Has a given concrete value of the Cook's distance an interpretation, or is it just qualitative, large is bad, small is good? In the later case, why square studentized residual and why not simply use leverage P~ii~).


[[model_selection]]
==== Model selection / feature selection

_Model selection_ (or _model tuning_) is the task of selecting a statistical model from a set of candiate models.  That may include determining the hyper-parameters of the choosen model and it may include determining the tuning parameters of the learning algorithm.

*to-do* clean up this terminolgy mess

Whether or not model selection shall additionally also train the selected model is not clearly defined (*to-do* really?)

Model selection is typically done by computing the estimated expected test error of a candidate model on _validation data_.  Validation data is the same as test data, but in the context of model selection instead model validation.

_Feature selection_ (or _variable selection_, _attribute selection_, _variable subset selection_) is the task of selecting a subset of the available predictors (aka features).

[[inference_after_model_selection]]
===== Inference after model/feature selection

Say we do best subset selection, and then look at the p-values of the resulting coefficent estimates.  These p-values will be rather low, even if the data is random noise.  After all, the task of the algorithm was to select the best predictors, so obviously their p-values are among the best possible given the data.  The algorithm did data snooping. See also selection effect.

**to-do**(2) Isn't the real cause of the problem that we have not enough data per predictor, such that we can't distinguish between true signal (the true model/curve) and noise (variance of error terms)?  Then by chance, for one axis, all data points really are close to some slope, opposed to randomly around the plane at zero.  In Rcode11.R, if I increase n, the problem (finding significant coefficients) goes away. See also similar next question.

**to-do**(2) See exercise series 9, 2b. Under global null, the distribution of the p-value for a given coefficient, say for x~1~, is uniform. I am missing the intuitive explanation. Intuitively I would say if sample size n is much larger than p, it should be much much more unlikely that a p-value is significant. See above question.

Analogy: Take a sample from a normal distribution. Take the max of the sample. Compute the p-value of the max (with respect to the original normal distribution). Of course it will be rather small, probably less than 0.05, i.e. significant.  What happened was kind of hidden multiple testing: testing the max is like testing every observation, because we need to look at all obvservations to find the max.  When doing this whole thing multiple times, and then drawing the empirical distribution of all these maxes, it will be a bell-shaped curved clearly shifted to the right relative to the normal distribution it is based upon.

**to-do**(2) Is this also data snooping. if so, interlink or combine the two

_sample splitting_: One simple way to deal with the problem is to split the data.  With one part do the feature selection, and with the other do the fit and measurement of p-values for coefficient estimates.  These p-values will be `honest'.  Disadvantages are reproducability issues due to the random split, and loss of power since we only use half of the training data (see also validation set approach).


[[best_subset_selection]]
===== Best subset selection

Basic idea: Try all 2^p^ different models.

1. For k = 0, ..., p: Try all C(p k) ways of choosing k from p predictors.  Take the best of these models, denoted M~k~.  In LS regression models, best can e.g. be according to smallest RSS, or equivalently largest R².

2. Choose the best model among M~0~, ..., M~p~.  Do that via either of cross validation, validation set approach or adjusting the training error of step 1.

**to-do**(4) Why use RSS or R² in the innermost loop? why not exptectedTestMSE or so? I suspect due to computational complexity. But say I don't mind. Other reasons?

**to-do**(4) Why break problem of choosing among 2^p^ models into these two stages. Why not simply cross validation on 2^p^ models. _only_ because it would be computational expensive?

Pro: Simple

Con: Computational expensive. Not feasible for larger p (say 30-40).

Con: When p gets larger, the search space gets larger,  and thus there's the risk of overfitting and high variance of the coefficient estimates.

See also <<best_subset_selection_as_shrinkage>>

References:

- Book ``An introduction to statistical learning'', chapter ``6.1 Subset Selection''


===== Forward stepwise selection

A greedy approach.  As in best subset selection, but we iteratively improve the best model so far by greedely adding the best next predictor.  So in step 1 (see best subset selection), in iteration k, we have model M~k-1~ in our hand and try out p-k predictors.  The best predictor we add, resulting in model M~k~.

Instead 2^p^, we now only try out 1 + ∑~0≤k≤p-1~(p-k) = 1 + p(p+1)/2 models.

Forward stepwise selection tends to do well in practice.  However its not guaranteed to find the best model.  For instance, suppose that in a given data set with p = 3 predictors, the best possible one-variable model contains X~1~, and the best possible two-variable model instead contains X~2~ and X~3~.  Then forward stepwise selection fails because in the first iteration it chooses X~1~, and can't undo that decision in the later iterations.


===== Backward stepwise selection

As forward stepwise selection, only that we start out with the full model, and iteratively remove the least useful predictor.

Con: Can't be used when n < p, because we need to start with the full model

*to-do* but we still can start with the biggest possible model.  I would still think that one would call this approach backward stepwise selection, no?


===== Hybrid approaches

First greedely add predictors with the forward stepwise selection method, then remove predictors that no longer provide and improvement with the backward stepwise selection method.


[[model_validation]]
==== Model validation

_Model validation_ (or _model assessment_ or _assessing performance (of a model)_) is the task of calculating the performance of a final model.  Final model means one whose hyper-parameter are already determined, e.g. by model selection.  Note that it's about a model, as opposed to a single given already trained estimate f̂.  For a meaningfull model validation we usually also need to calculate the bias and the variance of the calculated performence.

The problem of model validation is that for most ways to perfectly calculate a performance we often would need all possible test data to train multiple estimates f̂ and for each of those f̂ all possible test data.  See also definition of expected test MSE.

.Use previously unseen data for model validation or model selection

This is not realistic.  Even so, in that case we could combine the original data and the newly available data into one data set, and be logically at the same point as in the beginning of the problem statement.

.Use original data for model validation or model selection

When we use the original data for model assessment, the retreived performance will be biased. That's because we trained the model with exactly the same data as we measured the model's performance with.  It was the model trainings job to fit the model to the original data, so obviously the model will have a high performance on the original data.

When we use the original data for model selection, then (**to-do** Claude said that overfitting occures - but the term overfitting applies only to a single model training, no?).  The procedure would be: Calculate the performance for each of the m candidate models using the complete original data and then choose the candidate model which had the best performance.


[[validation_set_approach]]
===== Validation set approach

The _validation set approach_ is a technique for model assessment or model selection.

Partition data randomly in two equaly sized partitions, one constituting the training data and the other constituting the test data (or validation data).  Train an estimate f̂ using the training data, and then calculate an estimate of the expected test MSE using the test data.

Pro: Simple

Pro: Fair estimate of test MSE, i.e. closer to test error than to (adjusted) training error.

Contra: Loss of power: Fewer training data always always means a worse fit of the model.  In particular it typically means more bias.  In other words, it's too pessimistic: we get a biased (too high) estimate of the test error.

Contra: Large variance of the validation estimate (e.g. estimated expected test MSE) because the validation estimate might depend a lot on how the partition turned out to be.

References:

- Book ``An introduction to statistical learning'', chapter ``5.1.1 The Validation Set Approach''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4.3.1 Leave-one-out cross-validation''


[[cross_validation]]
===== Cross Validation

_Cross-validation_ (or _rotation estimation_) is a coarse technique for model assessment or model selection.  It's a coarse technique in the sense that it has multiple more concrete instanciations, such as <<LOOCV>> and <<k_fold_CV>>.  It tries to mitigate the problem of only having one data set (aka sample of the population) of finite size, altough we actually would like infinitely many training sets and test sets, each set of infinite size.

_CV for model validation_: Repeatedly partition the original data set into a training set and a test set, each time doing the partition in a different way.  In each iteration, calculate an estimate of the performance of the model using the training set and the test set of that iteration. At the end, average the results, delivering the (final) estimate of the model's performance.

*to-do* but then we didn't really measure a concrete trained model. It's more a measure of the way we train, not a measure of a concrete trained model.

mind that we estimate expected test mse, for the true one we need the population

we are a bit sloppy and take the number of CV for both the model and the estimate f

_CV for model selection_: Delivers best model (in sense of template, not trained), but you don't know yet how good it is on unseen data.  Analogous to CV for model validation, but do it for each candidate model.  I.e. there are two nested loops, an outer doing different partitions, and and an inner trying all candidate models (that is three nested loops in case of double cross validation).  At the end, we select the candidate model with the best averaged estimate of the performance.  As noted in chapter model selection, whether or not the selected model shall also be trained is not clearly defined.  If we want to train, we can train it on the complete original data set, or just take the already trained model which was trained on a subset.  The former is computationally more expensive, but larger training sets are generally better.  Note that we cannot use the optained performance estimates also for model validation, see also <<double_cross_validation>>.

*to-do* define terms overfitting, underfitting.

*to-do* Chapter/paragraph about choosing a statistic for model performance. e.g. why exactly is training MSE not good.

*to-do* compare and contrasts these terms: (statistical) learning method, model, estimate f̂ of f, systematic information f. More usually used terms, especially for f̂ and f?

*to-do* Is all of the this chapter correct, inclusive following subchapters? Be picky!

See also <<oob_error_estimation>>, which computes an estimate for the test error for bagged trees.

References:

- Book ``An introduction to statistical learning'', chapter ``5.1 Cross-Validation''

- Book ``All of statistics'', chapter ``13.6 Model Selection''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4 Cross-Validation''


[[double_cross_validation]]
===== Double cross validation

_Double cross validation_ (or _nested cross validation_) delivers you expected estimated test error of the following procedure (i.e. how good the model resulting from the procedure performs on unseen data). The procedure is to use cross validation to select the best model.

Note that we cannot do model selection with cross validation and then use the same data set for model assessment, because the assessment would test a model with data that the model already has seen.  We would thus get a biased, i.e. too optimistic, i.e. too small estimated expected test error.

Outer cross validation: `Model' assessment: Repeatedly partition the original data set into a test set and a training-valiation set.  In each of these outer iterations, pass the training-valiation set to the the inner CV which returns a trained model.  Calculate the estimated test error of that trained model using the test set.  At the end, averaging all those delivers the estimated expected test error of the precedure.  Recall that each trained model the outer layer sees was trained with slightly different training data than every other, thus overall we get this E~training,test~[...] the estimated expected test error demands.

Inner cross validation: Model selection: Do normal model selection via cross validation on the training-validation set received from the outer CV.  Return the trained selected model received from model selection.

Note that different inner cross validations may select different models.  That's ok, since as said at the beginning of this chapter, we asses the performance of the procedure, not of some model.


[[LOOCV]]
===== Leave one out cross validation (LOOCV)

A specialization of cross validation.  Do n iterations. In each iteration, make the i-th observation the test set, and the rest the training set. Let f̂^(-i)^ denote the estimate of the model fitted with training data being the original data without the i-th observation.

expectedTestMSÊ = 1/n ∑(y~i~ - f̂^(-i)^(x~i~))²

In linear regression, that simplifies to

expectedTestMSÊ = 1/n ∑(e~i~/(1-diag~i~(P)))², e = residuals, P = projection matrix

Pro: Much less bias than validation set approach due to larger training set

Pro: No randomization

Cons: Computationally expensive, especially for large n, since we have to fit n different models.  However for some models, e.g. for linear regression, there are short cuts.

References:

- Book ``An introduction to statistical learning'', chapter ``5.1.2 Leave-One-Out Cross-Validation''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4.3.1 Leave-one-out cross-validation''


[[k_fold_CV]]
===== k-fold cross validation

A specialization of cross validation.  Split observations randomly in k equaly sized partitions.

References:

- Book ``An introduction to statistical learning'', chapter ``5.1.3 k-Fold Cross-Validation''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4.3.2 K-fold Cross-Validation''


[[model_training]]
=== Model training

_Estimating f by paramtetric methods_: We assume f to be a parameterized function.  The parameters β of f are called _coefficients_ (or _unknown parameters_).  The problem of estimating f reducues now to computing estimators β̂ for the coefficients β.  For example the linear model has f̂(X) = Xβ, where β is a p ⨯ 1 vector constituting the unknown parameters.  We use the training data to _fit_ (or _train_ or _estimate_) our model, i.e. to compute estimates β̂ of the unknown parameters β.  Phrased as noun, this is called _model fitting_ (or _model training_ or _model estimation_).  There are multiple approaches for fitting the model, the most common approach being (ordinary) <<least_squares>>.


[[least_squares]]
==== Least squares regression

_Least squares_ (or _LS_) is a method of fitting a model which tries to minimize the RSS.  In other words, it is a method for computing the estimators β̂ of the model coefficients β.  Let β̂^LS^ (or β̂, since LS is kind of the standard) denote the estimator, as defined by LS, of the coefficients β of the model:

β̂^LS^ = argmin~β~(RSS(β)) , using given training data

There are two categories: _Ordinary least squares_ (or _OLS_ or _linear least squares_) and _non-linear least squares_.  Linear least squares has a closed-form solution, see <<linear_regression_models>>.  The nonlinear problem is usually solved by iterativie refinement.

[[BLUE]]
_Gauss-Markov theorem_: In a linear regression model with uncorreleated error terms ε, constant finite variance Var[ε~i~] = σ² and E[ε] = 0, the coefficents β̂ found by OLS are _BLUE_ (_best linear unbiased estimator_).  Here, `best' means Var~training~[β̂] is minimized compared to other unbiased estimators, and unbiased means Bias~training~[β̂] = 0.

*to-do* But that only works when the population regression function f (i.e. the true model) is linear, right? YES. Else also the linear function based on the coefficient β has bias. What is the appropriate terminology to phrase all this?

*to-do* From BLUE follows that Bias~training~[f̂] = 0? NO. That is, only if I choose all the predictors I should pick. I.e. expectedTestMSE = (Bias~training~[f̂])² + Var~training data~[f̂] + Var[ε]?

Contra: Var[β̂^LS^] is high in case of <<multicollinearity>>. <<shrinkage_method>>s try to solve this problem.


[[shrinkage_method]]
==== Shrinkage Methods / Regularization Methods

_Shrinkage methods_ (or _regularaization methods_) try to solve the problem of <<multicollinearity>>.  Altough ordenary least squares coefficient estimates are <<BLUE>> (i.e. unbiased and low variance), in case of correlated predictors the variance of a coefficent estimator might still be unacceptably high.  Shrinkage methods now trade lower variance for (hopefully only) a small increase in bias.  Also, least squares has problems wit large p or even p>n.  Shrinkage methods don't suffer from that as much.

_standardize variables_: Ridge and Lasso are not scale invariant, which least squares is. So we should standardize variables to have variance 1 beforehand (which is done by default in R's glmnet): xʹ~ij~ = x~ij~·(1/n·√(x~ij~-x̄~j~))^-1/2^.

_intercept is excluded from shrinkage_: β^(0)^ denotes β without intercept β~0~.  The rational for excluding the intercept β~0~ from being regularized is that we want to shrink the association of each predictor with the response.  The intercept is just a measure of the mean value of the response when predictors X = 0.

[[multicollinearity]]
_Multicollinearity_ describes the situation that a predictor can be linearly predicted from one or more other predictors.  In this situation, when using OLS, Var~training~[β̂] is hight, i.e. there's a higher chance of the estimator β̂ being far away from the real β.  Recall that in OLS, the estimators β̂ are unbiased.  Consider a model with two predictors X~1~ ≈ X~2~.  Then f̂(x) = β̂~0~ + β̂~1~X~1~ + β̂~2~X~2~ ≈ β̂~0~ + β̂~3~X~1~, where β̂~3~ = β̂~1~ + β̂~2~. If there was only one predictor as in the rhs of ≈, we could calculate β̂~3~ well. But in the model with two predictors, OLS doesn't know how to distribute β̂~3~ among β̂~1~ and β̂~2~ such that β̂~1~ + β̂~2~ = β̂~3~.  E.g in one training data sample, β̂~1~ will be large and β̂~2~ will be small, in the other training data sample the other way round, in yet another training data sample it's evenly distributed etc.  <<shrinkage_method>>s try to solve this problem.

References:

- Book ``An introduction to statistical learning'', chapter ``6.2 Shrinkage Methods''


===== Comparision

The criterion are convex in β for Ridge and Lasso (see geometrical interpretation explained in Ridge). That is computationally convenient. Given a set of λ (opposed to one λ), we still can compute the optimal solution β̂ in one go.

Best subsetset selection is computationally expensive. It is non-convex.  Lasso is in a sense the best convex relaxation of best subset selection.  The found solution, since there are less constraints, won't be the same, i.e. is an approximation, but we hope it's close enough.


[[ridge_regression]]
===== Ridge regression

_Ridge regression_ (or _Tikhonov regularization_ or _weight decay_) is a <<shrinkage_method>>. λ≥0 is a tuning parameter.

β̂^ridge^ = argmin~β~(RSS(β) + λ|β^(0)^|²)

β̂^ridge^ = argmin~β~(RSS(β)) subject to ∑~1≤i≤p~β²~i~≤s

Where |β^(0)^| denotes the L2-Norm (aka Eucledian distance) of β excluding the intercept β~0~, i.e. |β^(0)^|² = ∑~1≤i≤p~β²~i~.

_Geometric interpretation_: For the case of 2 predictors, i.e. also two coefficients β~1~ and β~2~: Imagine a 2D coordinate system, the x axis represents possible β~1~ values, the y axis possible β~2~ values.  Each point in the plane represents a possible β, or a possible estimate β̂.  The point (β̂^LS^~1~, β̂^LS^~2~) represents the LS estimate β̂^LS^, which minimizes RSS.  Around that point we can draw contour lines, each contour line representing another, higher, RSS.  Also, we draw a circle around center (0,0) with a diameter of √s, where s can be derived from λ and vice versa.  The geometrical interpretation of the shrinkage penalty term is that the estimate β̂ must lie within the circle.  Assuming β̂^LS^ lies outside the circle, the point where the first contour line touches the circle is the estimate β̂ found by Ridge.  β = E[β̂^LS^] is the point representing the true coefficient β. Around it we can draw ellipses with semi-axes n·Var[β̂^LS^], n={1,2,...}, represinting the area where {68%, 95%, ...} of the β̂^LS^ will lie.  Assuming β is outside the shrinkage penalty circle, the β̂^Ridge^ will only be on a comperatively small part on the shrinkage penalty circle, i.e. Var[β̂^Ridge^] is lower than Var[β̂^LS^]. However we also see that E[β̂^Ridge^] will no be away from β.

The elements of β̂^Ridge^ generally won't be zero, which is in contrast to β̂^Lasso^.  That's because the first contour line touching the shrinkage penalty circle probably doesn't do that at a point which exactly lies on one or more axes.

*to-do* what is the benefit of ridge / lasso? After all it's just yet another parametered model, and as always, due to bias variance tradeoff, the more flexible the model the less bias but the more variance.

The second term, λ|β|², is also called _shrinkage penalty_.  Its effect is that of _shrinking_ the coefficient estimates towards zero.  The tuning parameter λ serves to control the amount of shrinkage.  λ=0 means no shrinkage, resulting in β̂^ridge^ = β̂^LS^.  λ→∞ means infinite shrinkage, resulting in β̂^ridge^ = 0.

References:

- Book ``An introduction to statistical learning'', chapter ``6.2.1 Ridge Regression''


[[lasso_regression]]
===== Lasso regression

A <<shrinkage_method>>.  See also subchapter <<ridge_expression>>, where common topics are discussed.  λ≥0 is a tuning parameter.

β̂^lasso^ = argmin~β~(RSS(β) + λ‖β^(0)^‖~1~)

β̂^lasso^ = argmin~β~(RSS(β)) subject to ∑~1≤i≤p~|β~i~|≤s

Where ‖β^(0)^‖~1~ denotes the L1-Norm of β excluding the intercept β~0~, i.e. ‖β^(0)^‖~1~ = ∑~1≤i≤p~|β~i~|.

The elements of β̂^Lasso^ often are exactly 0, especially for high λ.  Look at the respective paragraph of <<ridge_regression>>.  Now in Lasso, the geometrical interpretation of the shrinkage penalty is a square rotated 45 deg, the corners on the axes.  Now it is more likely that the first contour line touches the square at an corner than at an edge.  Since the corner lies on an axis, β̂^ridge^ is 0 with respect to the other axis.

References:

- Book ``An introduction to statistical learning'', chapter ``6.2.2 The Lasso''


[[best_subset_selection_as_shrinkage]]
===== Best subset selection viewed as shrinkage

<<best_subset_selection>> can also be viewed as shrinkage method.  Especially in the 2nd variant of the definition we see that it selects not more than s predictors.

β̂^subset^ = argmin~β~(RSS(β) + λ‖β^(0)^‖~0~)

β̂^subset^ = argmin~β~(RSS(β)) subject to ∑~1≤i≤p~1~Bi≠0~≤s

Where ‖β^(0)^‖~0~ denotes the L0-Norm of β excluding the intercept β~0~, i.e. ‖β^(0)^‖~0~ = ∑~1≤i≤p~1~Bi≠0~.

The elements of β̂^subset^ often are exactly 0, especially for high λ.  Look at the respective paragraph of <<ridge_regression>>.  Now in best subset selection, the geometrical interpretation of the shrinkage penalty is a line on each axis from 0 to s. ...

*to-do* In the geometrical interpretation, the contour lines will almost surely either touch the tip of one line (making this β̂~i~ equal to s), or less likely some line (making this β̂~i~ somewhere between 0 and s). I don't see how it can be (apart from a tiny probability) that multiple predictors are selected.  I don't see at all how a β̂~i~ can be larger than s -- but im sure in best subset selelection when s predictors are choosen, the β̂~i~ can be larger than s.


[[bagging]]
==== Bootstrap aggregation (bagging)

_Bagging_ is a general method for reducing the variance of a statisticial learning method. It's particularly useful and frequently used in the context of <<decision_trees>>.

Recall that averaging a set of observations reduces variance.  The central limit theorem says that given independent X~1~, ..., X~n~, each with variance σ², the variance of the mean X̄ is σ²/n. The idea is to train B models, each model on its own training set of size n.  The final model is just a wrapper around these B models: it delegates the call to the B submodels, and returns the averaged result.  Thus we need B training sets of size n each.  Since we only have access to one training set, we generate B bootstrap training sets based on the original training set.

_bagging and decision trees_: We train B decision trees.  The trees are grown deep, and are not pruned, thus they have high variance but low bias.  Averaging the results of the trees reduces the variance.  Note that the final model is not a tree anymore, i.e. interpretition drops.  However the trees might be correlated. Averaging correlated quantities does not lead to an as large decrease in variance as we would like. <<random_forests>> try to tackle this problem.

_bagging for classification via majority voting_:  Each of the B models/trees `votes' for one of the K categories.  The categegorie with the most votes wins, i.e. is what is returned by the overall model.  A hint why this works, without giving a formal proof:  If the trees/models, that is classifiers in general, were independent, and each classifier has a missprediction rate of less than 1/2, K=2, the majority vote approaches a perfect classifier (i.e. classifies always correctly) for B→∞.  In reality however the trees are not independent (but see also <<random_forests>>) because all bootstrap samples are derived from one original sample.  Also, if each classifier has a missprediction rate of more than 1/2, the majority vote approaches a perfecty bad qualifier.  Take away: Bagging a good qualifier can improve performance, bagging a bad classifier can degrate performance.

_bagging does not do anything for linear predictors_: Thus bagging should be used for non-linear estimators, e.g. trees.

**to-do**(2) Bagging does nothing for linear predictors: Why does averaging not reduce variance of the mean anymore? The CLT is universal; where is the catch?

[[oob_error_estimation]]
_Out-of_bag (OOB) error estimation_ / _OOB MSE_:  Is an alternative to cross validation to estimate the test error, which is computationally less demanding.  It can be shown that each bootstrap sample on average uses about 2/3 of the observations of the originial sample. The non-used observations are called _out-of-bag observations_.  Given the i-th observation, for every tree in which the i-th observation is OOB, i.e. the observation did not particpate in training the tree, we make a prediction using that tree. This yields about B/3 predictions.  For regression, we average, for classification, we a take majority vote, yielding a single OOB prediction for the i-th observation.  Over all n observations this yields n OOB predictions.  We use them to compute the OOB error, which is an estimate of the test error.  It can be shown that for large B, OOB error is virtually equivalent to LOOCV error.

*to-do* CV estimates the expected test error, but OOB error estimation estimates the test error, so they are not really compareable?

Bagging of decision trees reduces variance, at the cost of lower interpretability. For more pros & cons, see <<comparison_of_tree_based_methods>>.

References:

- ISLR, chapter ``8.2.1 Bagging''


=== Models

==== Comparison of models

Ridge and Lasso are modifications of the linear model.  They make the linear model less flexible, in orer to reduce variance, at the cost of hopefully a small bias.

Nonlinear models like polynomial regression, step functions, regression splines, smoothing splines, local regression, generalized additive models (GAM) increase the flexibility to reduce bias, at the cost of hopefully only a small incease in variance.  Recall however, that when the true model is linar, then there's no bias already.


[[linear_regression_models]]
==== Linear regression models

In the linear multiple regression model the relation between the response Y and the predictors, i.e. the design matrix X, is linear, i.e. f(X) = Xβ.  Thus the estimated model is f̂(X) = Xβ̂ and thus Ŷ = f̂(X) + ε. Often the first column of X is all ones, which then makes β~0~ the _intercept_: the hyperplane described by f̂ intercepts the y-axis at β~0~.  _Simple linear regression_ is linear regression with only one predictor variable. f(X) = Xβ defines the _population regression hyperplane_, which is the best linear approximation to the true relationship between X and Y (recall that linearity was just an assumption). f̂(X) = Xβ̂ defines the _least square hyperplane_, which is an estimate based on the training data (*to-do* Is there are more general term which does not imply the fitting method).

The method to fit the model is commonly ordinary <<least_squares>>.  Other methods include <<ridge>> and <<lasso>>.

Assumptions of the linear model:

- E[ε] = 0, i.e. no systemtatic error, i.e. the linear model is the true model. Recall that we have Y = f(X) + ε and that the error terms ε are a catch-all for all we miss with our model, here the linear model f(X) = Xβ. If the true model is not linear, then when using f(X) = Xβ, E[ε] won't be 0.

  * As a consequence, we have E[β̂] = β (i.e. unbiased estimator β̂ of coefficients).

  * If this assumption does not hold, we need other models than the linar model.

  * Check with Tukey-Anscombe Plot

- Predictors X are exact, i.e. observed without erors. **to-do**(3) Isn't that per definition catched by the error terms ε?

  * If this assumption does not hold, we may can use Errors-in-variables models.

- Cov[ε] = σ²I~n×n~, i.e. the errors are uncorrelated and have constant variance. That implies homoscedasticity of the error terms ε.

  * As a consequence we have Cov[β̂] = σ²(X^T^X)^-1^, and from that Var[β̂] = diag(Cov̂[β̂])

  * If the error terms ε are not homoscedastic we may can use weighted least squares.

  * If the error terms ε are correlated we may can use generalized least squares.

  * Check with Tukey-Anscombe Plot

- Error terms ε are jointly normally distributed. I.e. together with the above: {ε~i~|∀i} iid \~ 𝓝(0, σ²).  (*to-do* Since here we say they are independent, doesn't that imply they are also uncorrelated. I.e. the previous point is actually implied by this point? Ok, they are only uncorrelated given they are independent if second moments are finite (sais wikipedia in `uncorrelated random variables'), but is that really the point here?)

  * As a consequence, we have
    ** β̂ \~ 𝓝~p~(E[β̂], Var[β̂])  [which we require for individual t-tests / p-values]
    ** Ŷ \~ 𝓝~n~(E[Ŷ], Var[Ŷ])
    ** e \~ 𝓝~n~(0, Var[e])
    ** σ̂² = RSE² ~ σ²/(n-p) 𝜒²(n-p)

  * If this assumption does not hold, we can rely on the CLT which implies that for large sample size n, the above distributions are still approximately true.  However we may should use robust methods (*to-do* which ones?) instead least squares.

- The matrix has full rank p < n.

  * If that is not fulfilled, then we simply cannot solve the equations delivering β.

*to-do* Is it in the context of this lecture good enough to know these assumptions and their concequences as a whole and also only for linear regression fitted with least squares? If only some asumptions are fulfilled but not others, I find it pretty hairy to properly understand which consequences / formulas still hold wand which doesn't.  Also I never really know which assumptions are relative to linear regression model and which are relative to the least squares fitting method.

_Projection matrix_ (or _influence matrix_ or _hat matrix_), denoted P (or H), *to-do*

_categorial (or qualitative) predictors_: Given a categorial predictor X~j~ representing l classes {class0, ..., class(l-1)). Make (l-1) dummy variables X~j|0~, ..., X~j|l-1~. X~j|k~ is 1 if X~j~ equals classk, and 0 otherwise. Instead the value pair (0, 1) we could also use other non-equal values like e.g. (-1, 1).  The class with no dummy variable, here denoted class0, is called the baseline.

_Pros/cons of adding more predictors_: pro) Better fit to the training data. con) overfitting. pro) lower bias. con) higher variance. con) lower interpretability. Rational for higher variance: the individual variabilities for each coefficient sum up and the variability of the estimated hyper-plane increases the more predictors are entered into the model, whether they are relevant or not. See also bias-variance trade-off. See also <<curse_of_dimensionality>>

[[notes_on_correlation]]
_Notes on predictor correlation_: If predictors are correlated in some way, but especially when they they are linearly correlated (e.g. measureable with <<pearsons_correlation_coefficient>>), then the variance and the p-value of the respective coefficient estimators β̂~i~ increase.  Imagine a linear model with two predictors, say there's the strong correlation X~1~ = 2X~2~, and say the true relation is Y = X~1~.  With some training data, LS migh deliver β̂~1~ = 1 and β̂~2~ = 0, with other training data β̂~1~ = 0 and β̂~2~ = 1/2, and with yet other training data anything inbetween.  Thus the variance and thus the p-value of β̂~1~ and β̂~2~ will be high.  You can also imagine a 3D plot, in the X~1~-X~2~ plane the observations are on a line, and the points in the 3D space are also almost on a line, and there's no stable way to lay a plane through a line.  When the predictors are correlated but not linearly correlated, there may or may not be a negative effect, but only when they are uncorrelated we are sure that we don't suffer from the mentioned negative effects.  See also <<interpretation_of_t_test>>.

[[notes_on_orthogonal_X_matrix]]
_Notes on orthogonal X matrix_:  An orthogonal X matrix directly implies uncorrelated predictors.  It can be shown that an orthogonal X implies that Cov[β̂] is diagonal matrix, i.e. the coefficient estimators β̂ are uncorrelated.  This in turn implies that β̂~j~ is identical to a β̃~j~ of a model using X~j~ as sole predictor, i.e. it can be shown that both have the same formula.  Followingly the t-test / p-value for the two are identical.

[[interpretation_of_t_test]]
_Interpretation of a t-test / p-value_ of a coeficient estimate β̂~j~:  Recall that from the assumptions it follows that the coefficient estimates β̂ are normally distributed.  Thus as always, the t-statistic for β̂~j~ is (β̂~j~ - β~j~)/sê[β̂~j~] \~ t~n-p~.  Note that sê[β̂~j~] depends on β̂, i.e. on the full model, because sê[β̂~j~] = ... = √diag(RSE²(X^T^X)^-1^),  where RSE depends on Ŷ and Ŷ in turn depends on β̂.  In a t-test for β̂~j~, the null hypothesis H~0~ is in general β~j~ = 0  (or in other words H~0~: y \~ -X~j~, H~a~: y \~ .).  A t-test for β̂~j~ quantifies the effect of the predictor X~j~ after having subtracted the linear effect of all other predictor variables: Y - ∑~0≤i≤p;i≠j~β̂~i~X~i-th col~ = β̂~j~X~j~.  Note that when looking at these individual t-tests, besides the multiple testing problem in general, it can happen that all individual t-tests show insignificant p-values, altough it is true that some predictor variables have a significant effect.  This occures if predictors are correlated, see <<notes_on_correlation>> paragraph.  That at least some predictors have an effect can be quantified with a F-test. If p-value of F-test is not significant, then you can't trust p-values of coefficients.   Uncorrelated predictors implies independent tests, and independent tests are nice to interpret.

_F-test / ANOVA_: The null hypothesis is that all β~i~ are zero (aka global null).  In other words, full model is compared against empty model.  Recall that ESS has p-1 d.o.f. and RSS has n-p d.o.f.  If p-value of F-test is not significant, then you can't trust the p-values of the coefficient estimates. In R, one way to calculate the p-value of the global F-test would be anova(fit.empty, fit.all), where fit.empty = lm(response \~ 1, ...) and fit.all = lm(response ~ predictor1 + ...).

H~0~: β~0~ = ... = β~p~ = 0 +
H~0~: F = (ESS/(p-1)) / (RSS/(n-p)) \~ F~p-1,n-p~

A _partial F-test_ compares a reduced model where q predictors are left out (i.e. their coefficents are zero) to the full model.  WLOG we assume their indicies are (p-q+1, ..., p).  The partial F-test can be interpreted as the partial effect of adding these q predictors to the model.  The p-value of a partial F-test leaving out one predictor equals the p-value of that predictor.

H~0~: β~p-q+1~ = ... = β~p~ = 0 +
H~0~: F = \((RSSʹ-RSS)/(pʹ-1)) / (RSS/(n-p)) \~ F~pʹ-1,n-p~

**to-do**(1) I found both statements "The F-statisic of leaving out a given predictor equals the squared t-statistic of that predictor" and "The p-value of a partial F-test leaving out one predictor equals the p-value of that predictor". Don't they contradict?

*to-do* Relation to <<multicollinearity>>

Var̂[β̂] = σ̂² / (Var[X]·(1-R²)). Thus Var̂[β̂] increases when variance σ̂² of error terms increases, and it decreases when n or Var[X] increase.

_choosing which predictors to add_ / _pros/cons of adding more predictors_: see <<model_selection,feature selection>>

_one multiple linear regression vs multiple single linear regressions_: Linear regression on a single predictor variable yields the same respective coefficient estimate β̂~i~ as the multiple linear regression only if the predictor variables are orthogonal, see also <<notes_on_correlation>>.

_Tukey-Anscombe plot_ (_residuals vs fitted_): Scatterplot of residuals e (on y-axis) versus fitted values Ŷ (on the x-axis).  Ideally points should be randomly around horizontal line through zero (due to assumpution E[ε] = 0), and their vertical spread should be constant (due to assumption of homoscedasticity). If the trendline shows a trend, there is some evidence that the linear model assumption is not correct.  If the standard deviation grows linearly with the fitted values, the log-transform Y → log(Y) stabilizes the variance *to-do* what is the overall picture of this log-transformation?

_Q-Q (quantile-quantile) plot_: Scatterplot of empirical quantiles of the residuals (on the y-axis) versus the theoretical quantiles of a N(0, 1) distribution (on the x-axis).  Ideally the points should be on a 45 deg line.  Deviations from that mean the residuals are not normally distributed. Q-Q plot is nice to verify that a distribution is normal distributed. See also https://data.library.virginia.edu/understanding-q-q-plots/

Reference for these diagnostic plots: https://data.library.virginia.edu/diagnostic-plots/

*to-do* I don't understand that plot. What does it mean to have something theoretical (i.e. not directly related to our actual data) on an axis?

**to-do**(2) What do I see on the Q-Q plot that I don't see (as easily) on the Turey-Anscombe plot?

|=====
| P = X(X^T^X)^-1^X^T^; p~ij~ = Cov[ŷ~i~,y~j~]/Var[y~j~] | Projection matrix *to-do* formula for p~ij~ using only x's. That would help to see what it mathematically means for a predictor to be far away from the mean of predictors.
| P^T^ = P | I.e. P is symmetric
| P² = P | I.e. P is idempotent
| PX = X | X is invariant under P
| diag(P) | Leverages of observations
| ∑p~ii~ = p |
| p~ii~ ∈ [1/n,1] |
| Y = f(X) = Xβ + ε | β and ε unknown, see however assumptions on ε.
| Ŷ = Xβ̂ = PY  |
| E[Ŷ] = Xβ |
| Cov[Ŷ] = σ²P |
| x~0~^T^β̂ ± σ̂√(x~0~^T^(X^T^X)^-1^x~0~)·qt~n-p;1-α/2~ | (1-α)100% confidence interval for E[y~0~], given new observation x~0~.
| x~0~^T^β̂ ± σ̂√(1+x~0~^T^(X^T^X)^-1^x~0~)·qt~n-p;1-α/2~ | (1-α)100% prediction interval for y~0~
| E[ε] = 0 | By assumption
| Var[ε] = σ²  | By assumption
| Var̂[ε] = σ̂² = RSE² ~ σ²/(n-p) 𝜒²(n-p) | *to-do* 𝜒² is the distribution of RSE², as opposed to σ̂², since σ̂² is an estimator and could also be estimated using other statistics, right?
| e = Y - Ŷ = (I~n~ - P)Y | Residuals
| E[e] = 0 |
| Cov[e] = σ²(I~n~ - P) | Residuals e are correlated (error terms ε are not)
| Var[e] = diag(Cov[e]) = σ²(1-diag(P)) | Variance of residuals is _not_ constant (unlike variance of error terms ε).
| sê[e] = se[e] = √Var[e] | (*to-do* is this absolutely correct? why/proofs?)
| e / sê[e] = e / (σ√(1-diag(P))) | Studentized (or standardized) residual
| OLS: β̂ = argmin~β~(RSS(β)) = (X^T^X)^-1^X^T^Y | Coefficient estimates, computed using OLS. Note that this forumula is handy for theoretical purposes.  For numerical computatioin QR decomposition is mutch more stable. https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Least_squares_estimator_for_.CE.B2[Proof]
| E[β̂] = β |
| Cov[β̂] = σ²(X^T^X)^-1^ | Note that in some notations Var instead of Cov is used; since it's a matrix, it's implicit that the covariance matrix is meant. https://stats.stackexchange.com/questions/68151/how-to-derive-variance-covariance-matrix-of-coefficients-in-linear-r
| Cov̂[β̂] = σ̂²(X^T^X)^-1^ |
| Var[β̂] = diag(Cov[β̂]) | Usual definition
| Var̂[β̂] = diag(Cov̂[β̂]) = σ̂² / (Var[X]·(1-R²)) |
| sê[β̂] = √Var̂[β̂] | Usual definition
| β̂ \~ 𝓝~p~(E[β̂], Var[β̂]) |
| (β̂~j~ - β~j~)/se[β̂~j~] ~ 𝓝(0, 1) | See distribution of β̂.
| t-value of β̂~j~: (β̂~j~ - β~j~)/sê[β̂~j~] \~ t~n-p~ | Commonly we set β~j~ = 0. See also distribution of β̂, t-statistic & t-distribution.
| p-value of β̂~j~: pseudo R-code: 2*pt(abs(t.value.beta), df=n-p, lower=FALSE) |
| β̂~j~ ± sê[β̂~j~]·qt~n-p;1-α/2~ | (1-α)100% confidence interval for β~j~. qt~n-p;1-α/2~ denotes the 1-α/2 quantile of a t~n-p~ distribution.
| F = [(TSS-RSS)/(p+1)] / [RSS/(n-p)]  | F-statistic / ANOVA. The null hypothesis is β~i~ = 0 for all i except 0.  F-statistic is 1 if there is no relationship between X and Y, larger otherwise. Under H~0~ F~p-1,n-p~ distributed.
|=====

*to-do* How to *correctly* interpret what the p-value of β̂~j~ really means. Not in the general sense, but the concrete context of linear regression. See script, box at bottom of page 10.

*to-do* write out diag of Cov[β̂] and P explicitely

References:

- https://www.stat.berkeley.edu/~aditya/resources/LectureSEVEN.pdf

- http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf


[[basic_functions]]
==== Basic functions

We only look at univariate models, i.e. one predictor X.  We have a family of k fixed known functions b~j~(X), j ∈ [k].  This transformation delivers the new linear model, where i denotes the data point index:

y~i~ = ∑~0≤j≤k~β~j~b~j~(x~i~) + ε~i~

It is a regular linear model with k predictors b~j~(X), j ∈ [k].  Hence the same rules apply, we can use the same fitting methods such as least squares, and the same inference tools are available.

*to-do* is it really customary to use k instead p to denote the number of functions?

Examples:

- <<polynomial_regression>>

- <<step_functions>>

- <<regression_splines>>

References:

- ISLR, chapter ``7.3 Basic functions''


[[polynomial_regression]]
==== Polynomial regression

Special case of the <<basic_functions>> approach.  We only look at univariate models, i.e. one predictor X.  We replace the standard linear model Y = Xβ + ε with a polynomial in X of degree d resulting in a new linear model

Y = P~d~(X) + ε = β~0~ + β~1~X + β~2~X^2^ + ... β~d~X^d^ + ε

In practice, it is unuasal to use d greater than 3 or 4, because of overfitting, i.e. the polynomial can take strange shapes, especially at the left and right boundary.

As always, its beneficial if predictors are uncorrelated, i.e. if the matrix X is orthogonal, see also <<notes_on_orthogonal_X_matrix>>.  In R, poly(...) returns an orthogonal matrix.

Pro: easy to fit via LS

Pro: all the properties of linear regression still hold (since technically it still is linear regression)

contra: instable at boundaries

References:

- ISLR, chapter ``7.1 Polynomial Regression''


[[step_functions]]
==== Step functions

Special case of the <<basic_functions>> approach.  Can also be seen as special case of piecewise polynomials in that the polynomials have degree 0.  We only look at univariate models, i.e. one predictor X.  We cut the X axis at k cut points c~1~, ..., c~k~ into k+1 bins / regions. In the j-th bin (starting at 0), we use the constant functions β~j~.  This amounts to conoverting the continous variable X into an ordered categorial variable.  The model then is as follows. Note that we don't use C~0~(x), since that is redundant to intercept β~0~.  i denotes the data point index.

y~i~ = β~0~ + ∑~1≤j≤k~β~j~C~j~(x~i~) + ε~i~

Where C~j~(·), sometimes called dummy variables, are defined as follows. I(·) denotes the indicator function.

C~j~(x) = I(c~j~ ≤ x < c~j+1~) ∀ j ∈ [1,k-1] +
C~k~(x) = I(c~k~ ≤ x)

β~0~ can be interpreted as the mean Y value for X < c~1~.

References:

- ISLR, chapter ``7.2 Step Functions''


[[regression_splines]]
==== Regression splines

===== Piecewise polynomials / Splines

We only look at univariate models, i.e. one predictor X.  A spline is a piecewise polynomimal with further boundary conditions which are explained later. The idea is to combine step functions and polynomial regression.  Or in other words, instead of having one global polynomial, we have multiple rather low degree polynomials, one for each of multiple mutually exclusive consecutive regions of X.

_Spline of degree d_: We have k+1 regions.  The k boundaries are called _knots_.  Each region as a polynomial of degree d.  Additionally we have the constraints that the global function derivatives must be continous up to degree d-1.  A _cubic spline_ is a spline of degree 3.

E.g degree 0: Step functions, i.e. non-continuous at knots. degree 1: piecwise linear, i.e. continuous at knots. degree 3: piecwise cubic, with continuous 0th, 1st, 2nd derivatives.

*to-do* but degre 0, i.e. stepfunctions, are not continuos in 0th derivative degree, so they are _not_ splines?!

The _truncated power basis function_ h(x, ξ) of degree d will be used shortly and is defined as follows.  ξ denotes the position of a knot.  Note that at ξ, h, h', h'' etc up to degree d-1 are continous, so the spline's constraints are fulfilled.

h(x, ξ) = { (x-ξ)^d^ if x>ξ; 0 otherwise

There many ways to represent splines of degree d.  We can e.g. base on the <<basic_functions>> approach, in which case there are still many ways of choosing a basis function.  One way is the following, where γs are further model coefficients additional to the βs, ξ~l~ is the position of the l-th knot, and the truncated power function h is defined above.  As always with the basis functions approach, the model can then be fit using linear regression.

y~i~ = ∑~0≤j≤d~β~j~x~i~^j^ + ∑~1≤l≤k~γ~l~h(x~i~, ξ~l~) + ε~i~

There are d+k+1 degrees of freedom.  Analysis:  There are k knots, i.e. k+1 regions, thus k+1 polynomials of degree d.  Recall that a polynomial of degree d has d+1 parameters.  At each knot there are d constraints.  (k+1)(d+1) - kd = k+d+1.

We still need to decide on the number of knots and where to place them.  In practice we often choose only the number of knots, and then distribute them uniformily regarding percentiles, i.e. each region has the same amount of data points.  This altough it seems natural to choose more knots where we feel the model should be able to change more rapidly.  This 2nd variant can work well, still in pratice often the knots are distributed uniformily.

Contra: high variance of f̂ at boundaries, since there polynials still can go wild.  Natural splines try to mitigate this problem.

A _natural spline_ is a spline with the additional boundary conditions that the global function must be linear in the two boundary regions.  Confidence intervals for f̂ in the two boundary regions are now much smaller compared to regular splines.   Note however since the two f̂ are different, parts of the confidence interval of natural splines might be outside of the confidence interval of regular splines.  k degrees of freedom; 2·2 less than regular spline because each of the two border knots has two additional constraints.

*to-do* formula analogous to the above y~i~ = ... for natural splines

*to-do* formula for basis functions of splines and for natural splines

References:

- ISLR, chapter ``7.4 Regression Splines''


===== Smoothing splines

We only look at univariate models, i.e. one predictor X.  As in ridge and lasso, the model takes a ``loss + penalty'' formulation and a tuning paramter λ.  In the definition below which also introduced g, RSS(g) is a loss function that encourages g to fit the training data well, and the term λ∫gʺ(x)²dx is a penalty term that penaltizes a highly ``wiggly'' g.

Let G be a class of functions where [a, b] ⊆ ℝ dontes the data range

G = {g: [a,b] → ℝ, gʺ exits and ∫~a≤x≤b~gʺ(x)²dx < ∞}

The estimate f̂ of the regression function is then defined as follows.  It can be shown that f̂ is a natural cubic splines with n knots, each training data point having one knot on it.  There multiple possible such cubic splines, and we want the one satisfying the given `argmin constraint'.

f̂ = argmin~g∈G~[RSS(g) + λ∫~a≤x≤b~gʺ(x)²dx]

λ = 0: f̂ perfectly fits the _training_ data (RSS=0), i.e. f̂ interpolates the training data, i.e. f̂ goes through every training data point.  It's is a flexible model (n degrees of freedom), thus has low bias and high variance.

λ → ∞: gʺ(x) is forced to be 0 for all x, i.e. g must be linear.  f̂ is equivalent to the linear regression least squares solution.  It's an inflexible model (2 degrees of freedom), thus more bias and less variance; recall that least squares only has zero bias if the true model is linear.

The nominal degree of freedom is n, see also natural splines.  Nominal degree of freedom usually refers to number of free parameters.  However here, these n parameters are constrained or shrunk down.  The effective degrees of freedom df~λ~ is a measure of the flexibility of smoothing splines.  It is between n (for λ = 0) and 2 (for λ → ∞), and in general its defined to be df~λ~ = trace(S~λ~), see definition of S~λ~ below.

*to-do* I don't properly understand the difference between degrees of freedom and effective degrees of freedom. Are equivalent degrees (script 3.4.3) of freedom and effective degrees of freedom synonyms?

The coefficients estimates β̂ (n⨯1 vector) can be computed using fast linear algebra.

As in least squares linear regression where we had Ŷ = Xβ̂ = PY, we can write Ŷ = S~λ~Y where the matrix S~λ~ can be computed using fast linear algrebra.

A way to find a good λ is <<cross_validation>>.  In particular leave one out cross validation (LOOCV), because it turns out that we can do LOOCV in essentially one single fit.

*to-do* why exactly 2nd derivative is chosen

*to-do* what exactly is the difference to cubic natural spline? As far as I understand it, the model f is identical, but the model parameters are fitted with different methods, wheras natural cubic spline is fitted with LS.  Also, smoothing spline has one knot at each data point (what if there are multipl data points per x? average?).  Is smoothing spline equivalent to natural cubic splines where knots are at the data points.

*to-do* how does fitting work? To a level of details important for this lecture.

References:

- ISLR, chapter ``7.5 Smoothing splines''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``3.4 Smoothing splines and penalized regression''


==== Generalized Addidtive Models (GAMs)

y~i~ = β~0~ + ∑~1≤j≤p~f~j~(x~ij~) + ε~i~

The f~j~ are unspecified univariate (smooth) functions (aka smoothers).  E.g. in linear regression, f~j~(x~ij~) = β~j~x~ij~. GAMs are called additive, because there's a separate f~j~ for each predictor X~j~, and then all results are added together.  Followingly there is also no interaction, which would require functions taking multiple predictors as arguments.  That keeps GAMs simple.  From no interaction also follows only little curse of dimensionality, see there.

f~j~ are constrained as follows in order to get an identifiable model. Without these constraint, one could add constant a to f~j~ and substract it from f~jʹ~.  Constraining the f~j~ s that way results in β~0~ = mean(Y).

E[f~j~(X~j~)] = ∑~1≤i≤n~f~j~(x~ij~) = 0

If all f~j~ are univariate models which allow to be fitted with LS, then the overall model can obviously be fitted with LS.  Interpetability is also good, since we can draw an x~j~-y plot for each f̂~j~ (recall that there's no interaction).

References:

- ISLR, chapter ``7.7 Generalized Additive Models''


===== Backfitting

If the f~j~ cannot be fitted using LS (e.g. smoothing splines), one possible fitting approach is backfitting.  It iteratively optimizes one coordinate (corresponding to a predictor), while keeping all others fixed.

--------------------------------------------------
backfitting(Y:n⨯1 vector, X:n⨯p matrix)
  beta0.hat.asvector = mean(Y) * "n⨯1 vector all 1"
  for each predictor j = 1...p:  
    g.j = n⨯1 vector all 0
  do:
    for each predictor j = 1...p:
      Yʹ = Y - beta0.hat.asvector - ∑~1≤k≤p;k≠j~g.k
      f.hat.j = fit.j(Yʹ ~ X.j)
      gnew.j = f.hat.j(X.j)
      gnew.j -= mean(gnew.j) (i.e. normalize)
    convergence = max~over all 1≤j≤p~( (|gnew.j-g.j|)/|g.j| ) < tolerance
    for each predictor j = 1...p:
      g.j = gnew.j
  until convergence
  Result: The f.hat.j s (i.e. their coefficent estimates)
--------------------------------------------------

Normalizing gnew.j is not strictly required, but gives better control of numeric errors.  The convergence criterion could also be choosen differently.  The one used here means to stop when f̂~j~(X~j~) doesn't substantially change anymore.  Tolerance might be 10^-6^.

Note: In most situations, backfitting is equivalent to the Gauss-Seidel method for solving a certain liniear system of equations.

pro) very general, works for all smoothers f~j~.

contra) slow


[[decision_trees]]
==== Decision trees

p denotes the number of predictors, y∈ℝ, x∈ℝ^p^. P = {R~1~, ..., R~M~} is a partition of ℝ^p^ into M regions / partitions.  β~1~, ..., β~M~ are coefficients of the model.  I(·) denotes the indicator function.

The underlying model: y = g~tree~(x) + ε, g~tree~(x) = ∑~1≤r≤M~β~r~I(x∈R~r~).

Note that g~tree~ is piecewise constant.

β~j~ equals the average of the y values in region R~j~. Thus, computing/estimating the coefficients β is easy once we know the partition P.

The hard part is finding a good partition, see <<estimation_of_partition>>.

Note that interactions are allowed.

*to-do* somewhere, probably close to correlation and or (in)depence, write what interaction is:  Say we have two predictors.  With no interaction, the curve Y(X1) remains the same independent of X2.

If the number of data points per region is too small (e.g. when tree is too deep), we are suffering from overfitting.

References:

- ISLR, chapter ``8 Tree-Based Methods''


[[estimation_of_partition]]
===== Estimation of partition

We restrict ourselves to partitions that can be represented as trees of axis parallel regions, i.e. p-dimensional boxes.  The idealized goal from training perspective is to find a partition which minimizes the RSS (which on the downside would result in a strong overfit).

Recursively split ℝ^p^ into regions.  At each split, greedely choose an axis / predictor and a split point such that the log likelihood is increased the most.  In other words, from all the possible trees resulting from the possible splits, choose the the tree/split with the highest log likelyhood.  In certain cases this is the same as reducing the RSS the most.  A possible stopping criterion might be that all regions contain fewer than a given number of observations.

*to-do* concrete example of log likelihood in this context

_prunning back_: The resulting full tree T~0~ is likely to overfit by beeing to deep / complex.  A smaller subtree T ⊂ T~0~ with fever splits is likely to have smaller variance, smaller test error and better interpretation at the cost of higher bias.  Intuitively we want to find the subtree that leads to the lowest test error. Note that in case of <<bagging>>, we don't prune.

Note that the alternative algorithm of stop growing the tree when a split doesn't seem worth it is not the same as the above algorithm of growing deep and then prune afterwards.  This is because a seemingly worthless split might be followed somewhere deeper down by a very worthwhile split, that is a split that leads to a large increase in the log likelyhood.

_cost complexity pruninning_: A concrete prunning back method.  We find a subtree T⊆T~0~ according to the following argmin formula, where |T| denotes the number of leaves in tree T, and α is a tuning parameter.  Note that it's a similar pattern as seen in <<lasso>> or <<ridge>>. α=0 means no shrinkage penalty resulting in T=T~0~, and α→∞ results in |T|=0, i.e. T has no split.  It turns out that as we increase α from zero, branches get pruned from the tree in a nested and predictable fashion, so obtaining the whole sequence of subtrees as a function of α is easy.  Note that in case of bagging, we don't prune.

argmin~T⊆T0~{RSS(T) + α|T|}

We can do cross validation to find the best α, and then do the recursive splitting and the cost complexity pruning using the optained α on the original data set.

See also <<bagging>> to further reduce variance.


[[random_forests]]
===== Random forests

Is a variant of bagging which aims to decorrelate the trees by the way how splitting works during building a tree.  At each split, only a random set of size m of the p predictors are allowed to be considered for the split.  Often m = √p for classification and m = p/3 for regression.  m = p amounts simply to bagging.

Recall that the idea behind bagging is that averaging quantities decreases variance.  However the variance is not as much reduced when the quantities are correlated.  In the context of bagging that might happen when there's a strong predictor.  Most of the bagged trees will have that predictor in the top split, and consequently most of the bagged trees will look quite similar, and thus the predictions from these bagged trees will be highly correlated.

References:

- ISLR, chapter ``8.2.2 Random Forests''


[[comparison_of_tree_based_methods]]
===== Comparison of tree-based methods

Pros/cons of trees:

pro) interpretation, visualization

pro) allow interactions

pro) can easily handle qualitative predictors (no need for dummy variables)

neutral) classification performance ok

con) prediction performance of functions rather poor (much improved by <<bagging>> or random forests)

con) high variance (much improved by <<bagging>>)

con) piecewise constant underlying function is `unnatural'

con) unstable splitting error at top of tree propagates down

Comparison:

|=====
|                     | trees   | bagged trees | random forests
| interpretation      | +       | -            | -
| computation         | +       | \--          | -
| performance         | -       | +            | \++
| OOB error estimates | -^1)^   | +            | \+
| overfitting         | yes^1)^ | ok           | ok
|=====

1) Use crossvalidation to mitigate


[[KNN]]
==== K-nearest-neighbor regression (KNN)

f̂(x~0~) = 1/k ∑~xi∈N0~y~i~, where N~0~ are the k closest observations to x~0~.

If the underlying model is linear, KNN is worse.

Curse of dimensionality: With more predictors KNN gets worse. Intuitively: When p is gettig larger, then in p-dimensional space the density of data point usually decreases.  Thus the number of neighbors usually decreases.  Linear regression also suffers from larger p's, but not as mutch as KNN. See also <<curse_of_dimensionality>>.

Note: When plotting test error (e.g. expected test MSE), then often on the x-axis we plot 1/k (opposed to k).

*to-do* is a special case of local regression, right? Weihthing function K is constant 1, and the underlying model is a constant function.


==== Local regression

Computes a fitted value ŷ~0~ given an target point x~0~, as opposed to computing coefficents for a global function f̂.  Select the k = s·n nearest training points x~i~ of x~0~, and assign a weight K~i~ = K(x~i~, x~0~) to each of them.  The fraction s ∈ (0,1] (also called span) of training points to select and the weighting function K are parameters of the algorithm.  The weighting function K should be such that the x~i~ closest to x~0~ is assigned the highest weight, and the x~i~ furthest away is assigned the lowest weight.  Do weighted least squares regression on these k points using K~i~ as weight, delivering f̂~x0~.  Whether the underlying model is constant, linear or a polynomial of some higher degree is a further parameter of the algorithm.  The fitted value ŷ~0~ at x~0~ is then ŷ~0~ = f̂~x0~(x~0~).

The most important choice of all these parameters is the span s. It controls the flexibility of the fit: small values results in very local and wiggly fit, large values result in global fit.  We can for example use cross-validation to choose s.

References:

- ISLR, chapter ``7.6 Local Regression''


[[LOESS]]
==== LOESS regression curve

LOESS is locally weighted polynomial regression.  Loosely speaking, LOESS can also be seen as a generalization of KNN which is smoother.  KNN can be thought of as a rectangle function.  The k neigherst neighbors get wheight 1/k, all others get weight 0.  LOESS has a smoother weighting function, parameterized by α.  Larger α means more moothing, which results in less variance and more bias.  Larger n (more samples, while keeping x-range constant) makes it more precise.

**to-do**(3) Comparison LOESS vs local regression

**to-do**(3) What exactly is meant by its getting more precise with larger n? Ssee Exerscise Set 3, exercise 3b. It seems to me that the variance drops, but the bias stays constant. Isn't that against the bias-variance tradeoff? Or is modifying n not covered by bias-variance trade off?


[[curse_of_dimensionality]]
==== Curse of dimensionality

In general, adding additional signal predictors that are highly associated with the response will improve the fitted model in terms of decreasing test MSE.  However even if they are associated, the increase in variance might outweight the reduction in bias.  Adding noise predictors that are not truly associated with the response will detoriate the fitted model, because they increase the dimensionality of the problem, exacerbating the risk of overfitting, since noise features may be assigned nonzero coefficients due to chance.

Models with no interaction suffer only little from curse of dimensionality, because they fit a predictor at a time, and when fitting a single predictor, they only `look in one direction' in the p-dimensional space.

KNN suffers more than linear regression or GAM with splines. KNN looks in all directions in the p-dimensional space at the same time.

See also respective paragraph in <<KNN>>.


=== Bootstrap

The _bootstrap_ is a method for estimating the distribution of an estimator θ̂ derived from on a population with unknown distribution (or known distribution family with unknown parameter).  We want the distribution of θ̂ for inference, e.g. for estimating se[θ̂] and computing confidence intervals for θ̂.  Let P be an unknown distribution, Z~1~, ..., Z~n~ iid \~ P a sample, and θ̂ (or θ̂~n~) an estimator of an unknown parameter θ.  Using the given sample we can calculate a realization of θ̂.  But we don't yet know the uncertainty involved with the estimator θ̂.  The bootstrap will deliver an empirical distribution of θ̂.

If we had P, we could simply simulate using P, i.e. take multipile samples, compute an θ̂ for each sample, delivering the empirical distribution of θ̂.  The key idea of bootstrap is to estimate P, denoted P̂ (or P̂~n~), and then use P̂ to simulate.  Estimating P is simple.  The estimate P̂ is an empirical distribution of Z~1~, ..., Z~n~ which places mass 1/n at each data point.  It is a discrete distribution on the data points, and each point is equally likely.  To sample from P̂ means to uniformily sample with replacement from {Z~1~, ..., Z~n~}.

Sampling from P̂ yields a so called _bootstrap sample_ Z~1~^∗i^, ..., Z~n~^∗i^ iid\~ P̂, where i denotes the i-th bootstrap sample.  Typically the bootstrap sample size is choosen to be the same size as the original sample size n.  Note that Z~1~^∗i^, ..., Z~n~^∗i^ are still random variables.  Side note: The probability that an observation z~i~ of the original sample is contained in the bootstrap sample is about 2/3.

The _bootstrapped estimator_ θ̂^∗^ is is then simply θ̂^∗^ = g(Z~1~^∗i^, ..., Z~n~^∗i^).  Is an estimator for θ̂; the point being that θ̂^∗^'s distribution can be simulated and so we can estimate θ̂'s distribution, expectation, variance etc.  Note that θ̂^∗^ is still a random variable, the (probability) sample space being the original sample, opposed to usual (probability) sample space being the population.

The _bootstrap distribution_ P^∗^ is the distribution of the bootstrapped estimator θ̂^∗^.  It is a conditional distribution given the (random) original sample.  P^∗^ is induced by sampling from P̂.

From now on its hard to use good terms.  One one side literature also uses only a few terms, and the few terms that are used are far from universal agreement.  And on the other side, when I tried to come up with a coherent set of terms, the individual terms become a so long sequence of words, that they become too unpractical.  I gave up and will mostly just use symbols.  The terms I do use I more or less liked, but they are not at all in universally used in literature.

_Bootstrapping an estimator_ θ̂ means sampling from P̂, delivering B bootstrap sample realization, the i-th denoted z~1~^∗i^, ..., z~n~^∗i^.  That then delivers B bootstrap estimator realizations (or _bootstrap replications_): θ̂^∗i^ = g(z~1~^∗i^, ..., z~n~^∗i^).  These B bootstrap estimator realizations then constitute the bootstrap distribution P^∗^.

Pr^∗^[·] is a conditional probability given the original sample.  The (probability) sample space is the original sample, as opposed to the usual (probability) sample space being the population.

The _bootstrap expectation_ E^∗^[·] (or E~P^∗^~[·]) is a conditional expectation given the original sample; the (probability) sample space is the original sample, as opposed to the usual (probability) sample space being the population.  I.e. changing the original sample will change the value of E^∗^[·].

Likewise the _bootstrap variance_ Var^∗^[·] (or Var~P^∗^~[·]) is a conditional variance given the (random) original sample.

The original problem included that we liked to know E[θ̂].  We can estimate E[θ̂] using E^∗^[θ̂^∗^] as an estimator. E^∗^[θ̂^∗^] is a consistent estimator for E[θ̂] if n is large, i.e. when <<bootstrap_consistency>> kicks in.  However we don't know that one either.  We can estimate E^∗^[θ̂^∗^] by averaging over the bootstrap replications θ̂^∗i^.  Ê^∗^[θ̂^∗^] is a good estimator for E^∗^[θ̂^∗^] if B is large. (*to-do* 1) is E^∗^[θ̂^∗^] ≈ Ê^∗^[θ̂^∗^] due to WLLN? 2) is Ê^∗^[θ̂^∗^] a consistent estimator for E^∗^[θ̂^∗^] -- or what is a a better term than `good' in ``Ê^∗^[θ̂^∗^] is a good estimator for E^∗^[θ̂^∗^] if B is large'')

E[θ̂] ≈(large n) Ê[θ̂] = E^∗^[θ̂^∗^] ≈(large B) Ê^∗^[θ̂^∗^] = 1/B ∑^B^ θ̂^∗i^

Likewise for variance:

E[θ̂] ≈(large n) Ê[θ̂] = E^∗^[θ̂^∗^] ≈(large B) Ê^∗^[θ̂^∗^] = 1/(B-1) ∑^B^(θ̂^∗i^ - Ê^∗^[θ̂^∗^])²

And for bias:

bias[θ̂] = E[θ̂] - E[θ] ≈(large n) E^∗^[θ̂^∗^] - θ̂ ≈(large B) Ê^∗^[θ̂^∗^] - θ̂

The _bootstrap quantile_ *to-do* it's conditional, right? Must be, since it's based on the conditional distribution P^∗^.


[[bootstrap_consistency]]
==== Bootstrap consistency

The boostrap is called to be _consistent_ with rate a~n~ for estimator θ̂ if for an increasing sequence a~n~, for all x:

Pr[a~n~·(θ̂-θ)≤x] - Pr^∗^[a~n~·(θ̂^∗^-θ̂)≤x] P→ 0 (as n→∞)

Explaining the formula:  For Pr^∗^[·] see previous chapter.  In the left Pr[...], the estimator θ̂ is a random variable and θ is fixed.  In the right Pr^∗^[...], the bootstrap estimator θ̂^∗^ is a random variable and θ̂ is the fixed realization of estimator θ̂ using the original sample.  Typically a~n~ = √n.  An oversimplified way is to think that θ̂-θ and θ̂^∗^-θ̂ must have the same CDF.

Bootstrap consistency is important because it makes things work.  Consistency of the boostrap typically holds if the limiting distribution of θ̂ is Normal, and if the original data Z~1~, ..., Z~n~ are iid. (*to-do* 1) What is exactly meant with `the bootstrap'? Isn't it more precise to say that the bootsraped estimator θ̂^∗^ is consistent under the following condition - but then why use another formula than that of an consistent estimator? 2) Which things in the context of our lecture do work only due to bootstrap consistency? The estimators for E[θ̂], Var[θ̂] etc (the first approximation/estimation) 3) Why is the bootstrap usefull if I almost only can use it when θ̂ is asymptotically normal -- the problem statement was that I don't know θ̂'s distribution)

Implication of bootstrap consistency: The shape of P^∗^ is that of θ̂. So they have same expectation, i.e. centered around same point, and same variance.  (*to-do* correctly phrased? see lecture slide5)

Consistency of the boostrap implies consistent variance and bias estimation, i.e.:

(E^∗^[θ̂^∗^]-θ̂) / (E[θ̂]-θ) P→ 1 +
Var^∗^[θ̂^∗^] / Var[θ̂] P→ 1


==== Bootstrap confidence intervals

We want to compute an estimate of 1-α confidence interval for the estimator θ̂.  As described in the bootstrap introduction chapter, we can optain the conditional distribution P^∗^ of the bootstraped estimator θ̂^∗^, and from that also its conditional quantile q~θ̂^∗^~. (*to-do* 1) all these are not true confidence intervals, each is an estimate of the confidence interval, right? What's the correct wording? 2) its a conditional quantile, right?)

_normal_: θ̂ ± q~Z~(1-α/2) · sd̂[θ̂], where Z ~ N(0, 1) and sd̂[θ̂] = √Var̂[θ̂] ≈ √Var̂^∗^[θ̂^∗^] as described in the bootstrap intro chapter.  Note θ̂ does lie in the middle of the interval.

_quantile_ (or _percentile_): [q~θ̂^∗^~(α/2), q~θ̂^∗^~(1-α/2)]. Special case of reversed quantile: it equals reversed quantile if the distribution of θ̂^∗^ - θ̂ is symmetric.  It's proovable that coverage is not 1-α.  Note that θ̂ might not lie in the middle of the interval.

_reversed quantile_ (or _basic_):  [θ̂ - q~θ̂^∗^-θ̂~(1-α/2), θ̂ - q~θ̂^∗^-θ̂~(α/2)].  If bootstrap consistency holds, its proovable that coverage is 1-α.  Note that θ̂ might not lie in the middle of the interval.  Performance in practice is sometimes critized.

*to-do* why do we make it so complicated - what's the intuition why we now get a better coverage that the quantile CI

*to-do* lecture notes say that performance is sometimes critized -- but nobody says that normal or quantile has better theoretical performance, right? Or whas performance meant in sence of computationally expensive?

_bootstrap T_ (or _studentized_):  [θ̂ - q~(θ̂^∗^-θ̂)/sd̂[θ̂^∗^]~(1-α/2) · sd̂[θ̂], θ̂ - q~(θ̂^∗^-θ̂)/sd̂[θ̂^∗^]~(α/2) · sd̂[θ̂]].  sd̂[θ̂] as in normal CI.  We approximate (θ̂^∗^-θ̂)/sd̂[θ̂^∗^] by the empirical distribution (θ̂^∗1^-θ̂)/sd̂[θ̂^∗1^], ..., (θ̂^∗B^-θ̂)/sd̂[θ̂^∗B^].  Bootstrap T has best theoretical properties, but is computationally very expensive.  Intuition:  Look at θ̂-θ ≈ θ̂^∗^-θ̂, where θ̂-θ is what I would like to have and θ̂^∗^-θ̂ is my observation.  If we instead take (θ̂-θ)/sd(θ̂) ≈ (θ̂^∗^-θ̂)/sd̂[θ̂^∗^], the two sides get similar more quickly.  Note that θ̂ might not lie in the middle of the interval.

References:

- Slides5.pdf


==== Parametric bootstrap

In parametric boostrap (or model-based bootstrap), the sample is Z~1~, ..., Z~n~ iid \~ P~δ~, where δ is an unknown parameter of a known distribution family P~δ~.  We make an estimate δ̂ of δ, and can then create B samples from P~δ̂~.  Now we're in the same situation as in non-parametric bootstrap: we have B new samples constituting an empircal distribution.  Thus confidence intervals etc work the same for parametric bootstrap.

Pro: Good _if_ parametric model is approximately correct, then P~δ̂~ is closer to P~δ~ than P̂ is to P.

Pro: For small n, non-parametric bootstrap might be poor, because estimates (Ê[θ̂], Var̂[θ̂] etc.) are only good for large n.

Contra: We need to make assumptions (the family P~δ~ and the estimate δ̂). Non-parametric bootstrap doesn't need assumptions (i.e. only those of bootstrap consistency)

Contra: Bad if parametric model is far from the truth, then P~δ̂~ is farther from P~δ~ than P̂ is from P.  I.e. parametric bootstrap is sensitive to model misspredictions.


==== Bootstrap for regression

The model is Y = f(x) + ε, where the error terms have unknown distribution, in general maybe not even iid.  There are multiple options, ranging from fully parametric to fully non-parametric. For example:

- Fully parametric regression: We assume parametric model f(x) = Xβ and ε iid\~ N(0,σ²), with the parameters β and σ. We estimate these parameters giving us β̂ and σ̂.  We then can sample like this: Y^∗^ = f̂(x) + ε^∗^, where f̂(x)=Xβ̂ and ε^∗^ iid~ N(0,σ̂²).

- Non-parametric residuals: No assumptions on error terms ε, but still assuming parametric model f(x) = Xβ.  We make estimation β̂.  This delivers residuals e = Y - f̂(x), where f̂(x)=Xβ̂.  Sampling with replacement from e~1~, ..., e~n~ gives sample of residuals e^∗^. Y^∗^ = f̂(x) + e^∗^.

- Non-parametric: Resample observations (i.e. rows in cbind(Y,X) matrix), i.e. vanilla non-parametric bootstrap


==== Bootstrap summary

non-parameteric case:

_Real world_: +
we have a sample (Z~1~, ..., Z~n~) \~ P (unknown) +
estimator θ̂ = g(Z~1~, ..., Z~n~) \~ unknown-distribution +
θ̂'s value known since g(Z~1~, ..., Z~n~) can be computed

_Boostrap world_: +
sampling from P̂ delivers bootstrap sample (Z~1~^∗^, ..., Z~n~^∗^) +
bootstraped estimator θ̂^∗^ = g(Z~1~^∗^, ..., Z~n~^∗^) ~ P^∗^ +
Distribution P^∗^ given by the B bootstrap sample realizations

parametric case:

_Real world_: +
P~δ~ is a known parameterized family of distributions, δ is unknown +
we have a sample (Z~1~, ..., Z~n~) \~ P~δ~ +
estimator θ̂ = g(Z~1~, ..., Z~n~) \~ unknown-distribution +
θ̂'s value known since g(Z~1~, ..., Z~n~) can be computed

_Boostrap world_: +
make estimate δ̂ +
sampling from P~δ̂~ delivers bootstrap sample (Z~1~^∗^, ..., Z~n~^∗^) +
bootstraped estimator θ̂^∗^ = g(Z~1~^∗^, ..., Z~n~^∗^) ~ P^∗^ +
Distribution P^∗^ given by the B bootstrap sample realizations


[cols="1,3"]
|=====
| B                          | Number of bootstrap samples
| P                          | The unknown distribution of the (original) sample Z~1~, ..., Z~n~
| P̂                         | Estimate of P. Empirical distribution of Z~1~, ..., Z~n~ which places probability mass 1/n on very data point Z~i~.
| Z~1~, ..., Z~n~ iid ~ P    | (Original) sample
| Z~1~^∗^, ..., Z~n~^∗^ iid ~ P̂ |  Bootstrap sample (or simulated data)
| z~1~^∗i^, ..., z~n~^∗i^    | i-th bootstrap sample realization
| θ (or θ~0~)                | Unknown parameter
| g                          | Function underlying the estimator θ̂
| θ̂ (or θ̂~n~) = g(Z~1~, ..., Z~n~) | Estimator for θ.  θ̂'s distribution is unknown, but we would like to know it.
| Pr^∗^[·]                   | Conditional probability given the original sample.
| E^∗^[·] (or E~P^∗^~[·])    | Bootstrap expectation. Conditional expectation given the original sample
| Var^∗^[·] (or Var~P^∗^~[·])| Bootstrap variance. Conditional variance given the original sample
| θ̂^∗^                      | Boostraped estimator. Random variable, the (probability) sample space being the original sample.
| θ̂^∗i^ = g(z~1~^∗i^, ..., z~n~^∗i^) | i-th bootstraped estimator realisation
| P^∗^                               | Bootstrap distribution. Distribution of θ̂^∗^.  Is a conditional distribution given the original sample.
| E[θ̂] ≈ Ê[θ̂] | large n
| Ê[θ̂] = E^∗^[θ̂^∗^] ≈ Ê^∗^[θ̂^∗^] | ≈ large B
| Ê^∗^[θ̂^∗^] = 1/B ∑^B^ θ̂^∗i^ |
| Var[θ̂] ≈ Var̂[θ̂] | large n
| Var̂[θ̂] = Var^∗^[θ̂^∗^] ≈ Var̂^∗^[θ̂^∗^] | ≈ large B
| Var̂^∗^[θ̂^∗^] = 1/(B-1) ∑^B^(θ̂^∗i^ - Ê^∗^[θ̂^∗^])² |
| bias[θ̂] = E[θ̂] - E[θ] ≈ biaŝ[θ̂] | large n
| biaŝ[θ̂] = E^∗^[θ̂^∗^] - θ̂ ≈ biaŝ^∗^[θ̂^∗^] | ≈ large B
| biaŝ^∗^[θ̂^∗^] = Ê^∗^[θ̂^∗^] - θ̂ |
|=====


References:

- Book ``An introduction to statistical learning'', chapter ``5.2 The Bootstrap''

- Book ``All of statistics'', chapter ``8 The Bootstrap''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``5 Bootsrap''


== Algebra and Arithmetic


=== Exponentiation, root, logarithm

base^exponent^ = power

^degree^√radicand = root

log~base~(antilogarithm) = logarithm

References:

- Notes on Logarithms and Units: https://www.cs.auckland.ac.nz/courses/compsci314s1c/resources/logNotes.pdf


=== Number theory

ℕ natural numbers. Whether 0 ∈ ℕ is not clearly defined.

ℕ~0~, ℕ^0^, ℤ~≥0~, ℤ^*^ non-negative integers

ℕ~>0~, ℤ^+^ positive integers

ℤ integers. Z is for the German word Zahlen.

ℚ rational numbers. Q is for the German word Quotient.

ℝ real numbers

0 is neither positive nor negative.

References:

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapters "Number Theory"

- Book "Introduction to algorithms", chapter "31 Number-Theoretic Algorithms"


==== Divisibility and greatest common divisor

**In this subchapter, we're only looking at integers.**

a _divides_ b (or a is a _divisor_ of b, or b is _divisible_ by a), denoted a | b, iff there is a k such that ak=b.  b and 1 are so-called _trivial divisors_ of b.  Nontrivial divisors of b are called _factors_ of b.  If additionally k ≥ 1, we say b is a _multiple_ of a.

Divisibility is reflexiv and transitiv, bot not symmetric. *to-do* write more explicitely using formulas

a|0 (by agreement)

f|a and f|b ⇒ f|(sa+tb) for any s and t (a linear combination of a and b is divisible by any common factor of a and b)

a|b and b|a ⇒ a=b

n is a _linear combination_ of b~0~, ..., b~k~ ⇔ n = ∑s~i~b~i~.

A _commonon divisor_ of a and b is a number that divides them both.  The _greatest common divisor_ (_GCD_) (or _greatest common factor_ or _highest common divisor_) of a and b is denoted gcd(a,b).  By convention gcd(0, 0) = 0.

gcd(a,b) = gcd(b,a) (commutative)

gcd(a, gcd(b,c)) = gcd(gcd(a,b), c) (associative)

gcd(a, b, c) = gcd(gcd(a, b), c) (gcd of more than two arguments)

d|a and d|b ⇒ d|gcd(a,b)

a|bc and gcd(a,b) = d ⇒ a/d | c

gcd(ma, mb) = m gcd(a, b) ∀ m ∈ ℕ

gcd(m + mb, b) = gcd(a, b)

gcd(a, ma) = a

gcd(a,0) = |a|

gcd(a,c)=1 and gcd(b,c)=1 ⇔ gcd(ab,c)=1

gcd(a,b) = gcd(b, a mod b) (see Euclid's algorithm)

Two integers and b are _relative prime_ if gcd(a,b) = 1.

From the fundamental theorem of arithmetic directly follows that gcd(a, b) = product of primes common to a and b.  Thus an inefficient algorithm to compute gcd(a, b) is to prime factorize a and b, compare the factors, and build the product of the common factors.

_Bézout's lemma_ (or _Bézout's idendity_): For any nonzero a and b:
1) gcd(a,b) = sa+tb for some s and t; i.e. gcd(a,b) is a linear combination of a and b.
2) gcd(a,b) is the smallest positive integer that can be written as sa+tb.
3) sa+tb | gcd(a,b) for any s and t; i.e. every linear combination of a and b is a multiple of gcd(a,b).

_Euclid's algorithm_ Recursively solve gcd(a,b) by gcd(a,b) = gcd(b, a mod b). The bottom case is b = 0, in which case gcd(a,0) = |a|.

binary method to compute gcd: *to-do*


==== Prime numbers

**In this subchapter, we're only looking at integers.**

A _prime_ is a number greater than 1 that is divisible only by itself and 1. A number other than 0, 1 and -1 that is not a prime is called _composite_.

_Fundamental Theorem of Arithmetic_: Every positive integer is a product of a unique weakly decreasing sequence of primes.

For all primes p and any a,b: if p|ab then p|a or p|b.

There are infinitely many primes.

The _prime-counting function_ π(x) is the function giving the number of primes less than or equal to a given number x.

_Prime Number Theorem_: π(x) ~ x/ln(x). Thus as a rule of thumb, a given integer x is prime with a probability of about 1/ln(x). For x>67: π(x) > x/ln(x).

_Chebyshev's Theorem on Prime Density_: π(x) > x / (3 ln x).

See also algorithms_and_data_structures.adoc, chapters ``primalty testing'' and ``generating primes''.


==== Modular arithmetic

**In this subchapter, we're only looking at integers.**

_Division Theorem_ (or _Division Algorithm_): Let n (_numerator_) and d (_denominator_) ≠ 0 be integers, then there exists a unique pair of integers q (_quotient_) and r (_remainder_) such that q·d + r = n and 0 ≤ r < |d|.
Note that by this definition, the remainder is always nonnegative, as opposed to how many programming languages define it.

Common notations for the _remainder operation_ (or _modulo operation_) are n mod d or rem(n, d).  Common notations for _quotient operation_ are n div d or qcnt(n, d).

_Modular arithmetic_ (or _clock arithmetic_) is the arithmetic of congruences.

A _congruence relation_ (or simply _congurence_) is an equivalence relation on an algebraic structure that is compatible with the structure.

_Congruence modulo n_ on the set of integers is a congruence relation. a ≡ b (mod n) denotes ``a is congruent to b (modulo n)'' or ``a and b are congruent modulo n''.  The number n is called the _modulus_.  These three claims are equivalent:

a ≡ b (mod n) ⇔ +
n | (a-b) ⇔ +
a = b + kn ∀ k ∈ ℕ

In the following, the explicit (mod n) is omitted for brevity.

An a^-1^ such that a·a^-1^ ≡ 1 is called _modular multiplicative inverse_ of a modulo n.  a^-1^ exists iff a is coprime with n.

a ≡ a [reflexiv]

a ≡ b ⇔ b ≡ a [symetric]

a ≡ b and b ≡ c ⇒ a ≡ c [transitiv]

a ≡ b ⇔ a + k ≡ b + k ∀ k ∈ ℤ [compatibility with translation]

a ≡ b ⇒ ka ≡ kb ∀ k ∈ ℤ [compatibility with scaling]

ka ≡ kb and k is coprime with n ⇒ a ≡ b

a ≡ b ⇒ a^k^ ≡ b^k^ ∀ k ∈ ℕ [compatibility with exponentation]

a ≡ b and c ≡ d ⇒ a + c ≡ b + d [compatibility with addition]

a ≡ b and c ≡ d ⇒ a - c ≡ b - d [compatibility with subtraction]

a ≡ b and c ≡ d ⇒ ac ≡ bd [compatibility with multiplication]

a ≡ b and a^-1^ exists ⇒ a^-1^ ≡ b^-1^ [compatibility with multiplicative inverse]

When a = x² mod p for an a ∈ ℤ~p~ and any x ∈ ℤ~p~, then a is called a _quadratic residue_.

When a ≠ x² mod p for an a ∈ ℤ~p~ and all x ∈ ℤ~p~, then a is called a _quadratic nonresidue_.

Exactly half of the nonzero elements of field ℤ~p~ are quadratic residues.

_Legendre symbol_: Leg(a|p) ≡ {1 if a is a quadratic residue, -1 if a is a quadratic non-residue, 0 if p|a}.

p is odd prime ⇔ a^(p-1)/2^ ≡ Leg(a|p) ∀ a ∈ ℤ~p~ - \{0}. [Euler's Criterion]

_Jacobi Symbol_: Jac(a|n) = ∏~1≤i≤l~Leg(a|p~i~)^k~i~^ = ∏~1≤i≤l~(a^(p~i~-1)/1^ mod p~i~)^k~i~^ = {1, -1}, where gcd(a,n) = 1 and where p~1~^k~1~^ · ... · p~l~^k~l~^ is the prime factorization of n.

p is prime and a ∈ ℤ and gcd(a,p) = 1 ⇒ a^p-1^ ≡ 1 [Fermat's little theorem]

p is prime and a ∈ ℤ~p~ - \{0} ⇒ a^-1^ = a^p-2^ mod p [Consequence of Fermat's little theorem]

p is prime ⇔ (p-1)! ≡ -1 [Wilson's theorem]

Chinese Remainder Theorem:  Let m = m~1~·...·m~k~ where k ∈ ℤ^+^ and m~i~ ∈ ℤ^≥2^ are pairwise coprimes. For any sequence r~1~ ∈ ℤ~m1~, ..., r~k~ ∈ ℤ~mk~ there is an unique r ∈ ℤ~m~ such that r ≡ r~i~ (mod m~i~) ∀ i ∈ [k].


==== Abstract Algebra

A set S is _closed_ under an n-ary operation f if f: S^n^ → S. A set S is closed under a collection of operations if it is closed under each of the operations individually.

An _algebraic structure_ (or simply _algebra_) is a pair (S, F) where S is a set closed under a set F of operations.

Given an algebra (S, ∗) where ∗ is a binary operation.  An element e ∈ S is called a _left identity_ if e ∗ x = x ∀ x ∈ S, and a _right identity_ if x ∗ e = x ∀ x ∈ S.  If e is both a left and a right identity, then it is called a _two-sided identity element_ (or _two-sided neutral element_ or simply _identity_) according to ∗ in S.

Given an algebra (S, ∗) where ∗ is a binary operation, elements a, b ∈ S, and the neutral element e ∈ S.  If a ∗ b = e, then a is called a _left inverse_ of b and b is called a _right inverse_ of a.  If an element is both a left and a right inverse, it is called a _two-sided inverse_ (or simply _inverse_).  The inverse element of element x ∈ S is denoted x^-1^ (or i(x) or -x if the algebra's operation is denoted +).

Note that the algebra (ℤ, +) where + denotes normal addition, subtraction a - b is modeled by adding the inverse, i.e. a + i(b) (or a + -b).  Likewise for division in algebra (ℝ, ·) where · denotes normal multiplication: division a / b is modeled by multiplying the inverse, i.e. a · i(b) (or a · b^-1^).

A _semigroup_ is an algebra (S, ∗) where ∗ is a binary associative operation on S.

A _monoid_ is an algebra (M, ∗) where ∗ is a binary associative operation and S has a neutral element.

A _group_ is an algebra (S, ∗) where ∗ is a binary associative operation and S has a neutral element and every element x ∈ S has an inverse element.

The _order_ (or _cardinality_) of a group G (or ring or field), denoted |G|, is the number of elements it contains.

A group is _commutativ_ (or _abelian_) if x ∗ y = y ∗ x ∀ x,y ∈ S.  If a group is not commutative, its called _noncommutative_ (or _non-abelian_).

Let (S, ∗) be a group with the neutral element e. The _i-th power_ of x ∈ S, denoted x^i^, is inductively defined as follows, for any x ∈ S and i ∈ ℤ:

i) x^0^ = e (x^0^ is called the _trivial power of x_)

ii) x^1^ = x

iii) x^i^ = x ∗ x^i-1^ ∀ i > 1 (x^i^ is called a _nontrivial power of a_)

iv) x^-i^ = (i(x))^i^ ∀ i ≥ 1

Given a group G = (S, ∗), an element x ∈ S is called a _generator_ of that group if S = {x^i^|i∈ℤ}.  We also denote that with ⟨x⟩ = G. If a group has a generator, then the group is called _cyclic_.

Given a group (S, ∗) with identity e. The _order_ of an element x ∈ S, denoted |x|, is the smallest positive integer n such that x^n^ = e. If there is no such n, then element x has _infinite order_.

A _ring_ is an algebra (R, +, ·) where (R, +) is a commutative group and (R, ·) is a semigroup and · is distributive over + (*to-do* unspecified whether left/right/total distributive).  Wether or not a ring requires an identity under · is under debate, see also ring with identity.

A ring (R, +, ·) with neutral element 0 under + is called _zero division free_ if x · y ≠ 0 ∀ x, y ∈ R - \{0}.

My personal derivation: The neutral element 0 under + has no inverse under ·, so we want to prohibit having to take the inverse of 0.  We take the inverse of an element when a = c · 1/b ⇔ a · b = c.  So in our use case we want to prohibit (for a,c ∈ R - \{0}) that a · 0 = c ⇔ 1/a · a · 0 = 1/a · c ⇔ 0 = 1/a · c, which is what zero division free said.

If the · operation of a ring (R, +, ·) is commutative, it's called a _commutative ring_.  If the · operation is not commutative, it's called a _noncommutative ring_ (or simply ring).

If the · operation of a ring (R, +, ·) with identity 0 under + has an identity for every element in R - \{0}, it's called a _ring with identity_.

A _field_ (_Körper_ in German) (R, +, ·) with neutral element 0 under + is a zero division free ring where for · the following holds: commutative, R - \{0} has an identity and there's an inverse for all x ∈ R - \{0}.

Note that some authors say that a ring (R, +, ·) is the commutative group (R, +) with identity 0 and the commutative group (R-\{0}, ·).  This is not entirely correct because it technically says that · is closed under R-\{0} which is not what we mean.  We do want to allow 0 as operand and result of ·, we only want to disallow division by 0.

For example, ℚ and ℝ build fields with respect to addition and multiplication.  However for ℤ it is impossible to define division.

Given a field K = (R, +, ·) with identity 1 under + and identity 0 under ·, the _field characteristic_ ch(K) is the minimum number of times 1 has to be added (e.g. 1+1 counts as two times) to equal 0.  If 0 is never reached, then ch(K) = 0.

A _finite field_ (or _Galois field_) is a field with a finite field order.  The order of a finite field is always a prime power p^k^, where p is a prime and k is a positive integer.  All finite fields of a given order are isomorphic.  In case k = 1, the finite field is called a _prime field_, denoted GF(p) (or 𝔽~p~), and is the field of residue classes modulo p, where the elements of GF(p) are denoted 0, ..., p-1.  Thus a = b in GF(p) means the same as a ≡ b (mod p).  p is the characterstic of the prime field.  The inverse with respect to · can be computed with the _extended Eucledian algorithm_ (*to-do*).  In case k > 1, the finite field is denoted GF(p^k^) (or 𝔽~p^k^~).

ℤ/pℤ denotes a special case of a quotient group (recall a group as only one operation), but is apparently sometimes used to denote a prime field (recall that a field has two operations).

ℤ~n~ denotes an abstract algebra over set {0, ..., n-1} with mod n modular arithmetic.  Wether ℤ~n~ denotes a finite group or a finite field (and thus prime field) depends on the context.

*to-do* vector space, norm, module,

Summary:

R denotes the set of elements of the algebraic structure.  ba denotes binary associative operation.  The identity under +, if it exists, is denoted 0.  NA denotes not available.  d denotes that · is distributive over +.  e denotes existence of an identity under the given operation.  e/0 denotes existence of an identity within R - \{0} under given operation.  inv denotes the existence of a inverse element for every element of the algebraic structure under the given operation.   inv/0 denotes the existence of a inverse element for every element R - \{0} under the given operation.  zdf denotes zero division free, see there.

|=====
|                    |      | + (ba)    | · (ba, d)
| semigroup          | +    |           | NA
| monoid             | +    | e         | NA
| group              | +-   | e, inv    | NA
| commutative group  | +-   | e, inv, c | NA
| ring               | +-·  | e, inv, c | [e/0]
| ring with identity | +-·  | e, inv, c | e/0
| commutative ring   | +-·  | e, inv, c | c
| zdf ring           | +-·  | e, inv, c | zdf
| field              | +-·/ | e, inv, c | zdf, c, e/0, inv/0
|=====


References:

- https://www.youtube.com/playlist?list=PLi01XoE8jYoi3SgnnGorR_XOW3IcK-TP6

- Book ``Algorithmics for Hard Problems: Introduction to Combinatorial Optimization, Randomization, Approximation, and Heuristics'', 2nd Edition, Juray Hromkovič, chapter ``2.2.4 Algebra and Number Theory''

- Book ``Design and Analysis of Randomized Algorithms'', chapter ``A.2 Algebra and Number Theory'' starting p. 239 bottom

=== Linear algebra

The _determinant_ of a square matrix A is denoted det(A) or |A|.

In the 2D case:

--------------------------------------------------
      |a b|
|A| = |   | = ad - bc
      |c d|
--------------------------------------------------

The geometric interpretation is that, when you think about the matrix representing a linear transformation, the absolute value of the determinant is the factor applied to an area (in the 2D case, volume in 3D case and so on).  Also, in the 2D case, if A is build by combining column vectors v1 and v2 side by side, the determinant is positive when v1 is clockwise from v2 (their tails coinciding), negative when v1 is counterclockwise, and zero when the two are colinear.



== Geometry

A _metric space_ M is an ordered pair (S, d) where S is a set and d is a metric on S.  A _metric_ (or _distance function_ or simply _distance_) is a function d that defines a distance between each pair of elements of a set S.  It is defined as d: S⨯S → ℝ~+~, where for all x,y,z ∈ S the following conditions are satisfied:

d(x,y) ≥ 0 [small]#(non-negatity)# +
d(x,y) = 0 ⇔ x = y [small]#(identity of indiscernibles)# +
d(x,y) = d(y,x) [small]#(symmetry)# +
d(x,z) ≤ d(x,y) + d(y,z) [small]#(triangle inequality)#

_triangle inequality_: Definition above. In other words, detours (two edges) are never shorter (in terms of d(·,·)) than the direct edge.

A _right angle_ is an angle of exactly 90° (π/2 radians).  Two vectors are _perpendicular_ iff their angle is a right angle, or equivalently, if their scalar product is zero.  A set of vectors is _orthogonal_ iff they are pairwise perpendicular.  A _normal_ vector of a point on a smooth surface is any vector perpendicular to the plane.

The _dot product_ (or _scalar product_ or _inner product_ (in Euclidean geometry)) of two vectors x⃗ and y⃗ is defined as x⃗·y⃗ = ∑x~i~y~i~ = ‖x⃗‖‖y⃗‖cos(θ).  The former variant is the algebraic interpretation, the later is the geometric interpration.  More concretely, the geometric interpretation is that x⃗·(y⃗/‖y⃗‖) is the projection of x⃗ onto y⃗, when the two vectors are placed so that their tails coincide.

The _cross product_ (or _vector product_ or _directed area product_ (in Euclidean geometry)) of two vectors x⃗ and y⃗ is defined as x⃗⨯y⃗ = ‖x⃗‖‖y⃗‖sin(θ)n⃗.  n⃗ is the unit vector normal to the plane containing x⃗ and y⃗.  By convention, the direction of n⃗ is given by the _right-hand rule_: The index finger represents x⃗, the middle finger y⃗, and the thumb x⃗⨯y⃗.  The maginitude of the cross product can be interpreted as the area of the parallelogram having x⃗ and y⃗ as sides: ‖x⃗⨯y⃗‖ = ‖x⃗‖‖y⃗‖sin(θ).  Cross product is zero ⇔ the lines are parallel. Cross product is positive (negative) ⇔ x⃗ is clockwise (counterclockwise) from y⃗ (their tails coinciding).



== Misc

=== Fibonacci sequence / numbers

reccurence relation: F~n~ = F~n-1~ + F~n-2~

closed form expression: F~n~ = (ϕ^n^ - ψ^n^) / √5 = [ϕ^n^ / √5], where
ϕ is golden ratio and ψ=1-ϕ, and [x] is the nearest integer function
(aka round function).

Note: lim~n→∞~ F~n~ / F~n-1~ = ϕ

Applications: Fibonacci heap


=== Golden ratio

ϕ = (1+√5)/2 ≈ 1.618…

Two quantities a and b are in the golden ratio ϕ iff a+b / a = a / b =
ϕ, i.e. a=ϕb

=== Factorial

reccurence relation: x! = x*(x-1) and 0!=1

stirlings approximation: n! ~ √(2πn)*(n/e)^n^



== References

- MIT course 6.042 "Mathematics for computer science".
  * spring 2015, index: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/course-index/
  * spring 2015, textbook: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/readings/MIT6_042JS15_textbook.pdf
  * fall 2010, video lectures: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/video-lectures/
  * fall 2010, readings: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/

- MIT course 18.650 Statistics for Applications, fall 2016: https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/[lecture notes], https://www.youtube.com/watch?v=VPZD_aij8H0&list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0[videos]

- Book ``Algorithmics for Hard Problems: Introduction to Combinatorial Optimization, Randomization, Approximation, and Heuristics'', 2nd Edition, Juray Hromkovič. The Introduction chapter serves as good summary of computer science fundamentals.

- Book ``Design and Analysis of Randomized Algorithms'', chapters ``A Fundamentals of Mathematics'' p. 227 and ``2.2 Elementary Probability Theory'' p. 20

- Book ``Modern Cryptography: Theory and Practice'' has a mathematical foundations part


== to-do

- skalarproduct
- greatest common divider/divisor
- log/exp relation to mul/div
- angle between vector
