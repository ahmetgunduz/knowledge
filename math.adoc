// The markup language of this document is AsciiDoc
:encoding: UTF-8
:toc:
:toclevels: 4


= Mathematics

Mainly mathematics tailored towards the nees of computer science.


== Misc basics

A _set_ is an unordered collection of distinct _elements_ (or _objects_, or _components_).
The notiation {a, b} defines a set consisting of elements a and b.
The _size_ (or _cardinality_) of a set S is the number of its elements, denoted as |S|.
A _sequence_ is an ordered collection of possibly duplicate elements,
The notation (b, a, a) defines a sequence.

The _powerset_ (or _power set_) of a set S is the set of all subsets of S, including the empty set and S itself.

[[permutation]]
A _permutation_ of a set S is a sequence containing every element of S exactly once.
The number of permutations of a n-element set is n!.

A (total) _function_ f : X → Y maps all elements of its input set X to elements of its output set Y.
Note a function is allowed to map multiple elements of X to a given element of Y.
The input set is called the _domain_.
The output set is called the _codomain_.
The set of elements in the codomain that appear in f's mappings is called the _range_ (or _image_).
A _partial function_ does not map all the elements of its input.
The term _total function_ is a synonym to `function' as defined above.
The term total function is typically only used when a disambiguition to partial function is needed.
Given a _injective function_, each element of Y is mapped to at most once.
Given a _bijective function_, each element of Y is mapped to exactly once.
Given a _surjective function_, each element of Y is mapped to at least once.
A _k-to-1 function_ maps exactly k elements of X to every element of Y.

A function is bijective if and only if it's both injective and surjective.


== Counting & Combinatorics

Typically we are typically good at determining the size of a set S of sequences.
Thus when faced with the problem of finding the size of a set P, we often try to define a set S of sequences and a function f : S → P.
Typically, once we defined the set S of sequences, the function f is obvious or implied by the definion of S.
Say we find a set of sequences S and a bijection function f : S → P, then the product rule gives us |S|, and the bijection property gives us |P| = |S|.
Likewise, say we find a k-to-1 function f : S → P, the product rule and the division rule give us |P| = |S| / k.

For example consider poker.
In poker a hand is 5 cards out of a deck of 52 cards.
Each card has a rank and a suit.
There are 13 ranks and 4 suits.

How many different hands have a four-of-a-kind?
We try to define a set S of sequences describing the problem, i.e. having a relation to the set P asked for by the problem statement.
A possible solution is that a sequence of S has the follwing 3 elements.
1) The first is the rank of the four card; 13 possibilities.
2) The second is the rank of the extra card; 12 possibilities.
3) The third is the suit of the extra card; 4 possibilities.
Due to the product rule, |S| = 13·12·4.
There's an obvious bijection from |S| to |P|.
Due to to bijective property, |P| = |S|.

How many diffrent hands have a two pairs; that is, two cards of one rank, two cards of another rank, and one card of a third rank?
We try to define a set S of sequences describing the problem, i.e. having a relation to the set P asked for by the problem statement.
A possible solution is that a sequence of S has the follwing 6 elements.
1) The rank of the first pair; 13 possibilities.
2) The suits of the first pair; C(4 2) possibilities.
3) The rank of the second pair; 12 possibilities.
4) The suits of the second pair; C(4 2) possibilities.
5) The rank of the extra card; 11 possibilities.
6) The suits of the extra card; C(4 1) possibilities. 
Due to the product rule, |S| = 13·C(4 2)·12·C(4 2)·11*C(4 1).
Since the first pair, i.e. element 1 and 2, and the second pair, element 3 and 4, are indistinguishable, there is a 2-to-1 mapping from |S| to |P|.
To be precise, it's 2 because there are 2! = 2 ways to permute an 2-element set.
Due to the 2-to-1 mapping and the division rule, |P| = |S| / 2.

_Pigeonhole principle_: If |A| > |B|, then for every total function f : A → B, there exists (at least) two diffrent elements of A that are mapped to the same element of B.
The _generalized pigeonhole principle_ states that if |A| > k·|B|, then every total function f : A → B maps at least k + 1 different elements of A to the same element of
B.

_Product Rule_: Given the sets A~1~, ..., A~n~.
Let A~1~ ⨯ ... ⨯ A~n~ denote the set of all sequences whose first term is drawn from A~1~, the second term is drawn from A~2~ and so forth.
|A~1~ ⨯ ... ⨯ A~n~| = ∏|A~n~|.
E.g. the number of different 3 digit hex numbers is 16·16·16.

_Division Rule_: If there is a k-to-1 function f : A → B, then |A|=k·|B|.

As a special case of the division rule: If there is a bijective function f : A → B, then |A| = |B|.

_Sum Rule_: Given disjoint sets A~1~, ..., A~n~, then |⋃A~n~| = ∑|A~n~|.

_Inclusion rule_: |A∪B| = |A| + |B| - |A∩B|.
Intuition: Just imagine the generic Venn diagram of sets A and B.

_Inclusion-exclusion rule_: Is a generalisation of the inclusion rule.
For the special case of three sets: |A∪B∪C| = |A| + |B| + |C| - |A∩B| - |A∩C| - |B∩C| + |A∩B∩C|. For the formula of the general case of n sets, the internet is your friend.

_Boole's inequality_: |A∪B| ≤ |A| + |B|. Intuition: Follows from the inclusion rule.

_Union Bound_: |⋃A~n~| ≤ |A~n~|. Intuition: Generalization of Boole's inequality.

_Monocity Rule_: If A ⊆ B, then |A| = |B| ≤ |B|.

_There are 2^n^ subsets of an n-element set_.
Proof: We define a sequence S from which there is a bijection to the problem set |P|.
The i-th element of the sequence S tells if element i of the original set is part of the subset or not.
The product rule gives |S|=2^n^, and the bijecton gives |P|=|S|.

A _k-combination_ of an n-element set S is a subset of k distinct elements of S.
The number of possible k-combinations is denoted by _C(n, k)_, pronounced `n choose k'.
Less concise formulated, it's the _number of k-element subsets of an n-element set_.
C(n, k) = n! / ((n-k)!k!).
Intuition: First we have n possibilities, then (n-1) and so on until (n-k+1).
That equals n! / (n-k)!.
So far we exactly have a k-permutation.
Since the order of those k elements doesn't matter, we have to devide by the number of permutations, which is k!.

C(n, k) = C(n, n-k)

C(n, 0) = C(n, n) = 1

_binomial theorem_ (aka _binomial expansion_): (x+y)^n^ = ∑~0≤k≤n~(C(n,k)·x^k^·y^n-k^). So C(n, k) is also called the _binomial coefficient_.

A _k-combination with repetitions_ (or _k-multicombination_, or _k-multisubset_) of an n-element set S is a multiset of k (possibly identical) elements of S.
The number of such k-multisubsets is denoted by \((n k)), pronounced `n multichoose k'.
\((n k)) = C(n+k-1, k).
Intuition, using the _stars and bars_ graphical aid.
Imagine the chosen multiset of elements ω~1~ as a group of stars, the chosen multiset of elements ω~2~ as another group of stars and so on.
More precisely, do it the following way.
You have a set of k+(n-1) positions.
Note that its a set, i.e. unordered.
The following visualizes it in an ordered manner, but conceptually it's unordered.
k positions are assigned a star, n-1 positions are assigned a bar.
The bars separate groups of stars.
For example for k=6 and n=3, a possible outcome is ★★|★★★|★.
Thus the original multicombination problem reduces to choosing a set of n-1 positions out of k+(n-1) positions in order to assign bars to.
C(k+(n-1), n-1) = C(k+(n-1), k) = C(n+k-1, k).
The first transformation is true due to the general rule C(n, k) = C(n, n-k).

A _k-permutation_ (or _variation_ or _partial permutation_) is a k-element sequence consisting of distinct elements out of an n-element set.
The nuber of possible k-permutations is denoted by _P(n,k)_ = C(n,k)*k! = n! / (n-k)!.
Intuition: First we have n possibilities, then (n-1) and so on until (n-k+1).
That equals n! / (n-k)! = C(n,k)*k!.

[[permutation_with_repetition]]
A _k-tuple_ (or _permutation with repetition_) is a k-element sequence consisting of (possibly identical) elements out of an n-element set.
The number of k-tubles of an n-element set is k^n^.
Intuition: First we have n possibilities, then again n, and so on, k times.

Overview denoting k-element entities and the number of such entities
given an n-element set (implies unordered and distinct):

|=====
|                    | without repetitions                | with repetitions
| subset (unordered) | k-combination, C(n, k)             | k-multicombination, C(n+k-1, k)
| sequence (ordered) | k-permutation, P(n, k) = C(n, k)k! | k-tuple, k^n^
|=====


Further typicall problems:

_bookkeeper rule_ (an inofficial term made up by the MIT): Given a k-element set {e~1~, ..., e~n~}, the number of sequences consisting of n~1~ e~1~, ..., n~k~ e~k~ is (∑n~i~)! / ∏(n~i~!).
Intuition, using the problem of finding the number of ways to rearange the letters in the word `bookkeeper'.
There are n~1~=1 b's, n~2~=2 o's and so on up to n~6~ r's.
I.e. k=6, but that is not really important.
There is a total of ∑n~i~ = 10 letters.
So there are 10! permutations of these letters.
However, we can't distinguish the n~2~=2 o's in each sequence, so we have to devide by 2!.
Likewise, we have to devide analogously for each of {b, o, k, e, p, r}.

Corollary to the bookkeeper rule: How many x-bit sequences contain y zeros? By the bookkeeper rule, n~1~ = y, n~2~ = x - y, thus x! / (y!·(x-y)!).

References:

- The above is largely based upon MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapter "Counting"


== Probability and Statistics

The _sample space_ S (or Ω) is the set of possible outcomes of an _experiment_.
An element ω ∈ S is called an _outcome_ (or _sample outcome_ or _element_ or _realization_ (is ambigous to the realization of a random variable)).
A subset E ⊆ S is called an _event_.
In other words, an event is a set of outcomes.
∅ denotes the _null event_ which is always false.
S denotes the _true event_ which is always true.
The set of `interesting' or `known' events is denoted 𝓕.
A _probability space_ (or _probability triple_) is the tripe (sample space S, set of events 𝓕, probability function Pr).
A _probability function_ (or _probability distribution_ or _propability measure_) Pr (or P or ℙ) on a sample space S is, a bit sloppily defined, a total function Pr : 𝓕 ⟶ [0, 1] having the following two properties:
1) Pr(ω) ≥ 0 for all outcomes ω ∈ S.
2) ∑~ω∈S~ Pr(ω) = 1.
3) Pr(E) = ∑~ω∈E~Pr(ω).
It's a sloppy definition because it enforces that 𝓕 contains every outcome.
A more precise definition is that a probability function is a total function Pr : 𝓕 ⟶ [0, 1] satisfying the three _probability axioms_ (or _Kolmogorov axioms_):
1) Pr(E) ≥ 0 for all events E ∈ 𝓕.
2) Pr(S) = 1.
3) If E~1~, E~2~, ... are disjoint then Pr(⋃E~i~) = ∑Pr(E~i~).
There are multiple notations denoting the evaluation of the function Pr: Pr(...) or Pr[...] or Pr{...}.
A finite probability space S is said to be _uniform_ if Pr(ω) is the same for every outcome ω ∈ S.
In an uniform probability space, Pr(E) = |E| / |S| for any event E ⊆ S.


_conditional probability_: The probability of event A given event B is known to be true is Pr(A|B) = Pr(A∩B) / Pr(B).
Pr(A) is also called the _prior probability_ of A and Pr(A|B) the _posterior probability_ of A.
Note that the order in time in which the events A and B occur does not matter.

Intuitively Pr(A|B) is the probability of event A when only considering the alternate sample space SB = B.

--------------------------------------------------
Areas are proportional to probabilities

  Sample space S      Pr(⋅|B) intuitively defines
                      a new sample space SB = B
           A
 S   whole 'column'
  +----+------+       Pr(A|B) = Pr(A∩B) / Pr(B)
  |    |      |       = Probability of A in sample space SB
  |    |      |
  |    |      |     SB
  +----+--+---+       +-------+---+
 B|       |   |      B|       |   |
  +-------+---+       +-------+---+
                               A∩B
--------------------------------------------------

_bayes theorem_: Pr(A|B) = Pr(B|A)Pr(A) / Pr(B).


_law of total probability_: Given a partition {B~1~, ...,B~n~} of the sample space S, then Pr(A) = ∑Pr(A∩B~i~) = ∑Pr(A|B~i~)Pr(B~i~).


[[independence]]
Two events A and B are _independent_ if Pr(A|B) = Pr(A) or if (Pr(B) = 0).
Or equivalently, called the _product rule for independent events_, iff Pr(A∩B) = Pr(A)Pr(B).
Note that disjoint does _not_ imply independent.
For example say A and B are disjoint and both are non-empty, then Pr(A|B) = 0 ≠ Pr(A).
Naturally independence is a symmetric relationship.
That's why we usually say `A and B are independend' rather than `A is independent of B'.
The form `Pr(A|B) = Pr(A) or if Pr(B) = 0' shows more clearly the meaning of `the occurence of B does not affect the probability of A'.
The form `Pr(A∩B) = Pr(A)Pr(B)' shows more clearly the symmetry of indpendence.

Informally stated, A and B are independend if the probability of A is independent of whether its relative to sample space S or when considering only the restricted sample space SB = B, _or_ vice versa for B.

--------------------------------------------------
Areas are proportional to probabilities

                   Pr(A|B) = Pr(A) or if (Pr(B)=0)
                   Informally: Ratio A∩B:B equals ratio A:S,
                   i.e. probability of A is independent of whether
                   its relative to SB or to S.
 S          A                      S          A
  +-------+---+                     +-------+---+
  |       |   |                     |       |   |
  |       |   |                     |       |   |
  |       |   |  SB                 |       |   |
  +-------+---+    +-------+---+    |       |   |
 B|       |   |   B|       |   |    |       |   |
  +-------+---+    +-------+---+    +-------+---+
                            A∩B
--------------------------------------------------

Example where A and B _are_ dependend:

--------------------------------------------------
Areas are proportional to probabilities

            A
  +-------+---+
  |       |   |
  |       +---+
  +-----+-+   |
 B|     |     |
  +-----+-----+
--------------------------------------------------


--------------------------------------------------
Areas are proportional to probabilities

            A        Pr(A∩B) = Pr(A)Pr(B)
  +-------+---+      Considering the above drawings,
  |       |   |      this can only be true if
  |       |   |      both of A and B can be drawn
  +-------+---+      with straight orthogonal lines,
 B|       |   |      in which case
  +-------+---+
--------------------------------------------------


To make that example more concrete, consider that blood can have a certain type and a certain rh factor.
Say the probability Pr(T) for type T is known, and the probability Pr(F) for rh factor F is known.
The previously described Venn diagram shows that the probability somebody has type T _and_ rh factor F equals Pr(T)Pr(F) _only_ if T and F are independent.
For independence, the ratio of people having rh factor F among all people (|F| / |S| = Pr(F)) must be equal to the ratio of people having rh factor F among those having also type T (|F∩T| / |T|).

The elements of α={A~1~, ..., A~n~} are _mutually independent_ iff Pr(⋂A~i~) = ∏Pr(A~i~) for _any_ subset of α.
Mutual independence does imply pairwise indpendence, but not vice versa.

The elements of α={A~1~, ..., A~n~} are _pairwise independent_ iff for all unordered pairs {A~i~, A~j~} of distinct elements (i.e. i ≠ j), A~i~ and A~j~ are independent.
Pairwise independence does _not_ imply mutual independence.

A _decision tree_ is a graphic tool for working with outcomes and events of an probability space.
The root is the start and is not directly associated a meaning.
Given a vertex, each outward edge represents that a given `subevent' occures.
The definition of an edge's associated subevent includes that the the subevent associated with the edge's source vertex has occured.
`Subevent' is an inofficial term made up by the author.
Each vertex thus represents the subevent that all subevents of the edges of the path from the root to that vertex have occured.
Note that the subevents on the path are not required to happen in the order implied by the path.
One just has to compute the correct _conditional_ probabilities of the edges.
Each outward edge of a vertex is assigned the conditional probability that the edge's associated subevent occures, given that the subevent associated with the vertex has occured.
For each internal vertex, the sum of the probabilities of all its outward edges is 1.
By the the above definitions, given a path, the subevents associated with the edges are independent, thus they can be multiplied to get the probability of taking that path.
Each leaf represents an outcome of the experiment.
Thus the set of all leaves represents the sample space.
I.e. there is a 1 to 1 relationship between the set of all leaves and and the set of all outcomes.

Alternatively, draw the tree using the treemapping method.
You start out with a rectangle representing the root vertex of the tree.
For each child, draw a line to create a subrectangle, the sizes of the subrectangles according to the weight of the edges. All llines mutually parallel.
Recurse.
At each new level in the recursion, toggle between horizontal and vertical lines.
The result has resemblance to a Venn diagram, only that here a given event is represented by a set of possibly disconnected areas, as opposed to a single connected area.

Recipe for solving many probability problems:

. Consequently follow the rules.
Don't try to be fast.
Often the human intuition is wrong.

. Define the sample space, i.e. all possible outcomes.

. Define events of interest.

. Compute probabilities (of required outcomes). Possibly the following way: Use the tree diagram method.  Assign a probability to each (required) edge.  Calculating the probability of an outcome is then trivial.

. Compute probability of your events, which is trivial, now that you have the probabilities of the outcomes.

References:

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapter "Probability"

- MIT course 18.650 "Statistics for Applications", Fall 2016, https://www.youtube.com/playlist?list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0[videos], https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/MIT18_650F16_Introduction.pdf[lecture notes]

- Book ``All of statistics'', chapter ``1 Probability''

- Khan Academy, ``Statistics'' playlist: https://www.youtube.com/watch?v=uhxtUt_-GyM&list=PL1328115D3D8A2566


=== Random variables and expectations

Formally a random variable is a function mapping from sample space to measure space, as defined in the following.  In practice, we often think of a random variable like a random number.  In practice, the sample space associated to a random variable is rarely explicitelly mentioned, but keep in mind that it really is there.  Random variables can be interpreted as link between data and sample spaces.

--------------------------------------------------
 probability space := (sample space S, events 𝓕, probability function Pr)

            probability
 set of     function Pr
 events 𝓕 =============> [0,1]
  ^
  |set of
  |subsets  random        measure
  |         variable R    space,      CDF_R(x) := Pr(R≤x)
 sample  ===============> mostly ℝ    ================> [0,1]  
 space S
                                      E[R] := ∫x· CDF_Rʹ(x)
                                      ----------------> measure space

                                      Var[R] := E[R-E[R]]²
                                      = ∫(x-E[R])²CDF_Rʹ(x)
                                      ----------------> measure space

   S is countable    discrete R       PMF_R(x) := Pr(R = x)
   set                                if R is the identity: PMF_R = Pr
                                      ================> [0,1]

                                      E[R] = ∑x·PMF_R(x) = ∑R(ω)·Pr(ω)

   S is infinit      continous R      PDF_R(x) = CDF_Rʹ(x) (informally)
   noncountable      PDF_R exists     Pr(a≤R≤b) = integrate PDF_R(x) over [a,b]
   set                                if R is the identity: PDF_R = Pr
                                      ================> [0,1]

                                      E[R] = ∫x·PDF_R(x)
--------------------------------------------------

A _random variable_ R is a measurable total function R : S ⟶ ℝ.
Technically the range of R is the _measure space_ E, but in computer science practice the measure space is mostly ℝ.
Roughly speaking, density functions exist only when the measuere space is ℝ.
The actually observed value of a random variable R is called _realization_ of R (or _observation_).
Note that the term `realization' is ambigously also used as a synonym for outcome ω ∈ S.
An _indicator random variable_ (or _Bernoulli variable_) is a random variable with codomain {0, 1}.
A random variable is _discrete_ if its domain is a countable set.
A random variable R is _continuous_ if there exists a probability density function for it.
Note that for a continuous random variable R, Pr(R = x) = 0 for every x.
We get a non-zero probability only in a non-empty range.

There's a strong relation between events and random variables.
Any assertion about the value of a random variable defines an event.
Say the random variable C counts number of heads in 3 coin flips.
The condition C = 1 defines the event {HTT, THT, TTH}, or the condition C ≤ 2 {TTT, HTT, THT, ...}.
Looking at it from the other direction, each event E is naturally associated with a corresponding indicator random variable I~E~, where I~E~(ω) equals 1 if outcome ω ∈ E and and 0 otherwise.

Given a random variable R with measure space ℝ, its _cumulative distribution function_ (or _CDF_ or _cumulative density function_) CDF~R~ (or F~R~) : ℝ ⟶ [0, 1] is defined as CDF~R~(x) = Pr(R ≤ x).

Given a random variable R with measure space ℝ, its _inverse CDF_ (or _quantile function_) is defined by CDF~R~^-1^(q) = inf{r: CDR~R~(x) > 1} for q ∈ [0, 1].
E.g. CDF~R~^-1^(1/2) tells you the x at which CDR(x) equals 1/2.
We call CDF~R~^-1^(1/4) the _first quartile_, CDF~R~^-1^(1/2) the _median_ (or _second quartile_) and CDF~R~^-1^(3/4) the _third quartile_.

_percentile_ is the same as quantile, only that it is in %, that is 100 times larger.

[[PDF]]
Given a continuos random variable R with measure space ℝ, its _probability density function_ (or _PDF_) PDF~R~ (or f~R~) : ℝ ⟶ [0, 1] is a function satisfying:

1) Pr(a ≤ R ≤ b) = ∫~a~^b^PDF~R~(x)·dx for every a ≤ b. +
2) Pr(x) ≥ 0 for all x. +
3) ∫~-∞~^∞^PDF~R~(x)·dx = 1.

Note that according to these rules a PDF, unlike a PMF, can be bigger than 1; it can even be unbounded. See also <<population>>.

[[PMF]]
Given a discrete random variable R with measure space ℝ, its _probability mass function_ (or _PMF_ or _probability function_) PMF~R~ (or f~R~) is defined as PMF~R~(x) = Pr(R = x).  See also <<population>>.

Both the probability density function and the cumulative distribution function capture the same information about the random variable, so take your choice.

PDF~R~(x) = CDFʹ~R~(x) at all points x at which CDF~R~ is differentiable.

CDF~R~(x) = ∫~−∞~^x^PDF~R~(x)·dx.

In sloppy notation, CDF~R~(-∞) = 0 and CDF~R~(∞) = 1.

A _univariate distribution_ is a probability distribution of only one random variable.  A _multivariate distribution_ is the _joint probability distribution_ of two or more random variables.

Two random variables R~1~ and R~2~ are _equal_ if R~1~(ω) = R~2~(ω) for all outcomes ω ∈ S.

Two random variables R1 and R2 are _equal in distribution_ if CDF~R1~(x) = CDF~R2~(x) for all x.
Note that equal in distribution does not imply equal.
E.g. consider X = `number of heads' and Y = `number of tails' in N fair coin tosses.

Two random variables R~1~ and R~2~ are _independent_ iff for all x~1~ ∈ codomain(R~1~), x~2~ ∈ codomain(R~2~), the two events [R~1~ = x~1~] and [R~2~ = 2~1~] are independent.

Random variables R~1~, ..., R~n~ are _mutually independent_ iff for all x~1~, ..., x~n~ the events [R~1~ = x~1~], ..., [R~2~ = x~2~] are mutually independent.
They are _k-way independent_ iff every subset of k of them are mutually independent.

A set of random variables is _independent and identically distributed_ (or _iid_ or __i.i.d.__) if all random variables are mutually indpendent and each random variable has the same probability distribution as the others.

Two events are independent iff their indicator variables are independent.

Let R and S be independent random variables, then f\(R) and g(S) are also independent random variables, where f and g are some functions.

Given a random variable R, then its _expected value_ (or _expectation_ or _mean_ or _average value_ or _first moment_, see also <<population_mean>>), denoted E[R] (or 𝔼\(R) or 𝔼R or μ or μ~R~ or by the use of on overline), is defined by:

E[R] = ∫x·CDFʹ~R~(x) +
If R is discrete: E[R] = ∑x~i~·PMF~R~(x~i~) = ∑~ω∈S~R(ω)·Pr(ω) +
If R is continuous: E[R] = ∫x·PDF~R~(x)

The _conditional expectation_ E[R|A] of a random variable R given event A is E[R|A] = ∑r·Pr(R=r|A).

[[variance]]
Given a random variable R, its _variance_ (or _mean square deviation_, see also <<population_variance>>), denoted by Var[R] (or 𝕍\(R) or 𝕍R or σ² or σ²~R~), is a measure of spread and is defined by

Var[R] = E[(R-E[R])²] = E[R²] - E[R]² = ∫(x-E[R])²CDFʹ~R~(x) +
If R is discrete: Var[R] = (∑x²~i~PMF~R~(x~i~)) - E[R]² +
If R is continuous: Var[R] = (∫x²PDF~R~(x)) - E[R]²

Note that an alternative measure of spread, thought much less often used than variance, is E[|R-E[R]|].

Given a random variable R, its _standard deviation_, denoted σ (or σ~R~ or sd\(R)), is defined by σ = √Var[R].

A set of random variables is called _homoscedastic_ if all of those random variables have the same finite variance.  This is also known as _homoscedasticity_ (or _homogeneity of variance_).  The complementrary notion is called _heteroscedasticity_.

The _covariance_ between two random variables R~1~ and R~2~ is defined as Cov[R~1~, R~2~] = E[(R~1~-E[R~1~])(R~2~-E[R~2~])] = E[R~1~R~2~] - E[R~1~]E[R~2~].

The _correlation_ between two random variables R~1~ and R~2~ is defined as ρ~R1,R2~[R~1~, R~2~] = Cov[R~1~, R~2~] / (√Var[R~1~]√Var[R~2~]). Note that the codomain is [-1,1].

E[a·R~1~ + b·R~2~] = a·E[R~1~] + b·E[R~2~] (_linearity of expectation_)

If R~1~, ..., R~n~ are mutually independent: E[∏R~i~] = ∏E[R~i~]

Var[R] = Cov[R, R]

Var[aR+b] = a²Var[R]

Var[R~1~ + R~2~] = Var[R~1~] + Var[R~2~] - 2Cov[R~1~, R~2~]

In general: Var[∑a~i~R~i~] = ∑∑a~i~a~j~Cov(R~i~,R~j~) = (∑a²~i~Var[R~i~]) + 2∑~j~∑~i<j~a~i~a~j~Cov[R~i~, R~j~]

If R~1~, ..., R~n~ are pairwise independent: Var[∑R~i~] = ∑Var[R~i~]

Cov[R, R] = Var[R]

Cov[R~1~, R~2~] = E[R~1~R~2~] - E[R~1~]E[R~2~]

If R~1~ and R~2~ are independent: Cov[R~1~,R~2~] = ρ~R1,R2~ = 0.

_Law of Total Expectation_: Let R be a random variable, and suppose that A~1~, ..., A~n~ is a partition of the sample space S, then E[R] = ∑~i~E[R|A~i~]·Pr(A~i~).

_Mean time to failure_: Given an event E and p = Pr(E), the number of independent experiments until E occures is 1 / p and the variance is (1-p)/p².

_Markov's inequality_: For non-negative R. Pr(R≥a) ≤ E[R] / a.

_Chebyshev's inequality_: Pr(|R-E[R]| ≥ a) ≤ Var[R]/a². Derived from Markov's inequality.

_Pairwise independent sampling_: Let R~1~, ..., R~n~ be pairwise independent random variables with the same mean μ and same deviation σ, and let S be their sum: Pr(|S/n-μ| ≥ x) ≤ 1/n σ²/x².

Given a sequence X~1~, ..., X~n~ of random variables.  X~n~, the last of the sequence, _converges in distribution_ (or _converges weakly_ or _converge in law_) towards the random variable X, denoted X~n~ D→ X (D above the arrow) or X~n~ ⇝ X, if lim~n→∞~ CDF~Xn~(x) = CDF~X~(x) ∀ x ∈ ℝ at which CDF~X~ is continuous.  An estimator is said _asymptotically Normal_ if (θ̂-θ)/se[θ] ⇝ N(0,1).  (*to-do* Is the term "asymptotically" as used in this sense really restricted to "assymptotically normal" and to estimators? I.e. can I say "assymptotically exponential" and most statisticans will feel confortable by such an usage. Def is from all of statistics, p. 92)

Given a sequence X~1~, ..., X~n~ of random variables.  X~n~, the last of the sequence, _converges in probability_ towards the random variable X, denoted X~n~ P→ X (P above the arrow) or plim~n→∞~ X~n~ = X, if for all ε > 0 lim~n→∞~ Pr(|X~n~ - X| > ε) = 0. Convergence in probability implies convergence in distribution.

_Weak Law of Large Numbers_ (or _WLNN_ or _Khintchine's law_): Let X~1~, ..., X~n~ be iid random variables with the same mean μ and same variance σ², and let X̄ = 1/n ∑X~i~ denote their sample mean. WLNN states that X̄ P→ μ. Interpretation: The distributionh of X̄ becomes infinitely concentrated, i.e. 0 variance, around μ as n gets large.  The sample mean is a consistent estimator for the population mean μ.  Note that while E[X̄] = μ and Var[X̄] = σ²/n are also true, they are different statements.

_central limit theorem_ (_CLT_):  Let the random variables X~1~, ..., X~n~ be independent, each X~i~ with some arbitrary unknown distribution but with known mean μ~i~ and finite variance σ²~i~.  Then (∑X~i~ - ∑μ~i~) / √∑σ²~i~ ⇝ N(0, 1), or formulated differently: 1/n ∑X~i~ ⇝ N(μ̄, σ̄²/n)  where μ̄ = 1/n ∑μ~i~ and σ̄² = 1/n ∑σ²~i~.  If additionally X~1~, ..., X~n~ are identically distributed with mean μ and variance σ², this simplifies to X̄ = 1/n ∑X~i~ ⇝ N(μ, σ²/n).  (*to-do* 1) better understand what http://mathworld.wolfram.com/CentralLimitTheorem.html says more 2) relation to `converges in distribution'? See all of statistics p 72 3) How do you call this thing on the lhs of ⇝? 4) Is it correct that I shouldn't use the term sample mean and thus also not the conventional X̄ = 1/n ∑X~i~ in the first general case, since the term sample is reserved for the case of taking a sample from a population, and by the definition, population means that its members have the same distribution. How you call 1/n ∑X~i~ in the first/general case? How you call (∑X~i~ - ∑μ~i~) / √∑σ²~i~ ?)

__WLNN vs CLT__: WLNN gives sample mean's value provided iid Xs.  CLT gives distribution of 1/n ∑X~i~ only provided independent Xs.  (*to-do* But then CLT is a proper superset of WLNN, since knowing the distribution implies knowing the mean. So the question remains, whats the real difference between CLT and WLNN?)

References:

- Book ``All of Statistics'', chapters ``2 Random Variables'' and ``3 Expectation''

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapters "Random Variables" and "Deviation from the Mean"


=== Important distributions


==== Comparison of distributions

*to-do*

References:

- http://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/


==== Uniform distribution

X ~ Uniform(a, b), where a < b, if

PDF(x) = { +
1/(b-a) for x ∈ [a, b]
0 otherise

CDF(x) = { +
0 for x < a +
(x-a)/(b-a) for x ∈ [a, b] +
1 for x > 0

==== Normal distribution / Gaussian distribution

X ~ N(μ, σ²), where μ∈ℝ is the mean and σ>0 the standard deviation, if

PDF(x) = 1/(σ√(2π)) exp(-1/(2σ²) (x-μ)²)

CDF(x) = Φ((x-μ)/σ)

We say that X has _standard Normal distribution_ if μ=0 and σ=1. Tradition dictates that a standard Normal random variable is denoted by Z.  The PDF and the CDF of Z are denoted by 𝜙(z) and Φ(z) respectively.

Φ(z) = 1/√(2π) ∫~-∞ to x~exp(-t²/2)dt = +
1/2 + 1/2 erf(x/√2)

erf(x) = 2/√π ∫~0 to x~exp(-t²)dt

Same useful facts:

X \~ N(μ,σ²) ⇒ (X-μ)/σ ~ N(0,1)

Z \~ N(0,1) ⇒ X = μ + σZ ~ N(μ, σ²)


==== Student's t-distribution

The _Student's t-distribution_ (or _t-distribution_) is the distribution of the sample mean where the population is normally distributed.  It is denoted t~ν~, where ν is its single parameter, the degrees of freedom.  More precisely: Let μ denote the population mean, X̄ the sample mean and S² the unbiased sample variance, then (X̄-μ)/sd̂[X̄] \~ t~n-1~, where sd̂[X̄] = S/√n, see estimator for standard error of the mean, and where t~n-1~ denotes a Student's t-distribution with n-1 degrees of freedom.

*to-do* I think that is not quite correct. It's just one of more possible use cases. After all many other statistics also have a t-distribution, no?

*to-do* list common statistics which follow a t-distribution (e.g. when statistic g1 follows a normal distribution and a scaling parameter depends on the data, e.g estimator sd̂[g1], then, under certain conditions, g2=g1/sd̂[g1] follows a student's distribution)

<<t_statistic>>
The _(Student's) t-statistic_ for an estimator β̂ of unknown parameter β is defined as t~β̂~ = (β̂ - β~0~) / sê[β̂], where β~0~ is a fixed value which may or may not match β.  β̂ must be normally distributed, which in case of OLS is the case if E[epsiolon]=0.  The t-statistic is commonly used in hypothesis testing, where the null hypothesis is that  be β = β~0~.  Typically β~0~ is 0.  If β̂ is an ordinary least squares estimator for a coefficient in the classical linear regression model, and if the true value of parameter β is equal to β~0~, then t~β̂~ \~ t~n-p~ where n is the number of observations, and p is the number of predictors (including the intercept).

Etymology: the term ``t-statistic'' is abbreviated from ``hypothesis test statistic''.

*to-do* I am confused. Here the denominator is se[β̂], in the t-distribution its sd̂[X̄] (the key point being that the later is an estimator).  Also apparently the Student's t-statstic is not guaranteed to be Student t-distributed, I find that confusing from a terminology point of view. How you call then the statistic used above in the definition of t-distribution?

*to-do* Also in <<t_test>> there multiple examples of t-statistics, all of which have as denominator an estimator, not se[...].  Only when we wanted a t-statistic for a t-test for a estimator β̂ of a OLS model coefficient β, we used t~β̂~ = (β̂ - β~0~) / se(β̂).


=== Testing

==== Hypothesis testing, basic idea and algorithm

A _statistical hypothesis test_ is a method of statistical inference.

A _two sided test_ (or _two tailed test_) is concerned with both regions of rejection, of the distribution.  A _one sided test_ (or _one tailed test_) is concerned with the region of rection for only one of the two tails of the distribution, and it states which one it is concerned with.

one sided vs two sided:

pro one sided test: higher power, i.e. less type II error rate.

*to-do* more pros & cons

Hypothesis test algorithm:

- Choose a suitable test statsistic T.  Compute its observed value t~obs~.

- Define the _null hypothesis_ and the complementary _alternate hypothesis_.  The null hypothesis (the hypothesis to be nullified), denoted H~0~, is a statement usually along the lines ``there is no relationship'' or ``there is no effect''.  The complementary alternate hypothesis is denoted H~a~ (or H~1~).  Note that in a one side test, H~0~ should not use =, but ≤ or ≥, while the complementary H~a~ then uses > or < respectively.  However it's mathematically still correct for the H~0~ to use = (*to-do* why is that?)

- Compute the p-value, see definition below.

- Choose a significance level α, see definition below.  Typically the significance level is chosen to be 5% or 1%.

- _Reject H~0~_ iff p-value < α.  Otherwise you _fail to reject H~0~_; you can't accept H~0~, see below.  An equivalent alternative criterion is to reject H~0~ when t~obs~ lies within the critical region, see definition below.

Hypothesis testing really is ``__proof by contradiction__''.  Only that we can't really proof or disprove anything,  since we only work with probabilities.  We only can gather evidence.  We start out assuming H~0~ is true and try to build a contradiction.  If we observe a t~obs~ such that p-value < α, then that is a `contradiction' to our assumption.  It's not a contradiction in a strict sense, but it's evidence that our assumption was incorrect.  In the other case, if p-value > α, we fail to build a contradiction, i.e. we fail to reject H~0~.  However we do not accept H~0~ either.  No conclusion can be drawn if you fail to build a contradiction.  The evidence is insufficient to support any conclussion about either H~0~ or H~a~.  Recall that we optained the p-value by assuming H~0~ is true, so we certainly can't derive from a p-value that H~0~ is true.

The _p-value_ (or _probability value_ or _asymptotic significance_) for a two sided test is Pr(T≥|t~obs~-E[T]| | H~0~), for a one sided test it is Pr(T≥t~obs~|H~0~) or Pr(T≤t~obs~|H~0~) respectively.  The interpretation of the p-value is: _Given_ H~0~ is true, then in (p-value)·100% of any hypothesis tests we see an result as extrem or more extrem than t~obs~.  I.e. _given_ H~0~ is true, in (p-value)·100% of these tests we would incorrectly reject the null hypothesis.  The p-value is _not_ the probability that either hypothesis is correct.  Regarding the case of a one sided H~a~, where the very unlikely case occures that t~obs~ is of on the `other' side of H~0~'s distribution:  then the p-value will be very large, and we will not reject H~0~, which is correct in that we didn't accept H~a~.

The _significance level_ (or _type I error rate_) α is the probability of rejecting H~0~ given that H~0~ is true.  Or equavilently, α is the area below the H~0~ distribution in the critical region.  α is choosen by the user, see algorithm above.

The _type II error rate_ β is the probability of not rejecting H~0~ given that H~a~ is true.  Or equivalently, β is the area below the H~a~ distribution in the acceptance region.  Note that the distribution of H~a~ is unknown. β = 1 - power.

The _power_ (or _statistical power_) of a test is the probability of making a true discovery, given that H~a~ is true.  I.e. it is the probability of rejecting H~0~ given that H~a~ is true.  Or equivalently, power equals the area below the distribution of H~a~ in the critical region.  power = 1 - β.

The _critical region_ (or _rejection region_):  In a two sided test the critical region is [-∞,t~crit_a~] ∪ [t~crit_b~,∞],  where crit_a and crit_b are defined via Pr(T≤t~crit_a~|H~0~) = α/2 and Pr(T≥t~crit_b~|H~0~) = α/2.  Or equivalently via the H~0~ distribution's quantile: t~crit_a~ = H0_dist_quantile(α/2) and t~crit_b~ = H0_dist_quantile(1-α/2).  In a onesided test its [-∞,t~crit~] where Pr(T≤t~crit~|H~0~) = α, or the other way round.  See also definition of significance level.

The _acceptance region_ is the complement to the critical region.

A _type I error_ (or _false positive_ or _false discovery_) is the incorrect rejection of H~0~.

A _type II error_ (or _false negative_) is the incorrect retaining of H~0~.  Recall that we should say ``failed to reject H~0~'', opposed to ``retained H~0~'', but in the previous sentence the double negation that follows from that wording would become confusing.

|=====
|                       | H~0~ really true | H~a~ really true
| failed to reject H~0~ | true positive | false negative, type I error, β
| H~0~ rejected         | false postive, type II error, false discovery, significance level α | true negative, true discovery, power
|=====


==== Multiple testing

The problem we're trying to solve here is this: If we make many hypothesis tests, each with significance level α, we're bound to make a false discovery α·100% of the times, because that's what significance level α says.  See also https://xkcd.com/882/ :-).

As in the case of finding the best expectedTestMSE, i.e. the best trade-off between increasing variance and decreasing bias, we now liked to find the best trade-off between inceease in type I error and increase in power.

_Classificaton of multiple hypothesis tests_: Consider m hypothesis tests. The following table defines variables counting how often each case occures. Upper case variables (U V T S and R) are random variables, lower case variables (m and m~0~) are fixed. The number of tests m is known, number of tests m~0~ where H~0~ is really true is unknown, the number of rejected H~0~ R is observable, the others are unobservable.

|=====
|                       | H~0~ really true | H~a~ really true | Total
| failed to reject H~0~ | U                | T                | m-R
| H~0~ rejected         | V                | S                | R
| Total                 | m~0~             | m-m~0~           | m
|=====

Q = V/R is the _false discovery proportion_ (_FDP_). By convention, if V = R = 0, then Q = 0.

The case of that H~0~ is always true, i.e. m = m~0~, is called the _gobal null_ (or _complete null_).

The _False discovery rate_ (_FDR_) is defined as FDR = E[Q] = E[V/R]. I.e. FDR is the expected proportion of type I errors (aka false discoveries) relative to all discoveries.

The _Famility wise error rate_ (_FWER_) is defined as FWER = Pr[V≥1].  I.e. FWER is the probability that we make an type I error (aka false discovery) at all.

α ≤ FWER ≤ mα +
FWER ≥ FDR +
FWER = FDR given global null +
FWER = 1 - (1-α)^m^  given global null and independend tests +
FWER ≈ αm given global null and independend tests and small α

A procedure offers _weak control_ at level α if FWER ≤ α holds is guaranteed only under global null.  A procedure offers _strong control_ at level α if FWER ≤ α holds always.  Note that here α denotes not the same thing as the significance level α of an individual test; here, it's the ``overall significance level''.

Techniques which control FWER: <<bonferroni_correction>>, <<westfall_young>>

Techniques which control FDR: <<bejamini_hochberg>>


===== No correction

*to-do*


[[bonferroni_correction]]
===== Bonferroni correction

Control of the FWER: goal is to get an FWER ≤ α.  Do each of the m individual tests at a significance level δ = α / m. As a result we get FWER ≤ α.

Neutral: Sensible if all tests are independent, because then FWER ≈ αm (assuming global null), see formulas after definition of FWER.

Contra: Can be too conservative (i.e. δ is smaller than needed), especially if the test statistics are positively correlated.  This is because the Boferroni correction assumes the worst case, which is mutually independent tests.  As an extreme example, under perfect positive dependence, there is effectively only one test, and thus we could choose δ = α and still have FWER = α, but instead we `needlessly' did choose δ = α / m.

Contra: As always wenn decrasing the siginificance level α, that comes at the cost of decreased statistical power, or equivalently, at the cost of increasing type II error rate.

*to-do* How much of the above applies to controlling FWER in general, and how much applies to Bonferroni in particular?


[[benjamini_hochberg]]
===== Benjamini Hochberg

Controls FDR.  *to-do*


[[westfall_young]]
===== Westfall Young permutation procedure

Weak control of FWER. Strong control of FWER under some assumptions.  Computes a significance level δ to be used for each test.

*to-do* what are these assumptions?

Two sample test based: m tests, n observations per test, two groups.  n ⨯ 1 vector Y = {0, 1}^n^ contains group assignment.  n ⨯ m matrix X contains observations.  Idea: under the global null y-values can be permuted.

Procedure:

* Repeat many times:

 ** For each test (i.e. for each X column), do a two sample test, compute p-value.

 ** Find minimum p-value of these m p-values, throw that minimum p-value into a collection of minimal p-values.

* Compute the empirical α quantile of the distribution given by the collection of minimal p-values, delivering δ.

* Use δ as significance level for each of the m tests.

References:

- Slides7.pdf


==== Misc. test properties / characteristics

_paramtetric test_: Assumes distribution family of the test statistics

_non-parametric test_ (aka _distribution free_): No assumpotions on the distribution of the test statistic.


==== One sample test

_one sample test_: Only one sample, only one test statistic, treat every member of the sample the same way. (*to-do* what is a accurate consice definition?)


==== Unpaired two sample test

_unpaired two sample test_ (or _independent two sample test_): Randomly partition the sample into two sets.  Or from an alternative viewpoint, start out with two independent samples.  Test treatment A on one set and treatment B (which might be `no treatment at all') on the other.

Disadvantage:

- The groups need to be really similar.  E.g. by chance the elements in either group might have something in common which has nothing to do with their treatment, but still influences the outcome of the test statistic.

- There might be a big variance in the test static.  E.g. if we measure how long people sleep, after treatment A and after treatment B: there is anyway a rather large variance in how long people sleep.   We don't want that variance to have an influence on our result.  In the paired two sample test, that variance cancels out in the step of building the difference.

Examples:

- parametric unpaired two sample tests:

  * <<z_test>> (assumes normal distr. with known variance)

  * <<t_test>> (assumes normal distr. with unknown variance)

- non-parametric unpaired two sample tests:

  * <<permutation_test>>

  * <<wilcoxon_rank_sum_test>>


==== Paired two sample test

_paired two sample test_  (or _paired difference test_ or _paired sample test_): Treat every element in the sample with treatment A and with treatment B (again, can be `no treatent at all').  The final test static is the difference of the `actual' test static static after treatment A and after treatment B.

Alternatively, we can match _match_ (or _pair_) every element in the treatment group with an element of the control group, the control group and the matching in a way that the matched pair shares similat observable characteristics.  Matching is however prominently critized.

*to-do* I don't see how the term two sample test still applies here -- the whole point is that its _not_ two samples

*to-do* Are the terms "paired difference test" and "unpaired two sample test" really refering to exactly the same thing?

*to-do* In case of matching, what is then the difference to unpaired two sample test?


[[permutation_test]]
==== Permutation test / randomization test

A non-parametric test.

General idea: use permutations to destroy the relationship that is to be tested under H~0~ while keeping all other relevant structure.

*to-do*: Also the lecture scripts list "Paired two sample test / one-sample test for symmetry" as an example (or examples?) for perumatation test.  I don't understand that.

t-test is an approximation to a permutation test.  Permutatin tests are known since long, but for a long time we didn't had the computational power to make them feasible, and as a consequence were forced to use approximations like t-test.  Nowadays permutation tests are feasible.

Pro: No parametric assumptions

Pro: Free to use any test statistic (as input to building the difference)

Pro: p-values and type I error control are exact if all permutations are considered. *to-do*: I don't understand. Also I can't make any sense out of what the slides from the lecture say: When the data come from H~0~, then the permutation distribution is the null distribution of the test statistic, and the permutation p‐value is the usual p‐value of the test statistic. This is all we need for type I error control, since we need to control the probability of a false
decision under H~0~.

Contra: Computationally expensive

Contra: Not everything can be formylated as permutation test. E.g. in linear regression, there is no straightforward permutation test for individual coefficients.


===== As unpaired two sample test

Given population F~1~ and F~2~, and a sample from each, Y~1~^(1)^,...Y~n1~^(1)^ \~ F~1~ and Y~1~^(2)^,...Y~n2~^(2)^ \~ F~2~. H~0~: F~1~ = F~2~ (i.e. treatment has no effect), H~a~ : F~1~ is a shifted version of F~2~ (either in a two tailed or one tailed way).

- *to-do* What are properties of a good test statistics? Concrete examples from lecture where ranks sum of group1, median(group1) - median(group2).  It seems often to be same sort of difference.  Note that rank sum of group1 is also sort of a difference.  It must be a function where the following permutation has no effect under H~0~.

- For all possible permutations, or repeatedly for a permutation selected uniformely at random from all possible permutations: compute t~obs,i~, where i denotes the i-th iteration.  We can permute since under H~0~, i.e. F~1~ == F~2~, permuting doesn't make a difference.

- The set of t~obs,i~ s form the conditional distribution of test statistic T given the data, also calle the _permutation distribution_.  Compute t~obs~ using the (original) sample.  Compute the p-value using t~obs~ and the obtained permutation distribution.

*to-do* add or replace with alternative version where instead an combinedsample we have sample1 and sample2 seperately.

*to-do* why those + 1 in calculating pvalue?

------------------------------------------------------------
  combinedsample <- ... # sample1 concatenate sample2
  n1 <- ... # size of sample1
  repetitioncount <- ... #

  # function underlying test statistic T
  g <- function(combinedsample, n1) { ... }

  g_on_permuted_sample <- function(combinedsample, n1) {
    n <- nrow(combinedsample)
    permutedcombinedsample <- combinedsample[sample(1:n, n, replace=F)]
    return(g(permutedcombinedsample));
  }

  t.obs.all <- replicate(repetitioncount, g_on_permuted_sample(combinedsample, n1))
  t.obs <- g(combinedsample)
  pvalue <- (sum(t.obs.all<=t.obs)+1) / (repetitioncount+1)

  hist(t.obs.all)
  abline(v=t.obs)
------------------------------------------------------------


===== As paired two sample test

Same concept as before. However as in any paired two sample test, we no longer have two populations and thus two samples.  We have one single sample from one population.  As in any paired two sample test, let test statistic T be the difference between test statistic U~1~ and U~2~.

*to-do*

------------------------------------------------------------
  g <- function(sample) { ... }

  g_on_permuted_sample <- function(sample) {
    n <- nrow(sample)
    signs <- sample(c(-1,1), n, replace=T)
    sample.new <- signs * sample
    return(g(sample.new));
  }
------------------------------------------------------------


===== Example: Correlation

H~0~: no relationship between X and Y. Thus under H~0~, we can permute the Y-values (or the X-values).

As test statistic, we can for example use a rank correlation test statistic, for example Spearman's rank correlation coefficient.


===== Example: Correlation / Linear regression

Y = β~0~ + β~1~X~1~ + ... + β~p~X~p~ + ε

H~0~: β~0~ = ... = β~p~ = 0

Under H~0~, we can permute the Y-values to destroy the relationship between Y and X~1~, ..., X~p~.

*to-do* more detailed


==== Comparison of tests

*to-do* flow chart with all the test: t-test, z-test, Wilcoxon, the Wald, .... Overview with pros and cons. E.g. http://health.uottawa.ca/biomech/courses/apa3381/hyp_test.pdf


[[z_test]]
==== Z-test

A _Z-test_ is any statistical hypothesis test in which the test statistic follows approximately a Normal distribution under the null hypothesis.  Because of the central limit theorem, many test statistics are approximately normally distributed for large samples.

Examples: see those of Student's t-test. Only that in an Z-test, we know the variance σ² of the population, or have a good enough estimator for it, which is often the case for large samples.  So e.g. building on t-test's example of a one sample test, see below, we just would change the test statistic to z = (x̄ - μ~0~) / sd[x̄], which is standard Normal distributed.  Recall that sd[x̄] = σ/√n, see standard error of the mean.


[[t_test]]
==== Student's t-test

A _Student's t-test_ (or simply _t-test_) is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis.

_As one sample test_:  Given one sample with sample mean x̄.  We hypothise that μ~0~ is the population mean and want to test that.  Let μ denote the (true) population mean and S² the unbiased sample variance.  H~0~: μ = μ~0~.  As test statistic we use the t-statistic t = (x̄ - μ~0~) / sd̂[x̄], where sd̂[x̄] = S/√n, see also estimator for standard error of the mean.  Under H~0~ it's distribution is t~n-1~.

_As unpaired two sample test_:  Given two samples of equal size, one treated with treatment A and the other with treatment B (no effect / controll).  We want to test whether treatment A has an effect.  Let s~p~ denote the <<pooled_variance>>, X~A~ a statistic on sample A and X~A~ the same statistic on sample B.  H~0~: X~A~ = X~B~.  As test statistic we use the t-statistic t = (X~A~ - X~B~) / s~p~√(2/n).  Under H~0~ it's distribution is t~2n-2~.  *to-do* Is my H~0~ correct?

_As paired two sample test_:  Given one sample, for each member, we calculate the difference of some test statistic after treatment A and after treatment B (no effect / controll), see also paire two sample test.  We want to test wether treatment A has an effect.  Let n denote the sample size, X~D~ the average of the differences and s²~D~ the variance of the differences.  H~0~: X~D~ = 0:  As test statistic we use the t-statistic t = (X~D~ - μ~0~) / sd̂[X~D~], where sd̂[X~D~] = s~D~/√n, see also estimator for standard error of the mean.  Under H~0~ its distsribution is t~n-1~.  *to-do* Is my H~0~ correct?

_Linear regression_, testing wether a coefficient has an effect: see <<linear_regression_models>>


*to-do* See also Wilcoxon, The Wald test


==== The Wald test

*to-do*


[[wilcoxon_rank_sum_test]]
==== Wilcoxon rank sum test (or Mann-Whitney U test)

===== As a non-parametric unpaired two sample permutation test

Set-up: Given population F~1~ and F~2~, and a sample from each, Y~1~^(1)^,...Y~n1~^(1)^ \~ F~1~ and Y~1~^(2)^,...Y~n2~^(2)^ \~ F~2~. H~0~: F~1~ = F~2~, H~a~ : F~1~ is a shifted version of F~2~ (either in a two tailed or one tailed way).  Let the test statistic U be the sum of ranks of group 1 (or group 2, doesn't matter).  The distribution of U under H~0~ is well known, the same way say the Student's distribution is known.  For small sample sizes (~20), it's given by tables, for large sample sizes it can be approximated by a Normal distribution.

Procedure: Regular hypothesis test. Compute u~obs~ from the given sample, and from u~obs~, using the known distribution of U, the p-value.

Pro: No parametric assumptions

Pro: Robust, because the sum of ranks of group 1 statistic is robust.  E.g. if the largest value in a sample gets even larger, the mean would change, but the sum of ranks doesn't.

Pro: Better than t-test if distributions are non-Normal.  Recall that t-test requires Normal distributions. (*to-do* bette in what sense? Because of less constraints? Or because of better power if t-test is used despide that distributions are not Normal?)

Neutral: Power almost identical to that of t-test if distributions are Normal.

Pro: The null distribution (i.e. U under H~0~) is independent of F~1~ and F~2~.


*to-do* also present the other variants: paired two sample test, unpaired two sample test, repeated measurements on a single sample


=== Misc statistic

==== Formula table

[cols="1,3"]
|=====
| 𝔉 = { f(x;θ) : θ ∈ Θ }  | Parametric model
| Pr~θ~[·],  E~θ~[·],Var~θ~[·]  | Probability is with respect to PDF/PMF f(x;θ)
| N | Size of population
| n | Size of sample
| μ | Population mean
| μ̂ = x̄ | Common estimator for μ
| σ² | Population variance
| σ̂² = S² | Common estimator for σ²
| x̄ = 1/n ∑x~i~ | Sample mean
| S² = 1/(n-1) ∑(x~i~-x̄)² | Unbiased sample variance
| 1/n ∑(x~i~-x̄)² | Biased sample variance
| se[·] = sd[·] | Standard error of a statistic = standard deviation that statistic
| SEM = se[x̄] = sd[x̄] = σ / √n | Standard error of the mean, assuming independence and same variance σ²
| SEM̂ = sê[x̄] = S / √n | Common estimator for se[x̄]
| g(X~1~, ..., X~n~) | Statistic: Result of function g on a random sample
| θ̂ or θ̂~n~ | Estimator for quantity θ. Estimator = a statistic plus stating which quantity is estimated.
| Bias~θ~[θ̂] = E~θ~[θ̂] - θ = E~θ~[θ̂ - θ] | Bias of estimator θ̂ with respect to θ
| θ̂ is said to be consistent if θ̂ P→ θ |
| MSE[θ̂] = E~θ~[(θ̂-θ)²] |
| MSE[θ̂] = Bias²~θ~[θ̂] + Var~θ~[θ̂] |
| (X̄-μ) / se[X̄] ~ 𝓝(0,1) | For random variables {X~i~:i∈[n]} iid ~ 𝓝(μ, σ²)
| (X̄-μ) / sê[X̄] \~ t~n-1~ | For random variables {X~i~:i∈[n]} iid ~ 𝓝(μ, σ²)
|=====


==== Population & Sample

[[statistical_parameter]]
A _statistical parameter_ is a numeric characteristic of a population or statistical model.  Typically unkown. Often denoted using Greek letters.

[[population]]
A _(statistical) population_ is the same as the <<PDF>> / <<PMF>>.  So a population can be finite or infinite.  That's my personal definition.  Commonly the population is defined as the set of all possible observations of a random variable.  Personally I find that misleading. At least such an definiton should add ``where different observations having the same value are still different members in the set''.

A _population parameter_ is a specialication of <<statistical_parameter>> describing a numeric characteristic of a population. Often unobservable because the population is to large to evaluate every member.  Prominent examples are population mean μ and population variance σ².

[[population_mean]]
The _population mean_ μ is a population parameter and is the same as the expectation of the corresponding distribution.  A common estimator for the population mean is the sample mean X̄.

[[population_variance]]
The _population variance_ σ² is a population parameter and is the same as the variance of the corresponding distribution.

In general a _sample_ is a `subset' (however elements might be repeated) of a population optained through _sampling_.  Sampling is some process of selecting members of the population, possibly randomly, possibly based on a certain criteria.

A _(simple random) sample_ (_SRS_) is a set of n random variables X~1~, ..., X~n~ iid~ P, where P is some population.  Often a simple random sample is also defined as a subset of the population, drawn uniformly with replacement.  However that important part ``with replacement'' is unfortunately often omitted.

A _statistic_ is a numeric characteristic of a sample, as explained in detail in chapter <<statistic>>.

The _sample mean_ (or _empirical mean_), denoted X̄, of a sample X~1~, ..., X~n~ is the arithmetic mean, as defined below.  Is a statistic, i.e. a random variable.  The sample mean is a consistent estimator for the population mean μ, by the LLN.

X̄ = 1/n ∑X~i~ +
X̄ P→ μ +
E[X̄] = μ +
Var[X̄] = σ²/n

The _unbiased sample variance_ (or _Bessel-corrected sample variance_), denoted S², is definied as follows.  Is a statistic, i.e. a random variable.  Can be used as unbiased estimator for the population variance.

S² = 1/(n-1) ∑(X~i~-X̄)² +
E[S²] = σ²

Similarily, the _biased sample variance_ is defined by 1/n ∑(X~i~-X̄)².  Is a statistic, i.e. a random variable.

[[pooled_variance]]
Given k samples of k populations with common variance σ² and possibly different means.  Let s²~i~ denote the unbiased sample variance of the i-th sample, and n~i~ the size of the i-th sample.  The _pooled variance_ (or _combined variance_ or _composite variance_ or _overall variance_) is the weighed average of the individual unbiased sample variances, weighed by (n~i~-1): s²~p~ = (∑^k^(n~i~-1)s²~i~) / (∑^k^(n~i~-1)).  In the special case of k=2 and n~1~ = n~2~,  s²~p~ = (s²~1~+s²~2~)/2.  The pooled variance s²~p~ can be used as unbiased estimator for the common populaton variance σ².


[[statistic]]
==== Statistic

A _statistic_, often denoted T (or T~n~), is a function, often denoted g, which has a sample X~1~, ..., X~n~ as its domain. Formally: T = g(X~1~, ..., X~n~).  Thus a statistic is a random variable since it depends on the random sample X~1~, ..., X~n~ of the population.  In other words, a statistic is an attribute of a sample.  Unfortunately the term statistic can mean two things.  The term statistic can mean the random variable as described before, in which case it's often denoted uppercase T.  The term statistic can also mean the _observed value_ (or _realized value_) of that random variable, in which case it's often denoted lowercase t (or t~obs~).  Prominent examples are sample mean and (unbiased) sample variance.

The _sampling distribution (of a statistic)_ (or _finite-sample distribution_) is the probability distribution of a given statistic.  Recall that a statistic is a random variable, and thus has a distribution.  If we would take infinitly many same sized samples and calculate the statistic each time, we would get the sampling distribution.

The _standard error_ (or _SE_) of a statistic is defined by the standard deviation of that statistic, i.e. by the standard deviation of its distribution.  Standard error can be used to compute confidence intervals.  The 95% confidence interval for some variable a is approximately mean(a) ± 2SE(a), assuming a is normal distributed. (*to-do* verify those  numbers again / give more examples of confidence intervals, e.g. 1%)

If the statistic is the mean, the standard error is called the _standard error of the mean_ (_SEM_) and is defined as follows.  However the population variance σ² is seldom known, thus the SEM is often estimated via estimating the population variance σ² by the unbiased sample variance S².

SEM = se[x̄] = sd[x̄] = +
σ / √n (if indpendent and same variance σ²)

SEM̂ = sê[x̄] = S / √n

Proof for sd[x̄] = σ / √n if independent and same variance σ²:  Var[x̄] = Var[1/n ∑x~i~] = 1/n² Var[∑x~i~] =(independent) 1/n² ∑Var[x~i~] =(same variance) 1/n² n Var[x~i~] = Var[x~i~] / n = σ² / n.

*to-do* ISLR p. 65 says that SE can be use to estimate how far off a single μ̂ might be from the true μ. But then the SE doesn't make sense if we calculate it on the basis of the population, since there we know μ exactly. Similarily, why is SE independent of the ratio populationsize:samplesize?

*to-do* what is done in the R script from lecture week 2?


==== Estimator

An _estimator_ (or _point estimator_ or _(point) estimate_), denoted θ̂ (or θ̂~n~), of a parameter θ, is technically a statistic g(X~1~, ..., X~n~) plus conceptually stating which paramater θ its an estimator of.  In other words, an estimator θ̂ is a single ``best guess'' of parameter θ.  An estimator is a random variable since a statistic is one, see there.

The _bias_ of an estimator θ̂ with respect to an unknown parameter θ is defined as Bias~θ~[θ̂] = E~θ~[θ̂] - θ = E~θ~[θ̂ - θ].  An estimator with zero bias is called _unbiased_.  Otherwise the estimator is said to be _biased_.  Note that there's also an analogously defined bias for the estimate f̂ of an regression function f, see there.

An estimator θ̂ with respect to an unknown parameter θ is said to be _consistent_ if: θ̂ P→ θ.

If bias[θ̂]→0 and se[θ̂]→0 as sample size n→∞, then estimator θ̂ is consistent.

[[MSE_of_estimator]]
The _mean squared error_ (or _MSE_) of an estimator θ̂ with respect to an unknown parameter θ is defined as follows. The MSE can be used to assess the quality of the estimator θ̂. Note that there's also an analogously defined MSE for the estimate f̂ of an regression function f, see there.

MSE[θ̂] = E~θ~[(θ̂-θ)²] = +
Bias²~θ~[θ̂] + Var~θ~[θ̂] (see also <<bias_variance_trade_off>>)

Recall that an estimator is a statistic and thus a random variable, so the _mean_ and the _variance_ of an estimator are defined the usual way.


==== Confidence Interval & Prediction Interval

Let C~n~ = (a,b) denote a 1-α _confidence interval_ for an unknown parameter θ, where a and b are statistics, and where 1-α is called the _confidence level_ (or _coverage_ of the interval).  A 1-α confidence interval is an interval such that in (1-α)·100% of the times you make an 1-α confidence interval for some parameter,  possibly each time for another parameter, the interval contains the true parameter.  See next paragraph for further explanations.  Common choices for the confidence level are 95% or 1%.

Note that a 1-α confidence interval does _not_ mean that given a realized interval there is a 1-α probability that it contains the true parameter.  The probability statement is about the interval which is defined by the statistics a and b, i.e. random parameters.  The probability statement is not about the fixed unknown parameter θ.  No probability statement concerning its value may be made.  (*to-do* 1) I don't get the difference.  What's the consequence whether (a,b) are random and θ is fixed or vice versa?  If you are given a 1-α confidence interval and the game is to predict whether it contains the true parameter, what percentage of your bet must the casino give you in order for the game to be fair?  At least in this example, I think it doesn't make a difference.  2) See forumula (6.9) on p. 92 in book "all of statistics". I'd say its _not_ P~θ~, its P~a,b~  3) See also Example 6.14 p. 93 in Book "All of statistics")

_prediction interval_: *to-do*


==== Statistical Model

A _statistical model_ 𝔉 is a set of distributions or regression functions (*to-do* but regression functions are quite a different thing than distributions; I don't understand). A _parametric model_ is set 𝔉 that can be parameterized by a finite number of parameters: 𝔉 = { f(x;θ) : θ ∈ Θ}, where θ is an _parameter_, or vector of parameters, that can take values in the _parameter space_ Θ. f is a function of x, parameterized by θ.

*to-do* clean up relation to <<statistical_parameter>>.

There's an loose distinction between parameters determined during fitting the model and _hyper-parameters_ which are determined before fitting the model, e.g by the user or during the higher level process of model selection.  You may think of splitting complete set of parameters into two subsets.  The values of the subset labeled parameters is computable cheaply when being provided with the values of the subset labeled hyper-parameters. References: https://stats.stackexchange.com/questions/149098/what-do-we-mean-by-hyperparameters?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa (*to-do* isn't a further difference that hyper-paremeters can influence the number of parameters, such as in polynomial regression?)

A _tuning parameter_  if the parameter's job is primarily a transient parameter of the learning algorithm.  Tuning parameters are also called hyper-parameters, conflicting somewhat the previous definition.  (*to-do* 1) But in this sence, a hyper 2) Clean up that parameter - hyper-parameter - tuning parameter mess)

The notations Pr~θ~[·],  E~θ~[·] and Var~θ~[·] mean that the probability is with respect to PDF/PMF f(x;θ), i.e. averaging over all possible observations x, the generating PDF/PMF being f(x;θ). (*to-do* in the context of an estimator θ̂, what if θ is not a parameter of a model, but some other population parameter)

p denotes the number of predictors and n the number of data points.  Predictors and data points will be defined shortly.  Given is a n ⨯ p matrix X where each column represents a _predictor_ (or _covariable_ or _covariate_ or _input variable_ or _independent variable_ or _feature_ or _regressor_ or _explanatory variables_ just _variable_).  Given is a n ⨯ 1 vector Y of _response variables_ (or just _reponse_ or _output variables_ or _dependent varables_).  The tuple (Y[i], X[i-th row]) represents the i-th _observation_ (or _data point_).  We assume that there is some fixed but unknown relationship between the response Y and the predictors X.  We model that by the _regression function_ f (or _population regression function_ or _PRF_) by writing Y ≈ f(X). This can be read as ``__is approximately modeled as__'' or Y _is regressed_ on X.  f represents the _systematic_ information that the predictors provide about the response.  ε is a n ⨯ 1 vector of a random _error terms_ (or _noise_ or _disturbance_), which are independent of X and have no systematic error (i.e. E[ε~i~] = 0 ∀ ε~i~).  We liked to have an estimate f̂ for f and use it like so Ŷ = f̂(X).  Ŷ is the resulting _prediction_ for Y.  The elements ŷ~i~ of Ŷ are called _fitted values_ (or _predicted values_).

*to-do* better merge the above paragraph with the first few paragraphs of this chapter

In general, we can use regression only for prediction of a response variable given new predictors.  In general the observiations on which the regression is based do not allow for conclusions about causal relations. (*to-do* Some reference to a trusted source which concisely accurately states this)

The variance Var(ε~i~) of the error terms ε~i~ is in general not known.  Often it is assumed that all error terms have the same constant variance σ²,  and that constant variance often is estimated via σ̂ = RSE.  Note that the error terms are in direction of the y axis, as opposed to perpendicular to a linear regression hyperplane.  This is important to note because the later is what most humans intuitively do in the 2D case when guessing which of multiple regression lines is a better fit.

e~i~ = y~i~ - ŷ~i~ is the i-th _residual_.

_Studentized residual_ (*to-do* what are standardized residuals? They appear in R plots) t~i~ = e / sê[e]. Can be used to dedect outliers, see there.

The _residual sum of squares_ (or _RSS_) is defined as RSS = |e|² = ∑~1≤i≤n~e²~i~.  Can be thought of as the amount of variability that is left unexplained after performing the regression.

The _total sum of squares_ (or _TSS_) is defined as TSS = |y-ȳ|² = ∑~1≤i≤n~(y~i~-ȳ)².  Can be thought of as the amount of variability inherent in the response before the regression is performed.

See more statistics and definitions in <<measuring_model_performance>>.

[[trainingsampe_testsample_notation]]
Notation: In Pr~training~[·], E~training~[·], Var~training~[·], Bias~training~[·] etc. the sample space is the set of all possible training samples taken from the population.  Each training sample trains the estimate f̂.  Thus f̂, or more specically its estimated coefficients β̂, are random variables with a sample space as described before.  In Pr~test~[·], E~test~[·], Var~test~[·], Bias~test~[·] etc., the sample space is the set of all possible test samples taken from the population.

|=====
| n | Number of samples
| p | Number of predictors
| X | Predictors. Given by being part of the given data set.
| Y | Responses. Given by being part of the given data set.
| ε | Error terms. Unknown.
| Often: Var[ε] = const = σ² | σ is in general not known. Often assumed to be constant.
| Often: Var̂[ε] = σ̂² = RSE² | Common estimator
| f | Regression function. Is guessed.
| Y ≈ f(X) | ``Approximately modeled as'' or ``X is regressed on Y''.
| Y = f(X) + ε |
| f̂ | Estimate for f
| Ŷ = f̂(X) | Predictions (or fitted values)
| e = Y - Ŷ | Residuals
| t~i~ = e / sê[e] | Studentized residuals. For sê[e] see your specific model.
| TSS = ∑(y~i~ - ȳ)² | Total sum of squares. ȳ is the sample mean, see there.
| RSS = ∑e²~i~ | Residual sum of squares
|=====

References:

- Statisitic Cheat Sheet: http://web.mit.edu/~csvoss/Public/usabo/stats_handout.pdf


=== Model selection, model validation

[[measuring_model_performance]]
==== Measuring model performance

===== Formula table

|=====
| t~i~ > 3 | Rule of thumb for identifying outliners
| p~ii~ > 2p̄ or p~ii~ > 3p̄  | Rule of thumb for identifying high-leverage data points, where p~ii~ is a diagonal cell in the projection matrix P and p̄=p/n is the mean leverage value.
| D~i~ = 1/p · t²~i~ · (P~ii~/(1-P~ii~)) = 1/(pσ̂²) · ∑~j~(ŷ~j~-ŷ~j(i)~)² | Cook's distance of i-th observation.  P is the projection matrix.  ŷ~j(i)~ excludes the i-th row.
| D~i~ > 1 or D~i~ > 4/n | Rules of thumb for identifying influencial data points.
| RSE = √(RSS/(n-p)) | Residual standard error
| (unadjusted) R² = (TSS - RSS) / TSS = 1 - RSS/TSS |
| adjusted R² = 1 - RSE² / (TSS/(n-1)) |
| trainingMSE = RSS/n |
|=====


===== Model performance measures

The _performance_ of a model is a measure of how `good' a model models a given population, most often in respect to its predictive power, i.e. its prediction capability on independent unseen test data.  For example, one could use the expected test MSE, that is, an estimate thereof.  (*to-do* is it always a property of the model, or can it also be a property of a concrete already trained estimate f̂? In what way does the difference between prediction and inference influence the topic of model performance)

The _residual standard error_ (or _RSE_) is given by RSE = √(RSS/(n-p)).  The RSE is considered an absolute measure of the lack of fit of the model to the data.  Roughly speaking RSE is the average amount that the response will deviate from the true regression hyperplane.  Even if the model were absolutely correct and the parameters of the model were known exactly, any prediction Ŷ is still off by RSE.  RSE is often used as an estimator for the variance Var(ε~i~) ≈ σ̂² = RSE² of the error terms ε~i~. *to-do* derive given formula from √Var[e] or whatever is the basis, and then write RSE = √Var[e] = √(RSS/(n-p)).

The __R²__ statistic is defined as R² = (TSS - RSS) / TSS = 1 - RSS/TSS.  It provides an relative measure of the lack of fit of the model to the data.  TSS - RSS can be thought of as the amount of variability in the response that is explained by performing the regression.  R² then measures the proportion of variability in the response that can be explained using X.  R² ∈ [0,1], 1 meaning good, 0 meaning bad.  Small values might occur becaus the used model (e.g. linear) is wrong or the inherent error σ² is high, or both.  The advantage of R² over RSE is that the former is relative (lies in [0,1]) and the later is absolute.

The __adjusted R²__ statistic is defined as 1 - RSE² / (TSS/(n-1)).

Recall: The notation E~training~[·], Var~training~[·], E~test~[·], Var~test~[·] etc. is defined by <<trainingsampe_testsample_notation>>.

The __bias of estimate f̂__ with respect to regression function f __at x~0~__ is defined analogous to the bias for an estimator θ̂, see there. f̂(x~0~) is a random variable since f̂, i.e. its parameters, depends on the random training data.

Bias~training~[f̂(x~0~)] = E~training~[f̂(x~0~)] - f(x~0~)

Bias~training,test~[f̂] = E~test~[Bias~training~[f̂(x~test~)]]

*to-do* how is variance of f̂ formally defined ?

Var~training~[f̂(x~0~)] = ...?

Var~training,test~[f̂] = E~test~[Var~training~[f̂(x~test~)]

*to-do* confidence interval for f̂

A _loss function_ L(Y, Ŷ) measures the error between Y and Ŷ = f̂(X). Typical choices are squared error (Y - Ŷ)² or absolute error |Y - Ŷ|.

The _training error_ Err~training~ of a model is the average loss over the training sample.  Can theoretically be used as estimator for the expected test error, but it generally would be a rather bad one since rather biased, see also *to-do*.

Err~training~ = 1/|trainingsample| ∑L(y~training,i~, f̂(x~training,i~))

The _test error_ (or _generalization error_) of a model is the expected loss given a training sample.  It is defined as follows.  f̂ is fixed and was trained by the given training sample.  The stated two variants are equivalent.  In the 1st variant the randomness lies in repeatedly randomly picking a test sample from the population.  y~test,i~ and x~test,i~ correspond to the i-th observation in that random sample set and thus are fixed values.  In the 2nd variant the randomness lies in X and Y being random variables.

Err~test~ = +
E~test~[1/|testsample| ∑L(y~test,i~, f̂(x~test,i~))|f̂] or +
E[L(Y, f̂(X))|f̂]

[[expected_test_error]]
The _expected test error_ (or _expected prediction error_) is the expected loss.  For a concrete example see <<expected_test_mse>>.  In respect to test error, now also the training sample is choosen at random from the population.

Err = E~training~[Err~test~]

Regarding `the' _MSE_, there are multiple variants, depending on what exactly we want to describe.  The basic idea is always the same, but depending on the specific MSE variant, it is calculated over different data and the used estimate f̂ is either fixed or varies by repeteadly training it with some data. See also MSE for an estimator θ̂, which is analogously defined.

The _training MSE_ is calculated using the training data and a fixed estimate f̂ which was trained using that training data:

trainingMSE = RSS/n.

[[expected_test_mse]]
The _expected test MSE_ is conceptually calculated in two levels. One level iterates over all possible training data sets, each iteration training a new estimate f̂.  For each of those f̂, the other level iterates over all possible test data sets. See also <<expected_test_error>> for the more general concept.

expectedTestMSE = E~training~[E~test~[(y~test~ - f̂(x~test~))²]] = +
E~test~[expectedTestMSE(x~test~)]

The __expected test MSE at x~0~__ is analogous, but here we only look at a fixed x~0~.  In the following formula, the inner E[·] is for the random variable Y~0~.   Note that the Y~0~ corresponding to x~0~ is a random variable due to the error term ε.

expectedTestMSE(x~0~) = E~training~[E[(Y~0~ - f̂(x~0~))²]]

[[bias_variance_trade_off]]
The _bias-variance trade-off_ (or _bias-variance dilemma_) states that that the expected test MSE at x~0~ can always be decomposed into three parts as as follows.  One important point is that we cant get rid off the irreducable error Var[ε].  Recall that we saw the same pattern also with an estimator.  It is a general principle in statistics that when bias decreases, then variance must increase and vice versa which is not directly captured by the following formula.  As model flexibily increases, bias decreases, variance increases, and the expectedTestMSE will be convex, i.e. have a U-shape.  Thus the goal is to find the model with minimal expectedTestMSE.

expectedTestMSE(x~0~) = (Bias~training~[f̂(x~0~)])² + Var~training~[f̂(x~0~)] + Var[ε]

Yes: If Var goes down, bias must go up, and vice verca. This is a principle, not a proof, in the context of our lecture.

*to-do*: What is "test MSE" (opposed to expected test MSE)?

*to-do*: Is all of the above about MSE & bias truly acurate? Be picky! E.g. I suspect I use non-standard / unusual notation.


[[assess_model_coefficients]]
===== Assess model coefficients

The _bias of model parameter estimator β̂_ is defined the usual way the bias of an estimator is defined: Bias~training~[β̂] = E~training~[β̂] - β

_t-statstic_ for an estimator β̂ of unknown parameter β: see <<t_statistic>>.

In a linear regression model the coefficents β̂ found by OLS are <<BLUE>>.


===== Assess data points

An _outlier_ is a data point for which its response y~i~ is unusual by being far from the value predicted by the model. Alternatively: A data point with large studentized residual.  Observations whose studentized residuals are greater than 3 in absolute value are possible outliers [ISLR chapter 3.3.3 Potential Problems, Section 4. Outliers].  In linear regression, typically an outlier has only a small influence on the regression hyper-plane.  However it may have a big influence on RSE and R².  And since RSE is used as estimator for σ, also a big influence on confidence intervals and p-values, i.e. a big influence on the interpretation of such a fit (*to-do* are those statements restrictued to linear regression?).

A data point with high _leverage_ is one for which its predictor is unusual by being far away from the mean of the predictors.  Regarding linear regression, given projection matrix P, the leverages are defined as diag(P). Recall that P (also denoted H) only depends on the predictors X, and that Ŷ = PY, i.e. ŷ~i~ = p~i1~y~1~ + ... + p~ii~y~i~ + ... + p~in~y~n~.  You see from this formula that the leverage p~ii~ quantifies the influence the response y~i~ has on its predicted value ŷ~i~.  When having a high-leverage data point, the lack of neighboring predictors means that the fitted regression model will pass close to that particular observation.  As a rule of thumb, a leverage value greater than 2p̄ (other authors say 3p̄) is considered large, where p̄=p/n is the mean leverage value.  (*to-do* 1) I only understand that for simple linear regression, but not confidentaly for multiple linear regression. Is figuratively `far away' the Eucledian distance in ℝ^p^?. ISLR has an example: Figgure 3.13, middle plot, p.98. Has the red predictor higher leverage than the predictors at the right/left border of the ellipse? Its closer in Eucledian distance to the center/mean.  2) The notion of outlier seems to be applicable not only to linear regression, but the notion of leverage seems only to be applicable to linear regression. Correct? Why this asymetry? If not, what is the general definition / formula for leverage? 3) There seem to be two intuitions for leverage: "a measure how far of a predictor is from the predictor mean" and "a measure of the influence of a response on its predicted value", and I cant bring them together in my head 4) I don't understand how high leverage by itself is a problem.  If I have high leverage but a tiny outlineingless, at least in linear rergession with OLS nothing bad at all happens, no?).

[[influencial_data_point]]
An _influencial data point_ is one whose deletion would noticeably change the calculation. In particular, in regression analysis it has a large effect on the parameter estimates. In other words, a measure of how influencial a data point is, is a measure of the effect of deleting that data point.  One possible measure is the <<cooks_distance>>.  Note that outliers and high-leverage data points have the potential to be influencial, but they not necessarily are influencial. For models with two parameters, a possible way to visually identify influencial data points is to do n `experiments', each removing the i-th data point and then fit the model using the remaining data points, and then draw a scatter plot of the two optained parameters of each `experiment'  (e.g. β~0~ on the x axis and β~1~ on the y axis).  All points should be close together.  References: https://onlinecourses.science.psu.edu/stat501/node/337.  (*to-do* If an data point both is an outlier and has high-leverage, is it guaranteed to be influencial or only very likely to be influencial? According to cooks distance, it is guaranteed to be influencial, no?)

[[cooks_distance]]
The _Cook's distance_ D~i~ is a commonly used estimate of the <<influencial_data_point,influence>> of the i-th data point when performing least-squares regression analysis.  In an OLS analysis it can also be used to indicate regions of the design space where it would be good to obtain more data points.  The Cook's distance is defined as D~i~ = 1/p · t²~i~ · (P~ii~/(1-P~ii~)) = 1/(pσ̂²) · ∑~j~(ŷ~j~-ŷ~j(i)~)², where t~i~ is the i-th studentized residual, and ŷ~j(i)~ excludes the i-th row.  If the `outlineniness' (middle term t²~i~) is high and the leverage (last term) is large then the Cook's distance is large and thus the data point is deemed influencial.  Thresholds for identifying highly influential data points are controversal.  One is D~i~ > 1, another is D~i~ > 4/n. (*to-do* 1) I still don't understand the summation definition. Why is the nominator not mostly zero? 2) Has a given concrete value of the Cook's distance an interpretation, or is it just qualitative, large is bad, small is good? In the later case, why square studentized residual and why not simply use leverage P~ii~). References: https://onlinecourses.science.psu.edu/stat501/node/340


[[model_selection]]
==== Model selection

_Model selection_ (or _model tuning_) is the task of selecting a statistical model from a set of candiate models.  That may include determining the hyper-parameters of the choosen model and it may include determining the tuning parameters of the learning algorithm.  (*to-do* clean up this terminolgy mess)

Whether or not model selection shall additionally also train the selected model is not clearly defined (*to-do* really?)

Model selection is typically done by computing the estimated expected test error of a candidate model on _validation data_.  Validation data is the same as test data, but in the context of model selection instead model validation.


==== Feature selection

_Feature selection_ (or _variable selection_, _attribute selection_, _variable subset selection_) is the task of selecting a subset of the available predictors (aka features).

*to-do* pros & cons of choosing more / less predictors. Is there already something in this document?


==== Model validation

_Model validation_ (or _model assessment_ or _assessing performance (of a model)_) is the task of calculating the performance of a final model.  Final model means one whose hyper-parameter are already determined, e.g. by model selection.  Note that it's about a model, as opposed to a single given already trained estimate f̂.  For a meaningfull model validation we usually also need to calculate the bias and the variance of the calculated performence.

The problem of model validation is that for most ways to perfectly calculate a performance we often would need all possible test data to train multiple estimates f̂ and for each of those f̂ all possible test data.  See also definition of expected test MSE.

.Use previously unseen data for model validation or model selection

This is not realistic.  Even so, in that case we could combine the original data and the newly available data into one data set, and be logically at the same point as in the beginning of the problem statement.

.Use original data for model validation or model selection

When we use the original data for model assessment, the retreived performance will be biased. That's because we trained the model with exactly the same data as we measured the model's performance with.  It was the model trainings job to fit the model to the original data, so obviously the model will have a high performance on the original data.

When we use the original data for model selection, then (*to-do* Claude said that overfitting occures - but the term overfitting applies only to a single model training, no?).  The procedure would be: Calculate the performance for each of the m candidate models using the complete original data and then choose the candidate model which had the best performance.


[[validation_set_approach]]
==== Validation set approach

The _validation set approach_ is a technique for model assessment or model selection.

Partition data randomly in two equaly sized partitions, one constituting the training data and the other constituting the test data (or validation data).

Pro: Simple

Pro: Fair estimate of test MSE (*to-do* but below we say the estimate will be biased, isn't that a contradiction?)

Contra: Fewer training data always always means a worse fit of the model.  In particular it typically means more bias.  In other words, it's too pessimistic: we get a biased estimate (e.g of expected test MSE).

Contra: Large variance of the validation estimate (e.g. estimated expected test MSE) because the validation estimate might depend a lot on how the partition turned out to be.

References:

- Book ``An introduction to statistical learning'', chapter ``5.1.1 The Validation Set Approach''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4.3.1 Leave-one-out cross-validation''


==== Cross Validation

_Cross-validation_ (or _rotation estimation_) is a coarse technique for model assessment or model selection.  It's a coarse technique in the sense that it has multiple more concrete instanciations, such as <<LOOCV>> and <<k_fold_CV>>.  It tries to mitigate the problem of only having one data set (aka sample of the population) of finite size, altough we actually would like infinitely many training sets and test sets, each set of infinite size.

_CV for model validation_: Repeatedly partition the original data set into a training set and a test set, each time doing the partition in a different way.  In each iteration, calculate an estimate of the performance of the model using the training set and the test set of that iteration. At the end, average the results, delivering the (final) estimate of the model's performance.

_CV for model selection_: Analogous to before.  In each of the iterations we calculate the estimate of the performance for each of the candidate models.  At the end, we select the candidate model with the best averaged estimate of the performance.  As noted in chapter model selection, whether or not the selected model shall also be trained is not clearly defined.  If we want to train, we can train it on the complete original data set, or just take the already trained model which was trained on a subset.  The former is computationally more expensive, but larger training sets are generally better.  Note that we cannot use the optained performance estimates also for model validation, see also <<double_cross_validation>>. (*to-do* are the statements about training true?)

*to-do* define terms overfitting, underfitting.

*to-do* Chapter/paragraph about choosing a statistic for model performance. e.g. why exactly is training MSE not good.

*to-do* compare and contrasts these terms: (statistical) learning method, model, estimate f̂ of f, systematic information f. More usually used terms, especially for f̂ and f?

*to-do* Is all of the this chapter correct, inclusive following subchapters? Be picky!

References:

- Book ``An introduction to statistical learning'', chapter ``5.1 Cross-Validation''

- Book ``All of statistics'', chapter ``13.6 Model Selection''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4 Cross-Validation''


[[double_cross_validation]]
==== Double cross validation

_Double cross validation_ (or _nested cross validation_) lets you do model selection _and_ model validation using a single original data set in a reasonable way.  Note that here `model' validation delivers the performance of the following procedure, _not_ the performance of a model.  The procedure is: given a data set, model selection is done on that data set via cross validation.

Note that we cannot do model selection with cross validation and then use the same data set for model assessment, because the assessment would test a model with data that the model already has seen.  We would thus get a biased, i.e. too optimistic, i.e. too small estimated expected test error.

Outer cross validation: `Model' assessment: Repeatedly partition the original data set into a test set and a training-valiation set.  In each of these outer iterations, pass the training-valiation set to the the inner CV which returns a trained model.  Calculate the estimated test error of that trained model using the test set.  At the end, averaging all those delivers the estimated expected test error of the precedure.  Recall that each trained model the outer layer sees was trained with slightly different training data than every other, thus overall we get this E~training,test~[...] the estimated expected test error demands.

Inner cross validation: Model selection: Do normal model selection via cross validation on the training-validation set received from the outer CV.  Return the trained selected model received from model selection.

Note that different inner cross validations may select different models.  That's ok, since as said at the beginning of this chapter, we asses the performance of the procedure, not of some model.


[[LOOCV]]
==== Leave one out cross validation (LOOCV)

A specialization of cross validation.  Do n iterations. In each iteration, make the i-th observation the test set, and the rest the training set.

MSE~i~ = (y~i~ - f̂^(-i)^(x~i~))².  LOOCV estimate of expected test MSE is 1/n ∑MSE~i~.

*to-do* The training of the finally used model is by the full original data set, right? And the above calculated MSE is the estimated MSE of that final model?

Pro: Much less bias than validation set approach due to larger training set

Pro: No randomization

Cons: Computationally expensive, especially for large n. However for some models, e.g. for linear regression, there are short cuts.

References:

- Book ``An introduction to statistical learning'', chapter ``5.1.2 Leave-One-Out Cross-Validation''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4.3.1 Leave-one-out cross-validation''


[[k_fold_CV]]
==== k-fold cross validation

A specialization of cross validation.  Split observation randomly in k equaly sized partitions.

*to-do*

References:

- Book ``An introduction to statistical learning'', chapter ``5.1.3 k-Fold Cross-Validation''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``4.3.2 K-fold Cross-Validation''


[[model_training]]
=== Model training

_Estimating f by paramtetric methods_: We assume f to be a parameterized function.  The parameters β of f are called _unknown parameters_ (or _coefficients_).  The problem of estimating f reducues now to computing estimators β̂ for the coefficients β.  For example the linear model has f̂(X) = Xβ, where β is a p ⨯ 1 vector constituting the unknown parameters.  We use the training data to _fit_ (or _train_ or _estimate_) our model, i.e. to compute estimates β̂ of the unknown parameters β.  Phrased as noun, this is _model fitting_ (or _model training_ or _model estimation_).  There are multiple approaches for fitting the model, the most common approach being (ordinary) <<least_squares>>.


[[least_squares]]
==== Least squares regression

_Least squares_ (or _LS_) is a method of fitting a model which tries to minimize the RSS.  In other words, it is a method for computing the estimators β̂ of the model coefficients β.  Let β̂^LS^ (or β̂, since LS is kind of the standard) denote the estimator, as defined by LS, of the coefficients β of the model:

β̂^LS^ = argmin~β~(RSS(β)) , using given training data

There are two categories: _Ordinary least squares_ (or _OLS_ or _linear least squares_) and _non-linear least squares_.  Linear least squares has a closed-form solution, see <<linear_regression_models>>.  The nonlinear problem is usually solved by iterativie refinement.

[[BLUE]]
_Gauss-Markov theorem_: In a linear regression model with uncorreleated error terms ε, constant finite variance Var[ε~i~] = σ² and E[ε] = 0, the coefficents β̂ found by OLS are _BLUE_ (_best linear unbiased estimator_).  Here, `best' means Var~training~[β̂] is minimized compared to other unbiased estimators, and unbiased means Bias~training~[β̂] = 0.

*to-do* But that only works when the population regression function f (i.e. the true model) is linear, right? YES. Else also the linear function based on the coefficient β has bias. What is the appropriate terminology to phrase all this?

*to-do* From BLUE follows that Bias~training~[f̂] = 0? NO. That is, only if I choose all the predictors I should pick. I.e. expectedTestMSE = (Bias~training~[f̂])² + Var~training data~[f̂] + Var[ε]?

Contra: Var[β̂^LS^] is high in case of <<multicollinearity>>. <<shrinkage_method>>s try to solve this problem.


==== Subset selection

Method for selecting a subset of all the available predictors

*to-do*

*to-do* link to model selection

*to-do* crosslink with shrinkage methods

References:

- - Book ``An introduction to statistical learning'', chapter ``6.1 Subset Selection''


[[shrinkage_method]]
==== Shrinkage Methods / Regularization Methods

_Shrinkage methods_ (or _regularaization methods_) try to solve the problem of <<multicollinearity>>.

*to-do* The wiki page to ridge regression sais its about ill posed problem, not multicollineaity as I wrote.

[[multicollinearity]]
_Multicollinearity_ describes the situation that a predictor can be linearly predicted from one or more other predictors.  In this situation, when using OLS, Var~training~[β̂] is hight, i.e. there's a higher chance of the estimator β̂ being far away from the real β.  Recall that in OLS, the estimators β̂ are unbiased.  Consider a model with two predictors X~1~ ≈ X~2~.  Then f̂(x) = β̂~0~ + β̂~1~X~1~ + β̂~2~X~2~ ≈ β̂~0~ + β̂~3~X~1~, where β̂~3~ = β̂~1~ + β̂~2~. If there was only one predictor as in the rhs of ≈, we could calculate β̂~3~ well. But in the model with two predictors, OLS doesn't know how to distribute β̂~3~ among β̂~1~ and β̂~2~ such that β̂~1~ + β̂~2~ = β̂~3~.  E.g in one training data sample, β̂~1~ will be large and β̂~2~ will be small, in the other training data sample the other way round, in yet another training data sample it's evenly distributed etc.  <<shrinkage_method>>s try to solve this problem.

References:

- Book ``An introduction to statistical learning'', chapter ``6.2 Shrinkage Methods''


[[ridge_regression]]
===== Ridge regression

_Ridge regression_ (or _Tikhonov regularization_ or _weight decay_) is a <<shrinkage_method>>. λ≥0 is a tuning parameter.

β̂^ridge^ = argmin~β~(RSS(β) + λ|β^(0)^|²), using given training data

Where |β^(0)^| denotes the L2-Norm (aka Eucledian distance) of β excluding the intercept β~0~, i.e. |β^(0)^|² = ∑~1≤i≤p~β²~i~.

*to-do* equivalent formulation which has nice geometric interpretation

*to-do* what is the benefit of ridge / lasso? After all it's just yet another parametered model, and as always, due to bias variance tradeoff, the more flexible the model the less bias but the more variance.

The rational for excluding the intercept β~0~ from being regularized is that we want to shrink the association of each predictor with the response.  The intercept is just a measure of the mean value of the response when predictors X = 0.

The second term, λ|β|², is also called _shrinkage penalty_.  Its effect is that of _shrinking_ the coefficient estimates towards zero.  The tuning parameter λ serves to control the amount of shrinkage.  λ=0 means no shrinkage, resulting in β̂^ridge^ = β̂^LS^.  λ→∞ means infinite shrinkage, resulting in β̂^ridge^ = 0.

References:

- Book ``An introduction to statistical learning'', chapter ``6.2.1 Ridge Regression''


[[ridge_regression]]
===== Lasso regression

A <<shrinkage_method>>.  See also subchapter <<ridge_expression>>, where common topics are discussed.  λ≥0 is a tuning parameter.

β̂^lasso^ = argmin~β~(RSS(β) + λ‖β^(0)^‖~1~), using given training data

*to-do* equivalent formulation which has nice geometric interpretation

Where ‖β^(0)^‖~1~ denotes the L1-Norm (aka Eucledian distance) of β excluding the intercept β~0~, i.e. ‖β^(0)^‖~1~ = ∑~1≤i≤p~|β~i~|.

*to-do*

References:

- Book ``An introduction to statistical learning'', chapter ``6.2.2 The Lasso''


=== Models


[[linear_regression_models]]
==== Linear regression models

<<misc_statistics>> introduces many terms and notations this subchapter is based upon.  In the linear multiple regression model the relation between the response and the predictors is linear, i.e. f(X) = Xβ.  Thus the estimated model is f̂(X) = Xβ̂ and thus Ŷ = f̂(X) + ε. Often the first column of X is all ones, which then makes β~0~ the _intercept_: the hyperplane described by f̂ intercepts the y-axis at β~0~.  _Simple linear regression_ is linear regression with only one predictor variable. f(X) = Xβ defines the _population regression hyperplane_, which is the best linear approximation to the true relationship between X and Y (recall that linearity was just an assumption). f̂(X) = Xβ̂ defines the _least square hyperplane_, which is an estimate based on the training data (*to-do* Is there are more general term which does not imply the fitting method).

The method to fit the model is commonly ordinary <<least_squares>>.  Other methods include <<ridge>> and <<lasso>>.

*to-do*: the predictors can also be functions of actual predictors, i.e. with that the linear model is actually more general

Assumptions of the linear model (*to-do* of what exactly are these assumptions? Linear model and/or least squares fitting?), roughly ordered after importance: *to-do* how to verify?


- E[ε] = 0, i.e. no systemtatic error. (the linear model is the correct model. *to-do* what has E[ε] = 0 to do with linear model being the correct model?)

  * As a consequence, we have E[β̂] = β (unbiased).

  * If this assumption does not hold, we need other models than the linar model.

- Predictors X are exact, i.e. observed without erors.

  * If this assumption does not hold, we may can use Errors-in-variables models.

- Cov[ε] = σ²I~n×n~, i.e. the errors are uncorrelated. That implies homoscedasticity of the error terms ε.

  * As a consequence we have Cov[β̂] = σ²(X^T^X)^-1^.

  * If the error terms ε are not homoscedastic we may can use weighted least squares.

  * If the error terms ε are correlated we may can use generalized least squares.

- {ε~i~|∀i} iid \~ 𝓝(0, σ²).  This implies that Y~i~ ~ 𝓝 ∀ i, and it implies constant variance σ² of the error terms. (*to-do* since here we say they are independent, doesn't that imply they are also uncorrelated. I.e. the previous point is actually implied by this point? Ok, they are only uncorrelated given they are independent if second moments are finite (sais wikipedia in `uncorrelated random variables'), but is that really the point here?)

  * As a consequence, we have β̂ ~ 𝓝(β, σ²(X^T^X)^-1^).

  * If asummption does not hold, we may can use robust methods (*to-do* which ones?) instead least squars.

- The matrix has full rank p < n.

  * If that is not fulfilled, then we simply cannot solve the equations delivering β.

_Projection matrix_ (or _influence matrix_ or _hat matrix_), denoted P (or H), *to-do*

_F-test_. See also <<multicollinearity>> *to-do* Where do I find good information? What is it in general, what are the general definition? What is it in case of linear regression via OLS. What about partial F-Test. I find not much in the script nor in ISLR, these are too close to specific small examples / applications. Also understand well why it is possible to get significant F-statistic but non-significant regressor t-tests. Make a paragraph about correlated predictors. What if the predictors are correlated, but still a plane is well defined, since I have at least three point clouds, e.g. a triangle function. https://stats.stackexchange.com/questions/3549/why-is-it-possible-to-get-significant-f-statistic-p-001-but-non-significant-r

_Choosing predictors_: The individual variabilities for each coefficient sum up and the variability of the estimated hyper-plane increases the more predictors are entered into the model, whether they are relevant or not. See also bias-variance trade-off.


|=====
| P = X(X^T^X)^-1^X^T^; p~ij~ = Cov[ŷ~i~,y~j~]/Var[y~j~] | Projection matrix *to-do* formula for p~ij~ using only x's. That would help to see what it mathematically means for a predictor to be far away from the mean of predictors.
| P^T^ = P | I.e. P is symmetric
| P² = P | I.e. P is idempotent
| PX = X | X is invariant under P
| diag(P) | Leverages of observations
| ∑p~ii~ = p |
| p~ii~ ∈ [1/n,1] |
| Y = f(X) = Xβ + ε | β and ε unknown, see however assumptions on ε.
| Ŷ = Xβ̂ = PY  |
| e = Y - Ŷ = (I~n~ - P)Y | Residuals.
| Cov[e] = σ²(I~n~ - P) | Residuals e are correlated (error terms ε are not)
| Var[e] = diag(Cov[e]) = σ²(1-diag(P)) | Variance of residuals is _not_ constant (unlike variance of error terms ε).
| sê[e] = se[e] = √Var[e] | (*to-do* is this absolutely correct? why/proofs?)
| e / sê[e] = e / (σ√(1-diag(P))) | Studentized residual (*to-do* from exercis-lecture, but really studentized or standardized?)
| OLS: β̂ = argmin~β~(RSS(β)) = (X^T^X)^-1^X^T^Y | Coefficient estimates, computed using OLS. https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares#Least_squares_estimator_for_.CE.B2[Proof]
| E[β̂] = β |
| Cov[β̂] = σ²(X^T^X)^-1^ | Note that in some notations Var instead of Cov is used; since it's a matrix, it's implicit that the covariance matrix is meant. https://stats.stackexchange.com/questions/68151/how-to-derive-variance-covariance-matrix-of-coefficients-in-linear-r| Cov̂[β̂] = RSE²(X^T^X)^-1^ | σ is unknown, we estimate it as usual using RSE
| Var[β̂] = diag(Cov[β̂]) | Usual definition
| Var̂[β̂] = diag(Cov̂[β̂]) | Usual definition
| sê[β̂] = √Var̂[β̂] | Usual definition
| β̂ \~ 𝓝~p~(E[β̂], Var[β̂]) |
| (β̂~j~ - β~j~)/sê[β̂~j~] \~ t~n-p~ | *to-do* valid always or only under some hypothesis?
| t-value of β̂~j~: β̂~j~ / se[β̂~j~] \~ t~n-p~ | *to-do* se[β̂~j~] typically not known, so I can use sê[β̂~j~], no? But then, when I want to be precisise, I have to distinguish between "t-value of β~j~" and "estimated t-value of β~j~", where as the latter is more often used?
| β̂~j~ ± sê[β̂~j~]t~1-α/2,n-p~ | 1-α confidence interval for β~j~
| (β̂~j~ - β~j~)/se[β̂~j~] ~ 𝓝(0, 1) | *to-do* valid always or only under some hypothesis?
| p-value of β~j~: pseudo R-code: 2*pt(abs(t.value.beta), df=n-p, lower=FALSE) |
| F = [(TSS-RSS)/(p+1)] / [RSS/(n-p)]  | F-statistic. The null hypothesis is β~i~ = 0 for all i except 0.  F-statistic is 1 if there is no relationship between X and Y, larger otherwise.
|=====

References:

- https://www.stat.berkeley.edu/~aditya/resources/LectureSEVEN.pdf

- http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf


[[polynomial_regression]]
==== Polynomial regression

*to-do*

*to-do* What's the generalization? Any other functions ultimatively based on linear regression?


[[KNN]]
==== K-nearest-neighbor regression (KNN)

f̂(y~i~) = 1/k ∑~j~y~j~ | Find the k x~j~ closest to x~i~

If the underlying model is linear, KNN is worse.

_Curse of diemsionality_: With more predictors KNN gets worse. Intuitively: When p is gettig larger, then in p-dimensional space the density of data point usually decreses.  Thus the number of neighbors usually decreases.  Linear regression also suffers from larger p's, but not as mutch as KNN.


[[LOESS]]
==== LOESS regression curve

Generalization of KNN which is smoother.  KNN can be thought of as a rectangle function.  The k neigherst neighbors get wheight 1/k, all others get weight 0.  LOESS has a smoother weighting function, parameterized by α.  Larger α means more moothing.


=== Bootstrap

The _bootstrap_ is a method for estimating the distribution of an estimator θ̂ derived from on a population with unknown distribuation, primarily for estimating se[θ̂] and computing confidence intervals for θ̂.  Let P be an unknown distribution, Z~1~, ..., Z~n~ iid \~ P a sample, and θ̂ (or θ̂~n~) an estimator of an unknown parameter θ.  Using the given sample we can calculate a realization of θ̂.  But the problem that the bootstrap will solve is that we don't yet know the distribution of θ̂.  I.e. we don't yet know the uncertainty involved with the estimator θ̂.

If we had P, we could simply simulate using P.  The key idea of bootstrap is to estimate P, denoted P̂ (or P̂~n~), and then use P̂ to simulate.  Estimating P is simple.  The estimate P̂ is an empirical distribution of Z~1~, ..., Z~n~ which places mass 1/n at each data point.  It is a discrete distribution on the data points, and each point is equally likely.  To sample from P̂ means to uniformily sample with replacement from {Z~1~, ..., Z~n~}.

Sampling from P̂ yields a so called _bootstrap sample_ Z~1~^∗i^, ..., Z~n~^∗i^ iid\~ P̂.  Typically the bootstrap sample size is choosen to be the same size as the original sample size n.  Note that Z~1~^∗i^, ..., Z~n~^∗i^ are still random variables.

The _bootstrapped estimator_ θ̂^∗^ is is then simply θ̂^∗^ = g(Z~1~^∗i^, ..., Z~n~^∗i^).  Is an estimator for θ̂; the point being that θ̂^∗^'s distribution can be simulated and so we can estimate θ̂'s distribution, expectation, variance etc.  Note that θ̂^∗^ is still a random variable, the (probability) sample space being the original sample, opposed to usual (probability) sample space being the population.

The _bootstrap distribution_ P^∗^ is the distribution of the bootstrapped estimator θ̂^∗^.  It is a conditional distribution given the (random) original sample.  P^∗^ is induced by sampling from P̂.

From now on its hard to use good terms.  One one side literature also uses only a few terms, and the few terms that are used are far from universal agreement.  And on the other side, when I tried to come up with a coherent set of terms, the individual terms become a so long sequence of words, that they become too unpractical.  I gave up and will mostly just symbols.  The terms I do use I more or less liked, but they are not at all in universally used in literature.

_Bootstrapping an estimator_ θ̂ means sampling from P̂, delivering B bootstrap sample realization, the i-th denoted z~1~^∗i^, ..., z~n~^∗i^.  That then delivers B bootstrap estimator realizations (or _bootstrap replications_): θ̂^∗i^ = g(z~1~^∗i^, ..., z~n~^∗i^).  These B bootstrap estimator realizations then constitute the bootstrap distribution P^∗^.

Pr^∗^[·] is a conditional probability given the original sample.  The (probability) sample space is the original sample, as opposed to the usual (probability) sample space being the population.

The _bootstrap expectation_ E^∗^[·] (or E~P^∗^~[·]) is a conditional expectation given the original sample; the (probability) sample space is the original sample, as opposed to the usual (probability) sample space being the population.  I.e. changing the original sample will change the value of E^∗^[·].

Likewise the _bootstrap variance_ Var^∗^[·] (or Var~P^∗^~[·]) is a conditional variance given the (random) original sample.

The original problem included that we liked to know E[θ̂].  We can estimate E[θ̂] using E^∗^[θ̂^∗^] as an estimator. E^∗^[θ̂^∗^] is a consistent estimator for E[θ̂] if n is large, i.e. when <<bootstrap_consistency>> kicks in.  However we don't know that one either.  We can estimate E^∗^[θ̂^∗^] by averaging over the bootstrap replications θ̂^∗i^.  Ê^∗^[θ̂^∗^] is a good estimator for E^∗^[θ̂^∗^] if B is large. (*to-do* 1) is E^∗^[θ̂^∗^] ≈ Ê^∗^[θ̂^∗^] due to WLNN? 2) is Ê^∗^[θ̂^∗^] a consistent estimator for E^∗^[θ̂^∗^] -- or what is a a better term than `good' in ``Ê^∗^[θ̂^∗^] is a good estimator for E^∗^[θ̂^∗^] if B is large'')

E[θ̂] ≈(large n) Ê[θ̂] = E^∗^[θ̂^∗^] ≈(large B) Ê^∗^[θ̂^∗^] = 1/B ∑^B^ θ̂^∗i^

Likewise for variance:

E[θ̂] ≈(large n) Ê[θ̂] = E^∗^[θ̂^∗^] ≈(large B) Ê^∗^[θ̂^∗^] = 1/(B-1) ∑^B^(θ̂^∗i^ - Ê^∗^[θ̂^∗^])²

And for bias:

bias[θ̂] = E[θ̂] - E[θ] ≈(large n) E^∗^[θ̂^∗^] - θ̂ ≈(large B) Ê^∗^[θ̂^∗^] - θ̂

The _bootstrap quantile_ *to-do* it's conditional, right? Must be, since it's based on the conditional distribution P^∗^.

sd̂[θ̂^∗^] *to-do*. Used e.g. for bootstrap T confidence interval estimation.


[[bootstrap_consistency]]
==== Bootstrap consistency

The boostrap is called to be _consistent_ with rate a~n~ for estimator θ̂ if for an increasing sequence a~n~, for all x:

Pr[a~n~(θ̂-θ)≤x] - Pr^∗^[a~n~(θ̂^∗^-θ̂)≤x] P→ 0 (as n→∞)

Explaining the formula:  For Pr^∗^[·] see previous chapter.  In the left Pr[...], the estimator θ̂ is a random variable and θ is fixed.  In the right Pr^∗^[...], the bootstrap estimator θ̂^∗^ is a random variable and θ̂ is the fixed realization of estimator θ̂ using the original sample.  Typically a~n~ = √n.  An oversimplified way is to think that θ̂-θ and θ̂^∗^-θ̂ must have the same CDF.

Bootstrap consistency is important because it makes things work.  Consistency of the boostrap typically holds if the limiting distribution of θ̂ is Normal, and if the original data Z~1~, ..., Z~n~ are iid. (*to-do* 1) What is exactly meant with `the bootstrap'? Isn't it more precise to say that the bootsraped estimator θ̂^∗^ is consistent under the following condition - but then why use another formula than that of an consistent estimator? 2) Which things in the context of our lecture do work only due to bootstrap consistency? The estimators for E[θ̂], Var[θ̂] etc (the first approximation/estimation) 3) Why is the bootstrap usefull if I almost only can use it when θ̂ is asymptotically normal -- the problem statement was that I don't know θ̂'s distribution)

Implication of bootstrap consistency: The shape of P^∗^ is that of θ̂. So they have same expectation, i.e. centered around same point, and same variance.  (*to-do* correctly phrased? see lecture slide5)

Consistency of the boostrap implies consistent variance and bias estimation, i.e.:

(E^∗^[θ̂^∗^]-θ̂) / (E[θ̂]-θ) P→ 1 +
Var^∗^[θ̂^∗^] / Var[θ̂] P→ 1


==== Bootstrap confidence intervals

We want to compute an estimate of 1-α confidence interval for the estimator θ̂.  As described in the bootstrap introduction chapter, we can optain the conditional distribution P^∗^ of the bootstraped estimator θ̂^∗^, and from that also its conditional quantile q~θ̂^∗^~. (*to-do* 1) all these are not true confidence intervals, each is an estimate of the confidence interval, right? What's the correct wording? 2) its a conditional quantile, right?)

_normal_: θ̂ ± q~Z~(1-α/2) · sd̂[θ̂], where Z ~ N(0, 1) and sd̂[θ̂] = √Var̂[θ̂] ≈ √Var̂^∗^[θ̂^∗^] as described in the bootstrap intro chapter.  Note θ̂ does lie in the middle of the interval.

_quantile_ (in R called perc): [q~θ̂^∗^~(α/2), q~θ̂^∗^~(1-α/2)]. Special case of reversed quantile: it equals reversed quantile if the distribution of θ̂^∗^ - θ̂ is symmetric.  It's proovable that coverage is not 1-α.  Note that θ̂ might not lie in the middle of the interval.

_reversed quantile_ (in R called basic):  [θ̂ - q~θ̂^∗^-θ̂~(1-α/2), θ̂ - q~θ̂^∗^-θ̂~(α/2)].  If bootstrap consistency holds, its proovable that coverage is 1-α.  Note that θ̂ might not lie in the middle of the interval. (*to-do* 1) why do we make it so compolicated - what's the intuition why we now get a better coverage that the quantile CI 2) lecture notes say that performance is sometimes critized -- but nobody says that normal or quantile has better theoretical performance, right? Or whas performance meant in sence of computationally expensive?)

_bootstrap T_ (in R called stud):  [θ̂ - q~(θ̂^∗^-θ̂)/sd̂[θ̂^∗^]~(1-α/2) · sd̂[θ̂], θ̂ - q~(θ̂^∗^-θ̂)/sd̂[θ̂^∗^]~(α/2) · sd̂[θ̂]].  sd̂[θ̂] as in normal CI.  sd̂[θ̂^∗^] computed by 2nd layer of bootstrap, see bootstrap intro chapter.  Intuition:  Look at θ̂-θ ≈ θ̂^∗^-θ̂, where θ̂-θ is what I would like to have and θ̂^∗^-θ̂ is my observation.  If we instead take (θ̂-θ)/sd(θ̂) ≈ (θ̂^∗^-θ̂)/sd̂[θ̂^∗^], the two sides get similar more quickly.  Note that θ̂ might not lie in the middle of the interval.

References:

- Slides5.pdf


==== Bootstrap summary

_Real world_: +
sampling from P delivers sample (Z~1~, ..., Z~n~) +
estimator θ̂ = g(Z~1~, ..., Z~n~) \~ unknown-distribution +
θ̂'s value known since g(Z~1~, ..., Z~n~) can be computed

_Boostrap world_: +
sampling from P̂ delivers bootstrap sample (Z~1~^∗^, ..., Z~n~^∗^) +
bootstraped estimator θ̂^∗^ = g(Z~1~^∗^, ..., Z~n~^∗^) ~ P^∗^ +
Distribution P^∗^ given by the B bootstrap sample realizations

[cols="1,3"]
|=====
| B                          | Number of bootstrap samples
| P                          | The unknown distribution of the (original) sample Z~1~, ..., Z~n~
| P̂                         | Estimate of P. Empirical distribution of Z~1~, ..., Z~n~ which places probability mass 1/n on very data point Z~i~.
| Z~1~, ..., Z~n~ iid ~ P    | (Original) sample
| Z~1~^∗^, ..., Z~n~^∗^ iid ~ P̂ |  Bootstrap sample (or simulated data)
| z~1~^∗i^, ..., z~n~^∗i^    | i-th bootstrap sample realization
| θ (or θ~0~)                | Unknown parameter
| g                          | Function underlying the estimator θ̂
| θ̂ (or θ̂~n~) = g(Z~1~, ..., Z~n~) | Estimator for θ.  θ̂'s distribution is unknown, but we would like to know it.
| Pr^∗^[·]                   | Conditional probability given the original sample.
| E^∗^[·] (or E~P^∗^~[·])    | Bootstrap expectation. Conditional expectation given the original sample
| Var^∗^[·] (or Var~P^∗^~[·])| Bootstrap variance. Conditional variance given the original sample
| θ̂^∗^                      | Boostraped estimator. Random variable, the (probability) sample space being the original sample.
| θ̂^∗i^ = g(z~1~^∗i^, ..., z~n~^∗i^) | i-th bootstraped estimator realisation
| P^∗^                               | Bootstrap distribution. Distribution of θ̂^∗^.  Is a conditional distribution given the original sample.
| E[θ̂] ≈ Ê[θ̂] | large n
| Ê[θ̂] = E^∗^[θ̂^∗^] ≈ Ê^∗^[θ̂^∗^] | ≈ large B
| Ê^∗^[θ̂^∗^] = 1/B ∑^B^ θ̂^∗i^ |
| Var[θ̂] ≈ Var̂[θ̂] | large n
| Var̂[θ̂] = Var^∗^[θ̂^∗^] ≈ Var̂^∗^[θ̂^∗^] | ≈ large B
| Var̂^∗^[θ̂^∗^] = 1/(B-1) ∑^B^(θ̂^∗i^ - Ê^∗^[θ̂^∗^])² |
| bias[θ̂] = E[θ̂] - E[θ] ≈ biaŝ[θ̂] | large n
| biaŝ[θ̂] = E^∗^[θ̂^∗^] - θ̂ ≈ biaŝ^∗^[θ̂^∗^] | ≈ large B
| biaŝ^∗^[θ̂^∗^] = Ê^∗^[θ̂^∗^] - θ̂ |
|=====


References:

- Book ``An introduction to statistical learning'', chapter ``5.2 The Bootstrap''

- Book ``All of statistics'', chapter ``8 The Bootstrap''

- ETH, Script ``Computational Statistics'', Peter Bühlmann und Martin Mächler, chapter ``5 Bootsrap''


== Algebra and Arithmetic


=== Exponentiation, root, logarithm

base^exponent^ = power

^degree^√radicand = root

log~base~(antilogarithm) = logarithm

References:

- Notes on Logarithms and Units: https://www.cs.auckland.ac.nz/courses/compsci314s1c/resources/logNotes.pdf


=== Number theory

References:

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapters "Number Theory"

- Book "Introduction to algorithms", chapter "31 Number-Theoretic Algorithms"


==== Divisibility and greatest common divisor

**In this subchapter, we're only looking at integers.**

a _divides_ b (or a is a _divisor_ (or _factor_) of b, or b is _divisible_ by a, or b is a _multiple_ of a), denoted a | b, iff there is a k such that ak=b. **Note that the order of operands is reversed relative to division**.

Divisibility is reflexiv and transitiv, bot not symmetric.

If f|a and f|b, then f|(sa+tb) for any s and t.
I.e. a linear combination of a and b is divisible by any common factor of a and b.

n is a _linear combination_ of b~0~, ..., b~k~ iff n = ∑s~i~b~i~.

_Division Theorem_ (or _Division Algorithm_): Let n (_numerator_) and d (_denominator_) ≠ 0 be integers, then there exists a unique pair of integers q (_quotient_) and r (_remainder_) such that qd+r = n and 0≤r<|d|.
Note that by this definition, the remainder is always nonnegative, as opposed to how many programming languages define it.

rem(a,b) and a mod b are diffrent notations for the same thing.

A _commonon divisor_ of a and b is a number that divides them both.  The _greatest common divisor_ of a and b is denoted gcd(a,b).

_Euclid's algorithm_ finds the gcd of a and b ≠ 0: gcd(a,b) = gcd(b, a mod b).

_Bézout's lemma_ (or _Bézout's idendity_): For any nonzero a and b:
1) gcd(a,b) = sa+tb for some s and t; i.e. gcd(a,b) is a linear combinatino of a and b.
2) gcd(a,b) is the smallest positive integer that can be written as sa+tb.
3) sa+tb | gcd(a,b) for any s and t; i.e. every linear combination of a and b is a multiple of gcd(a,b).

Two integers and b are _relative prime_ if gcd(a,b) = 1.

If gcd(a,c)=1 and gcd(b,c)=1 then gcd(ab,c)=1.


==== Prime numbers

**In this subchapter, we're only looking at integers.**

A _prime_ is a number greater than 1 that is divisible only by itself and 1. A number other than 0, 1 and -1 that is not a prime is called _composite_.

_Fundamental Theorem of Arithmetic_: Every positive integer is a product of a unique weakly decreasing sequence of primes.

For all primes p and any a,b: if p|ab then p|a or p|b.

The _prime-counting function_ π(x) is the function giving the number of primes less than or equal to a given number x.

_Prime Number Theorem_: π(x) ~ x/ln(x). Thus as a rule of thumb, about 1 integer out every ln n in the vicinity of n is a prime.

_Chebyshev's Theorem on Prime Density_: π(x) > x / (3 ln x).


==== Modular arithmetic

**In this subchapter, we're only looking at integers.**

a and b are said to be _congruent modulo_ n ≥ 0, denoted a ≡ b (mod n), iff n | (a-b).
Stated differently, iff a-b = kn.

The congruent modulo relationship is reflexiv, symetric and transitiv.

a ≡ b (mod n) iff a mod n = b mod n

a ≡ a mod n (mod n)

If a ≡ b (mod n) and c ≡ d (mod n), then a + c ≡ b + d (mod n), ac = bd (mod n).


=== Linear algebra

The _determinant_ of a square matrix A is denoted det(A) or |A|.

In the 2D case:

--------------------------------------------------
      |a b|
|A| = |   | = ad - bc
      |c d|
--------------------------------------------------

The geometric interpretation is that, when you think about the matrix representing a linear transformation, the absolute value of the determinant is the factor applied to an area (in the 2D case, volume in 3D case and so on).  Also, in the 2D case, if A is build by combining column vectors v1 and v2 side by side, the determinant is positive when v1 is clockwise from v2 (their tails coinciding), negative when v1 is counterclockwise, and zero when the two are colinear.



== Geometry

A _metric space_ M is an ordered pair (S, d) where S is a set and d is a metric on S.  A _metric_ (or _distance function_ or simply _distance_) is a function d that defines a distance between each pair of elements of a set S.  It is defined as d: S⨯S → ℝ~+~, where for all x,y,z ∈ S the following conditions are satisfied:

d(x,y) ≥ 0 [small]#(non-negatity)# +
d(x,y) = 0 ⇔ x = y [small]#(identity of indiscernibles)# +
d(x,y) = d(y,x) [small]#(symmetry)# +
d(x,z) ≤ d(x,y) + d(y,z) [small]#(triangle inequality)#

A _right angle_ is an angle of exactly 90° (π/2 radians).  Two vectors are _perpendicular_ iff their angle is a right angle, or equivalently, if their scalar product is zero.  A set of vectors is _orthogonal_ iff they are pairwise perpendicular.  A _normal_ vector of a point on a smooth surface is any vector perpendicular to the plane.

The _dot product_ (or _scalar product_ or _inner product_ (in Euclidean geometry)) of two vectors x⃗ and y⃗ is defined as x⃗·y⃗ = ∑x~i~y~i~ = ‖x⃗‖‖y⃗‖cos(θ).  The former variant is the algebraic interpretation, the later is the geometric interpration.  More concretely, the geometric interpretation is that x⃗·(y⃗/‖y⃗‖) is the projection of x⃗ onto y⃗, when the two vectors are placed so that their tails coincide.

The _cross product_ (or _vector product_ or _directed area product_ (in Euclidean geometry)) of two vectors x⃗ and y⃗ is defined as x⃗⨯y⃗ = ‖x⃗‖‖y⃗‖sin(θ)n⃗.  n⃗ is the unit vector normal to the plane containing x⃗ and y⃗.  By convention, the direction of n⃗ is given by the _right-hand rule_: The index finger represents x⃗, the middle finger y⃗, and the thumb x⃗⨯y⃗.  The maginitude of the cross product can be interpreted as the area of the parallelogram having x⃗ and y⃗ as sides: ‖x⃗⨯y⃗‖ = ‖x⃗‖‖y⃗‖sin(θ).  Cross product is zero ⇔ the lines are parallel. Cross product is positive (negative) ⇔ x⃗ is clockwise (counterclockwise) from y⃗ (their tails coinciding).



== Misc

=== Fibonacci sequence / numbers

reccurence relation: F~n~ = F~n-1~ + F~n-2~

closed form expression: F~n~ = (ϕ^n^ - ψ^n^) / √5 = [ϕ^n^ / √5], where
ϕ is golden ratio and ψ=1-ϕ, and [x] is the nearest integer function
(aka round function).

Note: lim~n→∞~ F~n~ / F~n-1~ = ϕ

Applications: Fibonacci heap


=== Golden ratio

ϕ = (1+√5)/2 ≈ 1.618…

Two quantities a and b are in the golden ratio ϕ iff a+b / a = a / b =
ϕ, i.e. a=ϕb

=== Factorial

reccurence relation: x! = x*(x-1) and 0!=1

stirlings approximation: n! ~ √(2πn)*(n/e)^n^



== References

- MIT course 6.042 "Mathematics for computer science".
  * spring 2015, index: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/course-index/
  * spring 2015, textbook: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/readings/MIT6_042JS15_textbook.pdf
  * fall 2010, video lectures: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/video-lectures/
  * fall 2010, readings: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/

- MIT course 18.650 Statistics for Applications, fall 2016: https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/[lecture notes], https://www.youtube.com/watch?v=VPZD_aij8H0&list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0[videos]

- Book ``Algorithmics for Hard Problems: Introduction to Combinatorial Optimization, Randomization, Approximation, and Heuristics'', 2nd Edition, Juray Hromkovič. The Introduction chapter serves as good summary of computer science fundamentals.

- Book ``Modern Cryptography: Theory and Practice'' has a mathematical foundations part


== to-do

- skalarproduct
- greatest common divider/divisor
- log/exp relation to mul/div
- angle between vector
