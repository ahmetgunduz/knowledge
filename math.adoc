// The markup language of this document is AsciiDoc
:encoding: UTF-8
:toc:
:toclevels: 4


= Mathematics

Mainly mathematics tailored towards the nees of computer science.


== Misc basics

A _set_ is an unordered collection of distinct _elements_ (or _objects_, or _components_).
The notiation {a, b} defines a set consisting of elements a and b.
The _size_ (or _cardinality_) of a set S is the number of its elements, denoted as |S|.
A _sequence_ is an ordered collection of possibly duplicate elements,
The notation (b, a, a) defines a sequence.

The _powerset_ (or _power set_) of a set S is the set of all subsets of S, including the empty set and S itself.

[[permutation]]
A _permutation_ of a set S is a sequence containing every element of S exactly once.
The number of permutations of a n-element set is n!.

A (total) _function_ f : X â†’ Y maps all elements of its input set X to elements of its output set Y.
Note a function is allowed to map multiple elements of X to a given element of Y.
The input set is called the _domain_.
The output set is called the _codomain_.
The set of elements in the codomain that appear in f's mappings is called the _range_ (or _image_).
A _partial function_ does not map all the elements of its input.
The term _total function_ is a synonym to `function' as defined above.
The term total function is typically only used when a disambiguition to partial function is needed.
Given a _injective function_, each element of Y is mapped to at most once.
Given a _bijective function_, each element of Y is mapped to exactly once.
Given a _surjective function_, each element of Y is mapped to at least once.
A _k-to-1 function_ maps exactly k elements of X to every element of Y.

A function is bijective if and only if it's both injective and surjective.

Metric prefixes and names of large numbers (American way): *to-do* add also European way https://simple.wikipedia.org/wiki/Names_for_large_numbers

|=====
| neg.    | pos.    | power  |             | notes
| m milli | k kilo  | 1 (3)  | thousand    |
| Î¼ micro | M mega  | 2 (6)  | million     | mega means `large', `big' in Greek
| n nano  | G giga  | 3 (9)  | billion     | giga means `giant' in Greek
| p pico  | T tera  | 4 (12) | trillion    | tera from Greek tetra (4)
| f femto | P peta  | 5 (15) | quadrillion | peta from Greek penta (5)
| a atto  | E exa   | 6 (18) | quintillion | exa from Greek hexa (6)
| z zepto | Z zetta | 7 (21) | sextillion  |
| y yocto | Y yotta | 8 (24) | septillion  |
|=====


== Counting & Combinatorics

Typically we are typically good at determining the size of a set S of sequences.
Thus when faced with the problem of finding the size of a set P, we often try to define a set S of sequences and a function f : S â†’ P.
Typically, once we defined the set S of sequences, the function f is obvious or implied by the definion of S.
Say we find a set of sequences S and a bijection function f : S â†’ P, then the product rule gives us |S|, and the bijection property gives us |P| = |S|.
Likewise, say we find a k-to-1 function f : S â†’ P, the product rule and the division rule give us |P| = |S| / k.

For example consider poker.
In poker a hand is 5 cards out of a deck of 52 cards.
Each card has a rank and a suit.
There are 13 ranks and 4 suits.

How many different hands have a four-of-a-kind?
We try to define a set S of sequences describing the problem, i.e. having a relation to the set P asked for by the problem statement.
A possible solution is that a sequence of S has the follwing 3 elements.
1) The first is the rank of the four card; 13 possibilities.
2) The second is the rank of the extra card; 12 possibilities.
3) The third is the suit of the extra card; 4 possibilities.
Due to the product rule, |S| = 13Â·12Â·4.
There's an obvious bijection from |S| to |P|.
Due to to bijective property, |P| = |S|.

How many diffrent hands have a two pairs; that is, two cards of one rank, two cards of another rank, and one card of a third rank?
We try to define a set S of sequences describing the problem, i.e. having a relation to the set P asked for by the problem statement.
A possible solution is that a sequence of S has the follwing 6 elements.
1) The rank of the first pair; 13 possibilities.
2) The suits of the first pair; C(4 2) possibilities.
3) The rank of the second pair; 12 possibilities.
4) The suits of the second pair; C(4 2) possibilities.
5) The rank of the extra card; 11 possibilities.
6) The suits of the extra card; C(4 1) possibilities. 
Due to the product rule, |S| = 13Â·C(4 2)Â·12Â·C(4 2)Â·11*C(4 1).
Since the first pair, i.e. element 1 and 2, and the second pair, element 3 and 4, are indistinguishable, there is a 2-to-1 mapping from |S| to |P|.
To be precise, it's 2 because there are 2! = 2 ways to permute an 2-element set.
Due to the 2-to-1 mapping and the division rule, |P| = |S| / 2.

_Pigeonhole principle_: If |A| > |B|, then for every total function f : A â†’ B, there exists (at least) two diffrent elements of A that are mapped to the same element of B.
The _generalized pigeonhole principle_ states that if |A| > kÂ·|B|, then every total function f : A â†’ B maps at least k + 1 different elements of A to the same element of
B.

_Product Rule_: Given the sets A~1~, ..., A~n~.
Let A~1~ â¨¯ ... â¨¯ A~n~ denote the set of all sequences whose first term is drawn from A~1~, the second term is drawn from A~2~ and so forth.
|A~1~ â¨¯ ... â¨¯ A~n~| = âˆ|A~n~|.
E.g. the number of different 3 digit hex numbers is 16Â·16Â·16.

_Division Rule_: If there is a k-to-1 function f : A â†’ B, then |A|=kÂ·|B|.

As a special case of the division rule: If there is a bijective function f : A â†’ B, then |A| = |B|.

_Sum Rule_: Given disjoint sets A~1~, ..., A~n~, then |â‹ƒA~n~| = âˆ‘|A~n~|.

_Inclusion rule_: |AâˆªB| = |A| + |B| - |Aâˆ©B|.
Intuition: Just imagine the generic Venn diagram of sets A and B.

_Inclusion-exclusion rule_: Is a generalisation of the inclusion rule.
For the special case of three sets: |AâˆªBâˆªC| = |A| + |B| + |C| - |Aâˆ©B| - |Aâˆ©C| - |Bâˆ©C| + |Aâˆ©Bâˆ©C|. For the formula of the general case of n sets, the internet is your friend.

_Boole's inequality_: |AâˆªB| â‰¤ |A| + |B|. Intuition: Follows from the inclusion rule. Is a tight bound if A and B do not overlapp (i.e. Aâˆ©B=âˆ…).

_Union Bound_: |â‹ƒA~n~| â‰¤ |A~n~|. Intuition: Generalization of Boole's inequality.

_Monocity Rule_: If A âŠ† B, then |A| = |B| â‰¤ |B|.

_There are 2^n^ subsets of an n-element set_.
Proof: We define a sequence S from which there is a bijection to the problem set |P|.
The i-th element of the sequence S tells if element i of the original set is part of the subset or not.
The product rule gives |S|=2^n^, and the bijecton gives |P|=|S|.

A _k-combination_ of an n-element set S is a subset of k distinct elements of S.
The number of possible k-combinations is denoted by _C(n, k)_, pronounced `n choose k'.
Less concise formulated, it's the _number of k-element subsets of an n-element set_.
C(n, k) = n! / ((n-k)!k!).
Intuition: First we have n possibilities, then (n-1) and so on until (n-k+1).
That equals n! / (n-k)!.
So far we exactly have a k-permutation.
Since the order of those k elements doesn't matter, we have to devide by the number of permutations, which is k!.

C(n, k) = C(n, n-k)

C(n, 0) = C(n, n) = 1

_binomial theorem_ (aka _binomial expansion_): (x+y)^n^ = âˆ‘~0â‰¤kâ‰¤n~(C(n,k)Â·x^k^Â·y^n-k^). So C(n, k) is also called the _binomial coefficient_.

A _k-combination with repetitions_ (or _k-multicombination_, or _k-multisubset_) of an n-element set S is a multiset of k (possibly identical) elements of S.
The number of such k-multisubsets is denoted by \((n k)), pronounced `n multichoose k'.
\((n k)) = C(n+k-1, k).
Intuition, using the _stars and bars_ graphical aid.
Imagine the chosen multiset of elements Ï‰~1~ as a group of stars, the chosen multiset of elements Ï‰~2~ as another group of stars and so on.
More precisely, do it the following way.
You have a set of k+(n-1) positions.
Note that its a set, i.e. unordered.
The following visualizes it in an ordered manner, but conceptually it's unordered.
k positions are assigned a star, n-1 positions are assigned a bar.
The bars separate groups of stars.
For example for k=6 and n=3, a possible outcome is â˜…â˜…|â˜…â˜…â˜…|â˜….
Thus the original multicombination problem reduces to choosing a set of n-1 positions out of k+(n-1) positions in order to assign bars to.
C(k+(n-1), n-1) = C(k+(n-1), k) = C(n+k-1, k).
The first transformation is true due to the general rule C(n, k) = C(n, n-k).

A _k-permutation_ (or _variation_ or _partial permutation_) is a k-element sequence consisting of distinct elements out of an n-element set.
The nuber of possible k-permutations is denoted by _P(n,k)_ = C(n,k)*k! = n! / (n-k)!.
Intuition: First we have n possibilities, then (n-1) and so on until (n-k+1).
That equals n! / (n-k)! = C(n,k)*k!.

[[permutation_with_repetition]]
A _k-tuple_ (or _permutation with repetition_) is a k-element sequence consisting of (possibly identical) elements out of an n-element set.
The number of k-tubles of an n-element set is k^n^.
Intuition: First we have n possibilities, then again n, and so on, k times.

Overview denoting k-element entities and the number of such entities
given an n-element set (implies unordered and distinct):

|=====
|                    | without repetitions                | with repetitions
| subset (unordered) | k-combination, C(n, k)             | k-multicombination, C(n+k-1, k)
| sequence (ordered) | k-permutation, P(n, k) = C(n, k)k! | k-tuple, k^n^
|=====


Further typicall problems:

_bookkeeper rule_ (an inofficial term made up by the MIT): Given a k-element set {e~1~, ..., e~n~}, the number of sequences consisting of n~1~ e~1~, ..., n~k~ e~k~ is (âˆ‘n~i~)! / âˆ(n~i~!).
Intuition, using the problem of finding the number of ways to rearange the letters in the word `bookkeeper'.
There are n~1~=1 b's, n~2~=2 o's and so on up to n~6~ r's.
I.e. k=6, but that is not really important.
There is a total of âˆ‘n~i~ = 10 letters.
So there are 10! permutations of these letters.
However, we can't distinguish the n~2~=2 o's in each sequence, so we have to devide by 2!.
Likewise, we have to devide analogously for each of {b, o, k, e, p, r}.

Corollary to the bookkeeper rule: How many x-bit sequences contain y zeros? By the bookkeeper rule, n~1~ = y, n~2~ = x - y, thus x! / (y!Â·(x-y)!).

References:

- The above is largely based upon MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapter "Counting"


== Probability

The _sample space_ S (or Î©) is the set of possible outcomes of an _experiment_.
An element Ï‰ âˆˆ S is called an _outcome_ (or _sample outcome_ or _element_ or _realization_ (is ambigous to the realization of a random variable)).
A subset E âŠ† S is called an _event_.
In other words, an event is a set of outcomes.
âˆ… denotes the _null event_ which is always false.
S denotes the _true event_ which is always true.
The set of `interesting' or `known' events is denoted ğ“•.
A _probability space_ (or _probability triple_) is the tripe (sample space S, set of events ğ“•, probability function Pr).
A _probability function_ (or _probability distribution_ or _propability measure_) Pr (or P or â„™) on a sample space S is, a bit sloppily defined, a total function Pr : ğ“• âŸ¶ [0, 1] having the following two properties:
1) Pr(Ï‰) â‰¥ 0 for all outcomes Ï‰ âˆˆ S.
2) âˆ‘~Ï‰âˆˆS~ Pr(Ï‰) = 1.
3) Pr(E) = âˆ‘~Ï‰âˆˆE~Pr(Ï‰).
It's a sloppy definition because it enforces that ğ“• contains every outcome.
A more precise definition is that a probability function is a total function Pr : ğ“• âŸ¶ [0, 1] satisfying the three _probability axioms_ (or _Kolmogorov axioms_):
1) Pr(E) â‰¥ 0 for all events E âˆˆ ğ“•.
2) Pr(S) = 1.
3) If E~1~, E~2~, ... are disjoint then Pr(â‹ƒE~i~) = âˆ‘Pr(E~i~).
There are multiple notations denoting the evaluation of the function Pr: Pr(...) or Pr[...] or Pr{...}.
A finite probability space S is said to be _uniform_ if Pr(Ï‰) is the same for every outcome Ï‰ âˆˆ S.
In an uniform probability space, Pr(E) = |E| / |S| for any event E âŠ† S.

_conditional probability_: The probability of event A given event B is known to be true is Pr(A|B) = Pr(Aâˆ©B) / Pr(B).
Pr(A) is also called the _prior probability_ of A and Pr(A|B) the _posterior probability_ of A.
Note that the order in time in which the events A and B occur does not matter.
Note that in general Pr(A|B)â‰ Pr(A|B). Pr(cute_thing|pubby) is high but Pr(pubby|cute_thing) is not so high.

Intuitively Pr(A|B) is the probability of event A when only considering the alternate sample space SB = B.

--------------------------------------------------
Areas are proportional to probabilities

  Sample space S      Pr(â‹…|B) intuitively defines
                      a new sample space SB = B
           A
 S   whole 'column'
  +----+------+       Pr(A|B) = Pr(Aâˆ©B) / Pr(B)
  |    |      |       = Probability of A in sample space SB
  |    |      |
  |    |      |     SB
  +----+--+---+       +-------+---+
 B|       |   |      B|       |   |
  +-------+---+       +-------+---+
                               Aâˆ©B
--------------------------------------------------

_bayes theorem_: Pr(A|B) = Pr(B|A)Pr(A) / Pr(B). +
From definition of conditional probability and community of âˆ©. +
Pr(B) often given by law of total probability.

_law of total probability_: Given a partition {A~1~, ...,A~n~} of the sample space S, then Pr(B) = âˆ‘Pr(Bâˆ©A~i~) = âˆ‘Pr(B|A~i~)Pr(A~i~).

Pr(Aâˆ©B) = Pr(A|B)Pr(B) = Pr(B|A)Pr(A) =~if Aâ««B~ Pr(A)Pr(B). +
From definition of conditional probability and community of âˆ©.

Pr(AâˆªB) = Pr(A) + Pr(B) - Pr(AâˆªB)

[[independence]]
Two events A and B are _independent_, denoted Aâ««B (or AâŸ‚B), if (Pr(A|B) = Pr(A) or Pr(B) = 0).
Or equivalently, called the _product rule for independent events_, iff Pr(Aâˆ©B) = Pr(A)Pr(B).
Note that disjoint does _not_ imply independent.
For example say A and B are disjoint and both are non-empty, then Pr(A|B) = 0 â‰  Pr(A).
Naturally independence is a symmetric relationship.
That's why we usually say `A and B are independend' rather than `A is independent of B'.
The form `Pr(A|B) = Pr(A) or if Pr(B) = 0' shows more clearly the meaning of `the occurence of B does not affect the probability of A'.
The form `Pr(Aâˆ©B) = Pr(A)Pr(B)' shows more clearly the symmetry of indpendence.

Informally stated, A and B are independend if the probability of A is independent of whether its relative to sample space S or when considering only the restricted sample space SB = B, _or_ vice versa for B.

--------------------------------------------------
Areas are proportional to probabilities

                   Pr(A|B) = Pr(A) or if (Pr(B)=0)
                   Informally: Ratio Aâˆ©B:B equals ratio A:S,
                   i.e. probability of A is independent of whether
                   its relative to SB or to S.
 S          A                      S          A
  +-------+---+                     +-------+---+
  |       |   |                     |       |   |
  |       |   |                     |       |   |
  |       |   |  SB                 |       |   |
  +-------+---+    +-------+---+    |       |   |
 B|       |   |   B|       |   |    |       |   |
  +-------+---+    +-------+---+    +-------+---+
                            Aâˆ©B
--------------------------------------------------

Example where A and B _are_ dependend:

--------------------------------------------------
Areas are proportional to probabilities

            A
  +-------+---+
  |       |   |
  |       +---+
  +-----+-+   |
 B|     |     |
  +-----+-----+
--------------------------------------------------


--------------------------------------------------
Areas are proportional to probabilities

            A        Pr(Aâˆ©B) = Pr(A)Pr(B)
  +-------+---+      Considering the above drawings,
  |       |   |      this can only be true if
  |       |   |      both of A and B can be drawn
  +-------+---+      with straight orthogonal lines,
 B|       |   |      in which case
  +-------+---+
--------------------------------------------------


To make that example more concrete, consider that blood can have a certain type and a certain rh factor.
Say the probability Pr(T) for type T is known, and the probability Pr(F) for rh factor F is known.
The previously described Venn diagram shows that the probability somebody has type T _and_ rh factor F equals Pr(T)Pr(F) _only_ if T and F are independent.
For independence, the ratio of people having rh factor F among all people (|F| / |S| = Pr(F)) must be equal to the ratio of people having rh factor F among those having also type T (|Fâˆ©T| / |T|).

The elements of Î±={A~1~, ..., A~n~} are _mutually independent_ iff Pr(â‹‚A~i~) = âˆPr(A~i~) for _any_ subset of Î±.
Mutual independence does imply pairwise indpendence, but not vice versa.

The elements of Î±={A~1~, ..., A~n~} are _pairwise independent_ iff for all unordered pairs {A~i~, A~j~} of distinct elements (i.e. i â‰  j), A~i~ and A~j~ are independent.
Pairwise independence does _not_ imply mutual independence.

A _decision tree_ is a graphic tool for working with outcomes and events of an probability space.
The root is the start and is not directly associated a meaning.
Given a vertex, each outward edge represents that a given `subevent' occures.
The definition of an edge's associated subevent includes that the the subevent associated with the edge's source vertex has occured.
`Subevent' is an inofficial term made up by the author.
Each vertex thus represents the subevent that all subevents of the edges of the path from the root to that vertex have occured.
Note that the subevents on the path are not required to happen in the order implied by the path.
One just has to compute the correct _conditional_ probabilities of the edges.
Each outward edge of a vertex is assigned the conditional probability that the edge's associated subevent occures, given that the subevent associated with the vertex has occured.
For each internal vertex, the sum of the probabilities of all its outward edges is 1.
By the the above definitions, given a path, the subevents associated with the edges are independent, thus they can be multiplied to get the probability of taking that path.
Each leaf represents an outcome of the experiment.
Thus the set of all leaves represents the sample space.
I.e. there is a 1 to 1 relationship between the set of all leaves and and the set of all outcomes.

Alternatively, draw the tree using the treemapping method.
You start out with a rectangle representing the root vertex of the tree.
For each child, draw a line to create a subrectangle, the sizes of the subrectangles according to the weight of the edges. All llines mutually parallel.
Recurse.
At each new level in the recursion, toggle between horizontal and vertical lines.
The result has resemblance to a Venn diagram, only that here a given event is represented by a set of possibly disconnected areas, as opposed to a single connected area.

Recipe for solving many probability problems:

. Consequently follow the rules.
Don't try to be fast.
Often the human intuition is wrong.

. Define the sample space, i.e. all possible outcomes.

. Define events of interest.

. Compute probabilities (of required outcomes). Possibly the following way: Use the tree diagram method.  Assign a probability to each (required) edge.  Calculating the probability of an outcome is then trivial.

. Compute probability of your events, which is trivial, now that you have the probabilities of the outcomes.

References:

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapter "Probability"

- MIT course 18.650 "Statistics for Applications", Fall 2016, https://www.youtube.com/playlist?list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0[videos], https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/MIT18_650F16_Introduction.pdf[lecture notes]

- Book ``All of statistics'', chapter ``1 Probability''

- Khan Academy, ``Statistics'' playlist: https://www.youtube.com/watch?v=uhxtUt_-GyM&list=PL1328115D3D8A2566


=== Random variables and expectations

Formally a random variable is a function mapping from sample space to measure space, as defined in the following.  In practice, we often think of a random variable like a random number.  In practice, the sample space associated to a random variable is rarely explicitelly mentioned, but keep in mind that it really is there.  Random variables can be interpreted as link between data and sample spaces.

--------------------------------------------------
 probability space := (sample space S, events ğ“•, probability function Pr)

            probability
 set of     function Pr
 events ğ“• =============> [0,1]
  ^
  |set of
  |subsets  random        measure
  |         variable R    space,      CDF_R(x) := Pr(Râ‰¤x)
 sample  ===============> mostly â„    ================> [0,1]  
 space S
                                      E[R] := âˆ«xÂ· CDF_RÊ¹(x)
                                      ----------------> measure space

                                      Var[R] := E[R-E[R]]Â²
                                      = âˆ«(x-E[R])Â²CDF_RÊ¹(x)
                                      ----------------> measure space

   S is countable    discrete R       PMF_R(x) := Pr(R = x)
   set                                if R is the identity: PMF_R = Pr
                                      ================> [0,1]

                                      E[R] = âˆ‘xÂ·PMF_R(x) = âˆ‘R(Ï‰)Â·Pr(Ï‰)

   S is infinit      continous R      PDF_R(x) = CDF_RÊ¹(x) (informally)
   noncountable      PDF_R exists     Pr(aâ‰¤Râ‰¤b) = integrate PDF_R(x) over [a,b]
   set                                if R is the identity: PDF_R = Pr
                                      ================> [0,1]

                                      E[R] = âˆ«xÂ·PDF_R(x)
--------------------------------------------------

A _random variable_ R is a measurable total function R : S âŸ¶ â„.
Technically the range of R is the _measure space_ E, but in computer science practice the measure space is mostly â„.
Roughly speaking, density functions exist only when the measuere space is â„.
The actually observed value of a random variable R is called _realization_ of R (or _observation_).
Note that the term `realization' is ambigously also used as a synonym for outcome Ï‰ âˆˆ S.
An _indicator random variable_ (or _Bernoulli variable_) is a random variable with codomain {0, 1}.
A random variable is _discrete_ if its domain is a countable set.
A random variable R is _continuous_ if there exists a probability density function for it.
Note that for a continuous random variable R, Pr(R = x) = 0 for every x.
We get a non-zero probability only in a non-empty range.

There's a strong relation between events and random variables.
Any assertion about the value of a random variable defines an event.
Say the random variable C counts number of heads in 3 coin flips.
The condition C = 1 defines the event {HTT, THT, TTH}, or the condition C â‰¤ 2 {TTT, HTT, THT, ...}.
Looking at it from the other direction, each event E is naturally associated with a corresponding indicator random variable I~E~, where I~E~(Ï‰) equals 1 if outcome Ï‰ âˆˆ E and and 0 otherwise.

Given a random variable R with measure space â„, its _cumulative distribution function_ (or _CDF_ or _cumulative density function_) CDF~R~ (or F~R~) : â„ âŸ¶ [0, 1] is defined as CDF~R~(x) = Pr(R â‰¤ x).

Given a random variable R with measure space â„, its _inverse CDF_ (or _quantile function_) is defined by CDF~R~^-1^(q) = inf{r: CDR~R~(x) > 1} for q âˆˆ [0, 1].
E.g. CDF~R~^-1^(1/2) tells you the x at which CDR(x) equals 1/2.
We call CDF~R~^-1^(1/4) the _first quartile_, CDF~R~^-1^(1/2) the _median_ (or _second quartile_) and CDF~R~^-1^(3/4) the _third quartile_.

_percentile_ is the same as quantile, only that it is in %, that is 100 times larger.

[[PDF]]
Given a continuos random variable R with measure space â„, its _probability density function_ (or _PDF_) PDF~R~ (or f~R~) : â„ âŸ¶ [0, 1] is a function satisfying:

1) Pr(a â‰¤ R â‰¤ b) = âˆ«~a~^b^PDF~R~(x)Â·dx for every a â‰¤ b. +
2) Pr(x) â‰¥ 0 for all x. +
3) âˆ«~-âˆ~^âˆ^PDF~R~(x)Â·dx = 1.

Note that according to these rules a PDF, unlike a PMF, can be bigger than 1; it can even be unbounded. See also <<population>>.

[[PMF]]
Given a discrete random variable R with measure space â„, its _probability mass function_ (or _PMF_ or _probability function_) PMF~R~ (or f~R~) is defined as PMF~R~(x) = Pr(R = x).  See also <<population>>.

Both the probability density function and the cumulative distribution function capture the same information about the random variable, so take your choice.

PDF~R~(x) = CDFÊ¹~R~(x) at all points x at which CDF~R~ is differentiable.

CDF~R~(x) = âˆ«~âˆ’âˆ~^x^PDF~R~(x)Â·dx.

In sloppy notation, CDF~R~(-âˆ) = 0 and CDF~R~(âˆ) = 1.

A _univariate distribution_ is a probability distribution of only one random variable.  A _multivariate distribution_ is the _joint probability distribution_ of two or more random variables.

Two random variables R~1~ and R~2~ are _equal_ if R~1~(Ï‰) = R~2~(Ï‰) for all outcomes Ï‰ âˆˆ S.

Two random variables R1 and R2 are _equal in distribution_ if CDF~R1~(x) = CDF~R2~(x) for all x.
Note that equal in distribution does not imply equal.
E.g. consider X = `number of heads' and Y = `number of tails' in N fair coin tosses.

Two random variables R~1~ and R~2~ are _independent_ iff for all x~1~ âˆˆ codomain(R~1~), x~2~ âˆˆ codomain(R~2~), the two events [R~1~ = x~1~] and [R~2~ = 2~1~] are independent.

Random variables R~1~, ..., R~n~ are _mutually independent_ iff for all x~1~, ..., x~n~ the events [R~1~ = x~1~], ..., [R~2~ = x~2~] are mutually independent.
They are _k-way independent_ iff every subset of k of them are mutually independent.

A set of random variables is _independent and identically distributed_ (or _iid_ or __i.i.d.__) if all random variables are mutually indpendent and each random variable has the same probability distribution as the others.

Two events are independent iff their indicator variables are independent.

Let R and S be independent random variables, then f\(R) and g(S) are also independent random variables, where f and g are some functions.

The _mode_ is the value of X where the PMF / PDF of X takes its maximum value. I.e. its the value of X that appears the most often.

Given a random variable R, then its _expected value_ (or _expectation_ or _mean_ or _average value_ or _first moment_, see also <<population_mean>>), denoted E[R] (or ğ”¼\(R) or ğ”¼R or Î¼ or Î¼~R~ or by the use of on overline), is defined by:

E[R] = âˆ«xÂ·CDFÊ¹~R~(x) +
If R is discrete: E[R] = âˆ‘x~i~Â·PMF~R~(x~i~) = âˆ‘~Ï‰âˆˆS~R(Ï‰)Â·Pr(Ï‰) +
If R is continuous: E[R] = âˆ«xÂ·PDF~R~(x)

The _conditional expectation_ E[R|A] of a random variable R given event A is E[R|A] = âˆ‘rÂ·Pr(R=r|A).

[[variance]]
Given a random variable R, its _variance_ (or _mean square deviation_, see also <<population_variance>>), denoted by Var[R] (or ğ•\(R) or ğ•R or ÏƒÂ² or ÏƒÂ²~R~), is a measure of spread and is defined by

Var[R] = E[(R-E[R])Â²] = E[RÂ²] - E[R]Â² = âˆ«(x-E[R])Â²CDFÊ¹~R~(x) +
If R is discrete: Var[R] = (âˆ‘xÂ²~i~PMF~R~(x~i~)) - E[R]Â² +
If R is continuous: Var[R] = (âˆ«xÂ²PDF~R~(x)) - E[R]Â²

Note that an alternative measure of spread, thought much less often used than variance, is E[|R-E[R]|].

Given a random variable R, its _standard deviation_, denoted Ïƒ (or Ïƒ~R~ or sd\(R)), is defined by Ïƒ = âˆšVar[R].

A set of random variables is called _homoscedastic_ if all of those random variables have the same finite variance.  This is also known as _homoscedasticity_ (or _homogeneity of variance_).  The complementrary notion is called _heteroscedasticity_.

The _covariance_ between two random variables R~1~ and R~2~ is defined as Cov[R~1~, R~2~] = E[(R~1~-E[R~1~])(R~2~-E[R~2~])] = E[R~1~R~2~] - E[R~1~]E[R~2~].

[[correlation]]
_Correlation_ is a statistical relationship between random variables, though in common usage it most often refers to how close two variables are to having a linear relationship with each other. E.g. the relationship between X and Y in regression/classification.

[[pearsons_correlation_coefficient]]
The _Pearson's product moment correlation cofficient_ (or _Pearson's correlation coefficient_ _correlation coefficient_ or simply _correlation_ (but see also <<correlation>>)) between two random variables R~1~ and R~2~ is the standardized covariance and is defined as Ï~R1,R2~[R~1~, R~2~] = Cov[R~1~, R~2~] / (âˆšVar[R~1~]âˆšVar[R~2~]).  Note that the codomain is [-1,1].  Intuitively, it measures how linear the relationship is.  It is 1 for a perfect linear relationship with positive slope, -1 for a perfect linear relationship with negative , and 0 for no relationship at all.

Two random variables R~1~ and R~2~ are said to be _uncorrelated_ if Cov[R~1~, R~2~] = 0.

independent â‡’ uncorrelated

_interaction_ is when the influence of two or more predictors on the response is not additive. E.g. say there are two predictors X1 and X2 and the response Y = f(X1,X2). Imagine the 3D graph/plane.  If a cut through the plane at X1 = some-constant and X2 = some-other-constant doesn't produce two same looking functions (appart from shift), then there's interaction.

If two predictors are highly correlated, it doesn't make sense to add an interaction between them to the model.

E[aÂ·R~1~ + bÂ·R~2~] = aÂ·E[R~1~] + bÂ·E[R~2~] (_linearity of expectation_)

R~1~, ..., R~n~ are mutually independent â‡’ E[âˆR~i~] = âˆE[R~i~]

Var[R] = Cov[R, R]

Var[aR+b] = aÂ²Var[R]

Var[R~1~ + R~2~] = Var[R~1~] + Var[R~2~] - 2Cov[R~1~, R~2~]

In general: Var[âˆ‘a~i~R~i~] = âˆ‘âˆ‘a~i~a~j~Cov(R~i~,R~j~) = (âˆ‘aÂ²~i~Var[R~i~]) + 2âˆ‘~j~âˆ‘~i<j~a~i~a~j~Cov[R~i~, R~j~]

If R~1~, ..., R~n~ are pairwise independent: Var[âˆ‘R~i~] = âˆ‘Var[R~i~]

Cov[R, R] = Var[R]

Cov[R~1~, R~2~] = E[R~1~R~2~] - E[R~1~]E[R~2~]

If R~1~ and R~2~ are independent: Cov[R~1~,R~2~] = Ï~R1,R2~ = 0.

_Law of Total Expectation_: Let R be a random variable, and suppose that A~1~, ..., A~n~ is a partition of the sample space S, then E[R] = âˆ‘~i~E[R|A~i~]Â·Pr(A~i~).

_Mean time to failure_: Given an event E and p = Pr(E), the number of independent experiments until E occures is 1 / p and the variance is (1-p)/pÂ².

_Markov's inequality_: For non-negative R. Pr(Râ‰¥a) â‰¤ E[R] / a.

_Chebyshev's inequality_: Pr(|R-E[R]| â‰¥ a) â‰¤ Var[R]/aÂ². Derived from Markov's inequality.

_Pairwise independent sampling_: Let R~1~, ..., R~n~ be pairwise independent random variables with the same mean Î¼ and same deviation Ïƒ, and let S be their sum: Pr(|S/n-Î¼| â‰¥ x) â‰¤ 1/n ÏƒÂ²/xÂ².

Given a sequence X~1~, ..., X~n~ of random variables.  X~n~, the last of the sequence, _converges in distribution_ (or _converges weakly_ or _converge in law_) towards the random variable X, denoted X~n~ Dâ†’ X (actually D is above the arrow) (or X~n~ â‡ X), if lim~nâ†’âˆ~ CDF~Xn~(x) = CDF~X~(x) âˆ€ x âˆˆ â„ at which CDF~X~ is continuous.

Given a sequence X~1~, ..., X~n~ of random variables.  X~n~, the last of the sequence, _converges in probability_ towards the random variable X, denoted X~n~ Pâ†’ X (P above the arrow) or plim~nâ†’âˆ~ X~n~ = X, if for all Îµ > 0 lim~nâ†’âˆ~ Pr(|X~n~ - X| > Îµ) = 0. Convergence in probability implies convergence in distribution.

_Weak Law of Large Numbers_ (or _WLLN_ or _Khintchine's law_): Let X~1~, ..., X~n~ be iid random variables with the same mean Î¼ and same variance ÏƒÂ², and let XÌ„ = 1/n âˆ‘X~i~ denote their sample mean. WLLN states that XÌ„ Pâ†’ Î¼. Interpretation: The distributionh of XÌ„ becomes infinitely concentrated, i.e. 0 variance, around Î¼ as n gets large.  The sample mean is a consistent estimator for the population mean Î¼.  Note that while E[XÌ„] = Î¼ and Var[XÌ„] = ÏƒÂ²/n are also true, they are different statements.

_central limit theorem_ (_CLT_):  Let the random variables X~1~, ..., X~n~ be independent, each X~i~ with some arbitrary unknown distribution but with known mean Î¼~i~ and finite variance ÏƒÂ²~i~.  Then (âˆ‘X~i~ - âˆ‘Î¼~i~) / âˆšâˆ‘ÏƒÂ²~i~ â‡ N(0, 1), or formulated differently: 1/n âˆ‘X~i~ â‡ N(Î¼Ì„, ÏƒÌ„Â²/n)  where Î¼Ì„ = 1/n âˆ‘Î¼~i~ and ÏƒÌ„Â² = 1/n âˆ‘ÏƒÂ²~i~.  If additionally X~1~, ..., X~n~ are identically distributed with mean Î¼ and variance ÏƒÂ², this simplifies to XÌ„ = 1/n âˆ‘X~i~ â‡ N(Î¼, ÏƒÂ²/n).  (*to-do* 1) better understand what http://mathworld.wolfram.com/CentralLimitTheorem.html says more 2) relation to `converges in distribution'? See all of statistics p 72 3) How do you call this thing on the lhs of â‡? 4) Is it correct that I shouldn't use the term sample mean and thus also not the conventional XÌ„ = 1/n âˆ‘X~i~ in the first general case, since the term sample is reserved for the case of taking a sample from a population, and by the definition, population means that its members have the same distribution. How you call 1/n âˆ‘X~i~ in the first/general case? How you call (âˆ‘X~i~ - âˆ‘Î¼~i~) / âˆšâˆ‘ÏƒÂ²~i~ ?)

__WLLN vs CLT__: WLLN gives sample mean's value provided iid Xs.  CLT gives distribution of 1/n âˆ‘X~i~ only provided independent Xs.  (*to-do* But then CLT is a proper superset of WLLN, since knowing the distribution implies knowing the mean. So the question remains, whats the real difference between CLT and WLLN?)

References:

- Book ``All of Statistics'', chapters ``2 Random Variables'' and ``3 Expectation''

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapters "Random Variables" and "Deviation from the Mean"


=== Important distributions


==== Comparison of distributions

*to-do*

References:

- http://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/


==== Uniform distribution

X ~ Uniform(a, b), where a < b, if

PDF(x) = { +
1/(b-a) for x âˆˆ [a, b]
0 otherise

CDF(x) = { +
0 for x < a +
(x-a)/(b-a) for x âˆˆ [a, b] +
1 for x > 0

==== Normal distribution / Gaussian distribution

X ~ ğ“(Î¼, ÏƒÂ²), where Î¼âˆˆâ„ is the mean and Ïƒ>0 the standard deviation.

PDF(x) = 1/(Ïƒâˆš(2Ï€)) exp(-1/(2ÏƒÂ²) (x-Î¼)Â²)

CDF(x) = Î¦((x-Î¼)/Ïƒ)

We say that X has _standard Normal distribution_ if Î¼=0 and Ïƒ=1. Tradition dictates that a standard Normal random variable is denoted by Z.  The PDF and the CDF of Z are denoted by ğœ™(z) and Î¦(z) respectively.

Î¦(z) = 1/âˆš(2Ï€) âˆ«~-âˆ to x~exp(-tÂ²/2)dt = +
1/2 + 1/2 erf(x/âˆš2)

Where erf(x) = 2/âˆšÏ€ âˆ«~0 to x~exp(-tÂ²)dt

A k-dimensional _multivariate normal distribution_ (or _k-variate normal distribution_) is denoted ğ“~k~(Î¼, ÏƒÂ²).

Some useful facts:

X \~ N(Î¼,ÏƒÂ²) â‡’ (X-Î¼)/Ïƒ ~ N(0,1)

Z \~ N(0,1) â‡’ X = Î¼ + ÏƒZ ~ N(Î¼, ÏƒÂ²)


==== Student's t-distribution

The _Student's t-distribution_ (or _t-distribution_) is the distribution of the sample mean where the population is normally distributed.  It is denoted t~Î½~, where Î½ is its single parameter, the degrees of freedom.  More precisely: Let Î¼ denote the population mean, XÌ„ the sample mean and SÂ² the unbiased sample variance, then (XÌ„-Î¼)/sdÌ‚[XÌ„] \~ t~n-1~, where sdÌ‚[XÌ„] = S/âˆšn, see estimator for standard error of the mean, and where t~n-1~ denotes a Student's t-distribution with n-1 degrees of freedom.

*to-do* I think that is not quite correct. It's just one of more possible use cases. After all many other statistics also have a t-distribution, no?

*to-do* list common statistics which follow a t-distribution (e.g. when statistic g1 follows a normal distribution and a scaling parameter depends on the data, e.g estimator sdÌ‚[g1], then, under certain conditions, g2=g1/sdÌ‚[g1] follows a student's distribution)

<<t_statistic>>
The _(Student's) t-statistic_ for an estimator Î²Ì‚ \~ ğ“ of unknown parameter Î² is defined as t~Î²Ì‚~ = (Î²Ì‚ - Î²~0~) / seÌ‚[Î²Ì‚], where Î²~0~ is a fixed value which may or may not match Î².  Î²Ì‚ must be normally distributed, which in case of OLS is the case if E[epsiolon]=0.  The t-statistic is commonly used in hypothesis testing, where the null hypothesis is that Î² = Î²~0~.  Typically Î²~0~ is 0.  If Î²Ì‚ is an ordinary least squares estimator for a coefficient in the classical linear regression model, and if the true value of parameter Î² is equal to Î²~0~, then t~Î²Ì‚~ \~ t~n-p~ where n is the number of observations, and p is the number of predictors (including the intercept).

Etymology: the term ``t-statistic'' is abbreviated from ``hypothesis test statistic''.

*to-do* I am confused. Here the denominator is se[Î²Ì‚], in the t-distribution its sdÌ‚[XÌ„] (the key point being that the later is an estimator).  Also apparently the Student's t-statstic is not guaranteed to be Student t-distributed, I find that confusing from a terminology point of view. How you call then the statistic used above in the definition of t-distribution?

*to-do* Also in <<t_test>> there multiple examples of t-statistics, all of which have as denominator an estimator, not se[...].  Only when we wanted a t-statistic for a t-test for a estimator Î²Ì‚ of a OLS model coefficient Î², we used t~Î²Ì‚~ = (Î²Ì‚ - Î²~0~) / se(Î²Ì‚).


==== Chi-squared-distribution (ğœ’Â²)

Given random variables X~1~, ..., X~k~ iid~ ğ“(0,1), then

âˆ‘X~i~Â² \~ ğœ’~k~Â²

*to-do* what if X~1~, ..., X~k~ iid~ ğ“(Î¼, ÏƒÂ²)?


==== F-distribution / Fisher-Snedecor distribution

A random variable X having a F distribution with parameters d~1~ and d~2~ is denoted X \~ F(d~1~, d~2~).

It is the distribution of X = (U~1~/d~1~) / (U~2~/d~2~), where U~1~ and U~2~ are independent and have distributions ğœ’Â²(d~1~) and ğœ’Â²(d~2~) respectively, where ğœ’Â² denotes the chi-squared distribution.

Or equivalently, it's the distribution of X = ...

*to-do*

Independence of U~1~ and U~2~ might be demonstrated by applying Cochran's theorem.

Applications: Appears often as the distribution of the test statistic in ANOVA.


== Algebra and Arithmetic


=== Exponentiation, root, logarithm

base^exponent^ = power

^degree^âˆšradicand = root

log~base~(antilogarithm) = logarithm

References:

- Notes on Logarithms and Units: https://www.cs.auckland.ac.nz/courses/compsci314s1c/resources/logNotes.pdf


=== Number theory

â„• natural numbers. Whether 0 âˆˆ â„• is not clearly defined.

â„•~0~, â„•^0^, â„¤~â‰¥0~, â„¤^*^ non-negative integers

â„•~>0~, â„¤^+^ positive integers

â„¤ integers. Z is for the German word Zahlen.

â„š rational numbers. Q is for the German word Quotient.

â„ real numbers

0 is neither positive nor negative.

References:

- MIT course 6.042 "Mathematics for computer science", lecture notes "Mathematics for computer science", chapters "Number Theory"

- Book "Introduction to algorithms", chapter "31 Number-Theoretic Algorithms"


==== Divisibility and greatest common divisor

**In this subchapter, we're only looking at integers.**

a _divides_ b (or a is a _divisor_ of b, or b is _divisible_ by a), denoted a | b, iff there is a k such that ak=b.  b and 1 are so-called _trivial divisors_ of b.  Nontrivial divisors of b are called _factors_ of b.  If additionally k â‰¥ 1, we say b is a _multiple_ of a.

Divisibility is reflexiv and transitiv, bot not symmetric. *to-do* write more explicitely using formulas

a|0 (by agreement)

f|a and f|b â‡’ f|(sa+tb) for any s and t (a linear combination of a and b is divisible by any common factor of a and b)

a|b and b|a â‡’ a=b

n is a _linear combination_ of b~0~, ..., b~k~ â‡” n = âˆ‘s~i~b~i~.

A _commonon divisor_ of a and b is a number that divides them both.  The _greatest common divisor_ (_GCD_) (or _greatest common factor_ or _highest common divisor_) of a and b is denoted gcd(a,b).  By convention gcd(0, 0) = 0.

gcd(a,b) = gcd(b,a) (commutative)

gcd(a, gcd(b,c)) = gcd(gcd(a,b), c) (associative)

gcd(a, b, c) = gcd(gcd(a, b), c) (gcd of more than two arguments)

d|a and d|b â‡’ d|gcd(a,b)

a|bc and gcd(a,b) = d â‡’ a/d | c

gcd(ma, mb) = m gcd(a, b) âˆ€ m âˆˆ â„•

gcd(m + mb, b) = gcd(a, b)

gcd(a, ma) = a

gcd(a,0) = |a|

gcd(a,c)=1 and gcd(b,c)=1 â‡” gcd(ab,c)=1

gcd(a,b) = gcd(b, a mod b) (see Euclid's algorithm)

Two integers and b are _relative prime_ if gcd(a,b) = 1.

From the fundamental theorem of arithmetic directly follows that gcd(a, b) = product of primes common to a and b.  Thus an inefficient algorithm to compute gcd(a, b) is to prime factorize a and b, compare the factors, and build the product of the common factors.

_BÃ©zout's lemma_ (or _BÃ©zout's idendity_): For any nonzero a and b:
1) gcd(a,b) = sa+tb for some s and t; i.e. gcd(a,b) is a linear combination of a and b.
2) gcd(a,b) is the smallest positive integer that can be written as sa+tb.
3) sa+tb | gcd(a,b) for any s and t; i.e. every linear combination of a and b is a multiple of gcd(a,b).

_Euclid's algorithm_ Recursively solve gcd(a,b) by gcd(a,b) = gcd(b, a mod b). The bottom case is b = 0, in which case gcd(a,0) = |a|.

binary method to compute gcd: *to-do*


==== Prime numbers

**In this subchapter, we're only looking at integers.**

A _prime_ is a number greater than 1 that is divisible only by itself and 1. A number other than 0, 1 and -1 that is not a prime is called _composite_.

_Fundamental Theorem of Arithmetic_: Every positive integer is a product of a unique weakly decreasing sequence of primes.

For all primes p and any a,b: if p|ab then p|a or p|b.

There are infinitely many primes.

The _prime-counting function_ Ï€(x) is the function giving the number of primes less than or equal to a given number x.

_Prime Number Theorem_: Ï€(x) ~ x/ln(x). Thus as a rule of thumb, a given integer x is prime with a probability of about 1/ln(x). For x>67: Ï€(x) > x/ln(x).

_Chebyshev's Theorem on Prime Density_: Ï€(x) > x / (3 ln x).

See also algorithms_and_data_structures.adoc, chapters ``primalty testing'' and ``generating primes''.


==== Modular arithmetic

**In this subchapter, we're only looking at integers.**

_Division Theorem_ (or _Division Algorithm_): Let n (_numerator_) and d (_denominator_) â‰  0 be integers, then there exists a unique pair of integers q (_quotient_) and r (_remainder_) such that qÂ·d + r = n and 0 â‰¤ r < |d|.
Note that by this definition, the remainder is always nonnegative, as opposed to how many programming languages define it.

Common notations for the _remainder operation_ (or _modulo operation_) are n mod d or rem(n, d).  Common notations for _quotient operation_ are n div d or qcnt(n, d).

_Modular arithmetic_ (or _clock arithmetic_) is the arithmetic of congruences.

A _congruence relation_ (or simply _congurence_) is an equivalence relation on an algebraic structure that is compatible with the structure.

_Congruence modulo n_ on the set of integers is a congruence relation. a â‰¡ b (mod n) denotes ``a is congruent to b (modulo n)'' or ``a and b are congruent modulo n''.  The number n is called the _modulus_.  These three claims are equivalent:

a â‰¡ b (mod n) â‡” +
n | (a-b) â‡” +
a = b + kn âˆ€ k âˆˆ â„•

In the following, the explicit (mod n) is omitted for brevity.

An a^-1^ such that aÂ·a^-1^ â‰¡ 1 is called _modular multiplicative inverse_ of a modulo n.  a^-1^ exists iff a is coprime with n.

a â‰¡ a [reflexiv]

a â‰¡ b â‡” b â‰¡ a [symetric]

a â‰¡ b and b â‰¡ c â‡’ a â‰¡ c [transitiv]

a â‰¡ b â‡” a + k â‰¡ b + k âˆ€ k âˆˆ â„¤ [compatibility with translation]

a â‰¡ b â‡’ ka â‰¡ kb âˆ€ k âˆˆ â„¤ [compatibility with scaling]

ka â‰¡ kb and k is coprime with n â‡’ a â‰¡ b

a â‰¡ b â‡’ a^k^ â‰¡ b^k^ âˆ€ k âˆˆ â„• [compatibility with exponentation]

a â‰¡ b and c â‰¡ d â‡’ a + c â‰¡ b + d [compatibility with addition]

a â‰¡ b and c â‰¡ d â‡’ a - c â‰¡ b - d [compatibility with subtraction]

a â‰¡ b and c â‰¡ d â‡’ ac â‰¡ bd [compatibility with multiplication]

a â‰¡ b and a^-1^ exists â‡’ a^-1^ â‰¡ b^-1^ [compatibility with multiplicative inverse]

When a = xÂ² mod p for an a âˆˆ â„¤~p~ and any x âˆˆ â„¤~p~, then a is called a _quadratic residue_.

When a â‰  xÂ² mod p for an a âˆˆ â„¤~p~ and all x âˆˆ â„¤~p~, then a is called a _quadratic nonresidue_.

Exactly half of the nonzero elements of field â„¤~p~ are quadratic residues.

_Legendre symbol_: Leg(a|p) â‰¡ {1 if a is a quadratic residue, -1 if a is a quadratic non-residue, 0 if p|a}.

p is odd prime â‡” a^(p-1)/2^ â‰¡ Leg(a|p) âˆ€ a âˆˆ â„¤~p~ - \{0}. [Euler's Criterion]

_Jacobi Symbol_: Jac(a|n) = âˆ~1â‰¤iâ‰¤l~Leg(a|p~i~)^k~i~^ = âˆ~1â‰¤iâ‰¤l~(a^(p~i~-1)/1^ mod p~i~)^k~i~^ = {1, -1}, where gcd(a,n) = 1 and where p~1~^k~1~^ Â· ... Â· p~l~^k~l~^ is the prime factorization of n.

p is prime and a âˆˆ â„¤ and gcd(a,p) = 1 â‡’ a^p-1^ â‰¡ 1 [Fermat's little theorem]

p is prime and a âˆˆ â„¤~p~ - \{0} â‡’ a^-1^ = a^p-2^ mod p [Consequence of Fermat's little theorem]

p is prime â‡” (p-1)! â‰¡ -1 [Wilson's theorem]

Chinese Remainder Theorem:  Let m = m~1~Â·...Â·m~k~ where k âˆˆ â„¤^+^ and m~i~ âˆˆ â„¤^â‰¥2^ are pairwise coprimes. For any sequence r~1~ âˆˆ â„¤~m1~, ..., r~k~ âˆˆ â„¤~mk~ there is an unique r âˆˆ â„¤~m~ such that r â‰¡ r~i~ (mod m~i~) âˆ€ i âˆˆ [k].


==== Abstract Algebra

A set S is _closed_ under an n-ary operation f if f: S^n^ â†’ S. A set S is closed under a collection of operations if it is closed under each of the operations individually.

An _algebraic structure_ (or simply _algebra_) is a pair (S, F) where S is a set closed under a set F of operations.

Given an algebra (S, âˆ—) where âˆ— is a binary operation.  An element e âˆˆ S is called a _left identity_ if e âˆ— x = x âˆ€ x âˆˆ S, and a _right identity_ if x âˆ— e = x âˆ€ x âˆˆ S.  If e is both a left and a right identity, then it is called a _two-sided identity element_ (or _two-sided neutral element_ or simply _identity_) according to âˆ— in S.

Given an algebra (S, âˆ—) where âˆ— is a binary operation, elements a, b âˆˆ S, and the neutral element e âˆˆ S.  If a âˆ— b = e, then a is called a _left inverse_ of b and b is called a _right inverse_ of a.  If an element is both a left and a right inverse, it is called a _two-sided inverse_ (or simply _inverse_).  The inverse element of element x âˆˆ S is denoted x^-1^ (or i(x) or -x if the algebra's operation is denoted +).

Note that the algebra (â„¤, +) where + denotes normal addition, subtraction a - b is modeled by adding the inverse, i.e. a + i(b) (or a + -b).  Likewise for division in algebra (â„, Â·) where Â· denotes normal multiplication: division a / b is modeled by multiplying the inverse, i.e. a Â· i(b) (or a Â· b^-1^).

A _semigroup_ is an algebra (S, âˆ—) where âˆ— is a binary associative operation on S.

A _monoid_ is an algebra (M, âˆ—) where âˆ— is a binary associative operation and S has a neutral element.

A _group_ is an algebra (S, âˆ—) where âˆ— is a binary associative operation and S has a neutral element and every element x âˆˆ S has an inverse element.

The _order_ (or _cardinality_) of a group G (or ring or field), denoted |G|, is the number of elements it contains.

A group is _commutativ_ (or _abelian_) if x âˆ— y = y âˆ— x âˆ€ x,y âˆˆ S.  If a group is not commutative, its called _noncommutative_ (or _non-abelian_).

Let (S, âˆ—) be a group with the neutral element e. The _i-th power_ of x âˆˆ S, denoted x^i^, is inductively defined as follows, for any x âˆˆ S and i âˆˆ â„¤:

i) x^0^ = e (x^0^ is called the _trivial power of x_)

ii) x^1^ = x

iii) x^i^ = x âˆ— x^i-1^ âˆ€ i > 1 (x^i^ is called a _nontrivial power of a_)

iv) x^-i^ = (i(x))^i^ âˆ€ i â‰¥ 1

Given a group G = (S, âˆ—), an element x âˆˆ S is called a _generator_ of that group if S = {x^i^|iâˆˆâ„¤}.  We also denote that with âŸ¨xâŸ© = G. If a group has a generator, then the group is called _cyclic_.

Given a group (S, âˆ—) with identity e. The _order_ of an element x âˆˆ S, denoted |x|, is the smallest positive integer n such that x^n^ = e. If there is no such n, then element x has _infinite order_.

A _ring_ is an algebra (R, +, Â·) where (R, +) is a commutative group and (R, Â·) is a semigroup and Â· is distributive over + (*to-do* unspecified whether left/right/total distributive).  Wether or not a ring requires an identity under Â· is under debate, see also ring with identity.

A ring (R, +, Â·) with neutral element 0 under + is called _zero division free_ if x Â· y â‰  0 âˆ€ x, y âˆˆ R - \{0}.

My personal derivation: The neutral element 0 under + has no inverse under Â·, so we want to prohibit having to take the inverse of 0.  We take the inverse of an element when a = c Â· 1/b â‡” a Â· b = c.  So in our use case we want to prohibit (for a,c âˆˆ R - \{0}) that a Â· 0 = c â‡” 1/a Â· a Â· 0 = 1/a Â· c â‡” 0 = 1/a Â· c, which is what zero division free said.

If the Â· operation of a ring (R, +, Â·) is commutative, it's called a _commutative ring_.  If the Â· operation is not commutative, it's called a _noncommutative ring_ (or simply ring).

If the Â· operation of a ring (R, +, Â·) with identity 0 under + has an identity for every element in R - \{0}, it's called a _ring with identity_.

A _field_ (_KÃ¶rper_ in German) (R, +, Â·) with neutral element 0 under + is a zero division free ring where for Â· the following holds: commutative, R - \{0} has an identity and there's an inverse for all x âˆˆ R - \{0}.

Note that some authors say that a ring (R, +, Â·) is the commutative group (R, +) with identity 0 and the commutative group (R-\{0}, Â·).  This is not entirely correct because it technically says that Â· is closed under R-\{0} which is not what we mean.  We do want to allow 0 as operand and result of Â·, we only want to disallow division by 0.

For example, â„š and â„ build fields with respect to addition and multiplication.  However for â„¤ it is impossible to define division.

Given a field K = (R, +, Â·) with identity 1 under + and identity 0 under Â·, the _field characteristic_ ch(K) is the minimum number of times 1 has to be added (e.g. 1+1 counts as two times) to equal 0.  If 0 is never reached, then ch(K) = 0.

A _finite field_ (or _Galois field_) is a field with a finite field order.  The order of a finite field is always a prime power p^k^, where p is a prime and k is a positive integer.  All finite fields of a given order are isomorphic.  In case k = 1, the finite field is called a _prime field_, denoted GF(p) (or ğ”½~p~), and is the field of residue classes modulo p, where the elements of GF(p) are denoted 0, ..., p-1.  Thus a = b in GF(p) means the same as a â‰¡ b (mod p).  p is the characterstic of the prime field.  The inverse with respect to Â· can be computed with the _extended Eucledian algorithm_ (*to-do*).  In case k > 1, the finite field is denoted GF(p^k^) (or ğ”½~p^k^~).

â„¤/pâ„¤ denotes a special case of a quotient group (recall a group as only one operation), but is apparently sometimes used to denote a prime field (recall that a field has two operations).

â„¤~n~ denotes an abstract algebra over set {0, ..., n-1} with mod n modular arithmetic.  Wether â„¤~n~ denotes a finite group or a finite field (and thus prime field) depends on the context.

*to-do* vector space, norm, module,

Summary:

R denotes the set of elements of the algebraic structure.  ba denotes binary associative operation.  The identity under +, if it exists, is denoted 0.  NA denotes not available.  d denotes that Â· is distributive over +.  e denotes existence of an identity under the given operation.  e/0 denotes existence of an identity within R - \{0} under given operation.  inv denotes the existence of a inverse element for every element of the algebraic structure under the given operation.   inv/0 denotes the existence of a inverse element for every element R - \{0} under the given operation.  zdf denotes zero division free, see there.

|=====
|                    |      | + (ba)    | Â· (ba, d)
| semigroup          | +    |           | NA
| monoid             | +    | e         | NA
| group              | +-   | e, inv    | NA
| commutative group  | +-   | e, inv, c | NA
| ring               | +-Â·  | e, inv, c | [e/0]
| ring with identity | +-Â·  | e, inv, c | e/0
| commutative ring   | +-Â·  | e, inv, c | c
| zdf ring           | +-Â·  | e, inv, c | zdf
| field              | +-Â·/ | e, inv, c | zdf, c, e/0, inv/0
|=====


References:

- https://www.youtube.com/playlist?list=PLi01XoE8jYoi3SgnnGorR_XOW3IcK-TP6

- Book ``Algorithmics for Hard Problems: Introduction to Combinatorial Optimization, Randomization, Approximation, and Heuristics'', 2nd Edition, Juray HromkoviÄ, chapter ``2.2.4 Algebra and Number Theory''

- Book ``Design and Analysis of Randomized Algorithms'', chapter ``A.2 Algebra and Number Theory'' starting p. 239 bottom

=== Linear algebra

The _determinant_ of a square matrix A is denoted det(A) or |A|.

In the 2D case:

--------------------------------------------------
      |a b|
|A| = |   | = ad - bc
      |c d|
--------------------------------------------------

The geometric interpretation is that, when you think about the matrix representing a linear transformation, the absolute value of the determinant is the factor applied to an area (in the 2D case, volume in 3D case and so on).  Also, in the 2D case, if A is build by combining column vectors v1 and v2 side by side, the determinant is positive when v1 is clockwise from v2 (their tails coinciding), negative when v1 is counterclockwise, and zero when the two are colinear.

A symmetric nÃ—n real matrix A is said to be _positive definite_ if the scalar v^T^Av is strictly positive for every non-zero nÃ—1 vector v of real numbers. _Positive semi-definite_ matrices are defined similarly, except that additionally v^T^Av might be zero.  A is positive semi definite iff there exists a matrix B such that A = B^T^B.


== Geometry

A _metric space_ M is an ordered pair (S, d) where S is a set and d is a metric on S.  A _metric_ (or _distance function_ or simply _distance_) is a function d that defines a distance between each pair of elements of a set S.  It is defined as d: Sâ¨¯S â†’ â„~+~, where for all x,y,z âˆˆ S the following conditions are satisfied:

d(x,y) â‰¥ 0 [small]#(non-negatity)# +
d(x,y) = 0 â‡” x = y [small]#(identity of indiscernibles)# +
d(x,y) = d(y,x) [small]#(symmetry)# +
d(x,z) â‰¤ d(x,y) + d(y,z) [small]#(triangle inequality)#

_triangle inequality_: Definition above. In other words, detours (two edges) are never shorter (in terms of d(Â·,Â·)) than the direct edge.

A _right angle_ is an angle of exactly 90Â° (Ï€/2 radians).  Two vectors u and v are _perpendicular_, denoted uâŸ‚v, iff their angle is a right angle, or equivalently, if their scalar product is zero.  A set of vectors is _orthogonal_ iff they are pairwise perpendicular.  A _normal_ vector of a point on a smooth surface is any vector perpendicular to the plane.

The _dot product_ (or _scalar product_) of two vectors xâƒ— and yâƒ— is defined as xâƒ—Â·yâƒ— = âˆ‘x~i~y~i~ = â€–xâƒ—â€–â€–yâƒ—â€–cos(Î¸).  The former variant is the algebraic interpretation, the later is the geometric interpration.  More concretely, the geometric interpretation is that xâƒ—Â·(yâƒ—/â€–yâƒ—â€–) is the projection of xâƒ— onto yâƒ—, when the two vectors are placed so that their tails coincide.

The _inner product_ generalizes the dot product to abstract vector spaces over a field of scalars. It is usually denoted using angular brackets by âŸ¨a,bâŸ©.  In Euclidean geometry, the two are equivalent.

The _cross product_ (or _vector product_ or _directed area product_ (in Euclidean geometry)) of two vectors xâƒ— and yâƒ— is defined as xâƒ—â¨¯yâƒ— = â€–xâƒ—â€–â€–yâƒ—â€–sin(Î¸)nâƒ—.  nâƒ— is the unit vector normal to the plane containing xâƒ— and yâƒ—.  By convention, the direction of nâƒ— is given by the _right-hand rule_: The index finger represents xâƒ—, the middle finger yâƒ—, and the thumb xâƒ—â¨¯yâƒ—.  The maginitude of the cross product can be interpreted as the area of the parallelogram having xâƒ— and yâƒ— as sides: â€–xâƒ—â¨¯yâƒ—â€– = â€–xâƒ—â€–â€–yâƒ—â€–sin(Î¸).  Cross product is zero â‡” the lines are parallel. Cross product is positive (negative) â‡” xâƒ— is clockwise (counterclockwise) from yâƒ— (their tails coinciding).



== Misc

=== Fibonacci sequence / numbers

reccurence relation: F~n~ = F~n-1~ + F~n-2~

closed form expression: F~n~ = (Ï•^n^ - Ïˆ^n^) / âˆš5 = [Ï•^n^ / âˆš5], where
Ï• is golden ratio and Ïˆ=1-Ï•, and [x] is the nearest integer function
(aka round function).

Note: lim~nâ†’âˆ~ F~n~ / F~n-1~ = Ï•

Applications: Fibonacci heap


=== Golden ratio

Ï• = (1+âˆš5)/2 â‰ˆ 1.618â€¦

Two quantities a and b are in the golden ratio Ï• iff a+b / a = a / b =
Ï•, i.e. a=Ï•b

=== Factorial

reccurence relation: x! = x*(x-1) and 0!=1

stirlings approximation: n! ~ âˆš(2Ï€n)*(n/e)^n^



== References

- MIT course 6.042 "Mathematics for computer science".
  * spring 2015, index: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/course-index/
  * spring 2015, textbook: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/readings/MIT6_042JS15_textbook.pdf
  * fall 2010, video lectures: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/video-lectures/
  * fall 2010, readings: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/

- Book ``Algorithmics for Hard Problems: Introduction to Combinatorial Optimization, Randomization, Approximation, and Heuristics'', 2nd Edition, Juray HromkoviÄ. The Introduction chapter serves as good summary of computer science fundamentals.

- Book ``Design and Analysis of Randomized Algorithms'', chapters ``A Fundamentals of Mathematics'' p. 227 and ``2.2 Elementary Probability Theory'' p. 20

- Book ``Modern Cryptography: Theory and Practice'' has a mathematical foundations part


== to-do

- skalarproduct
- greatest common divider/divisor
- log/exp relation to mul/div
- angle between vector
